{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 726715,
          "sourceType": "datasetVersion",
          "datasetId": 262
        },
        {
          "sourceId": 11384,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 6216
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Gemma Python Chat (GPC)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/Gemma_Python_Chat_(GPC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T052017Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D18c1461ae8e63e723f579f82aa65408f1dfe371713faefae6a2f9162a5e53d7cb07470170425b7e616688e6a76627a48925f5e1e63a4069bce6acca95df4c51ac24c330a3724b6e88ab43a39b3e5c90f39fe8f59796bd25abe904f2b54a77548f69ce2f27a3c5cc8efa83faed9bc9a4fa94fc8c11b981dc6faee64916b583ed7614101c1dde2bb3f93420a6afd2d5664f36a06548f6f0a1db050e1adaa07df14dd3004dd8d84c8a00fb4c81b5313112e9bdf746ef10228577ee0f0bf04f7d2794558958908dc5daafe5a313f03994e00848167824d5e2f873e3529a2e5b09d4e7917d123e55bfd1ed4373e6275282e6b340a78fe655654c84435db99b003e503,pythonquestions:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F262%2F726715%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T052017Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da409fc8008f7a291d111c76883f9e8cb1f4dd444a5d857700085a6b714b2f490f3fdb711c54e2cb47912738b09bb0a968cba2e65f60988421deae89492fb29f406bd947fe884dc45453862fee1eabcf3026008d3deb2ac4150f815bacbb8c000630813c3eb060ba5be62a6e1e8b8cc82521735eab3a1408f416f1539c899cdd3a4fcae64539621308f72c5ccc995cb9b6d5f4e774b8367b11f056a408f5db4fef7986c8832db9dd6f21d3b7f0a22d4ac32542bbb01ef7f92b9b725337774de08aab92375ab36b010002b39fc80fbc91742947b8d3c8f01e24883cfa6368517c3bb5f13eb1c12b1c191ee31930186dce217cb8b8ac70fab2b807e1e81f267041a,gemma/transformers/2b/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F6216%2F11384%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T052018Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D82d473fe2b470eb77a27c0daff33042eee108eb6fadf9c5111c62070d40cc91d4aa2e1443095befec6a4990f591b84ea511b533f5f95da0f21b3092c578f4be0925a406653adcf0938f319b5fa839adcbabe47c00cb1f2dfd31e98b0c1018a86e3d5fead4ef36927173c5a3abc5f30c1c22af8f032eb0afa93c979f1228782bb32d554ccc6f5811ed102ec9cf84dda07d57167fcd749bdeb5615b4b2512cb0366d906eb7d63e455c5f6a3fe08020f70c032394ab553a8224dc2fca95890261079529b5b42ead419f53c4be6e1d49dde0a924edd0d85a0264a46ead835fc15a12a1b5b1ab966a7c8efa016e0fd7d258b6afe0d8c84a44fe2ac53956ac6a8a0fe1'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "KgkWPcxVv-Tg"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/GPC_(Gemma_Python_Chat).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP ü§ñ Gemma Python Chatbot"
      ],
      "metadata": {
        "id": "Qx8hbihKsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.postimg.cc/Nfpn7mxR/gemma.png)"
      ],
      "metadata": {
        "id": "FaXdoJnbv-Tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gemma Python Chatbot üöÄüöÄ help you to answer common questions about the üêç Python programming language, powered by [Gemma 2B IT](https://blog.google/technology/developers/gemma-open-models/) with a few-shot strategy using some questions of [Stackoverflow python question kaggle dataset](https://www.kaggle.com/datasets/stackoverflow/pythonquestions) and updated with the Python Enhancement Proposal ([PEPs](https://peps.python.org/)) documentation using a Retrival-Augmented Generation ([RAG](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)) supported by a Chroma vectorial database using the open source embeddings [hkunlp/instructor-large](https://huggingface.co/hkunlp/instructor-large) of 768 entries, orchested by [langchaing](https://python.langchain.com/docs/use_cases/chatbots/) that is a framework that let you use differen models and less code changes on the architecture when you decide to test other providers"
      ],
      "metadata": {
        "id": "5C9kSlAVsoW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "5GhJQPvLsoW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb #!pip install chromadb==0.3.26\n",
        "# !pip install ydata-profiling #!pip install ydata-profiling==4.6.1\n",
        "!pip install langchain #!pip install langchain==0.0.345 ##!pip install langchain-core==0.1.31\n",
        "#!pip install pydantic  #!pip install pydantic==1.10.14\n",
        "!pip install sentence-transformers   #!pip install sentence-transformers==2.6.1\n",
        "!pip install InstructorEmbedding  #!pip install InstructorEmbedding==1.0.1"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "scrolled": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-12T03:31:45.009479Z",
          "iopub.execute_input": "2024-04-12T03:31:45.009781Z",
          "iopub.status.idle": "2024-04-12T03:33:03.625501Z",
          "shell.execute_reply.started": "2024-04-12T03:31:45.009757Z",
          "shell.execute_reply": "2024-04-12T03:33:03.624362Z"
        },
        "trusted": true,
        "id": "oVsAj2TDv-Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import *\n",
        "from IPython.display import display, Markdown\n",
        "# variables\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "# model\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "# data\n",
        "from datasets import load_dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# embeddings\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "# vector database\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "Vq5ZFhX7soW4",
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:03.627491Z",
          "iopub.execute_input": "2024-04-12T03:33:03.627772Z",
          "iopub.status.idle": "2024-04-12T03:33:04.421629Z",
          "shell.execute_reply.started": "2024-04-12T03:33:03.627749Z",
          "shell.execute_reply": "2024-04-12T03:33:04.42086Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "5ZNb3IYqv-Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use object-oriented Python programming üíª to develop the chatbot, using the classes üìÅ in the corresponding order to support the Agent class that is the final service of the chatbot ü§ñ."
      ],
      "metadata": {
        "id": "U2HktGffv-Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions and answers data\n",
        "\n"
      ],
      "metadata": {
        "id": "QniSPK9tv-Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class fit the data of [Stackoverflow python question kaggle dataset](https://www.kaggle.com/datasets/stackoverflow/pythonquestions) üìö to use it in a fine-tuned format ‚öôÔ∏è or a list of dicts üóÇÔ∏è... it depends on the case of use you need! it also will be use to select some samples to make the few-shoot strategy"
      ],
      "metadata": {
        "id": "ip15OyPuv-Tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pythonQAData:\n",
        "    \"\"\"\n",
        "    Processes data from Questions and Answers CSV files to provide a structured Q&A format.\n",
        "\n",
        "    Attributes:\n",
        "        questions_path (str): Path to the Questions CSV file\n",
        "        answers_path (str): Path to the Answers CSV file\n",
        "        tags_path (str): Path to the tags CSV file\n",
        "\n",
        "    Methods:\n",
        "        load_data(): Loads the CSV data into DataFrames.\n",
        "        merge(): Cleans, merges, and formats the question and answer data.\n",
        "        get_formatted_qa(): Returns a list of formatted question-answer strings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.questions_path = '../input/pythonquestions/Questions.csv'\n",
        "        self.answers_path = '../input/pythonquestions/Answers.csv'\n",
        "        self.tags_path = '../input/pythonquestions/Tags.csv'\n",
        "        self.regex = r\"<\\/?[\\w\\s]*>\"\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Loads Questions and Answers data from CSV files.\"\"\"\n",
        "        df_questions = pd.read_csv(\n",
        "            self.questions_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Score',\n",
        "                'Title'\n",
        "            ]\n",
        "        )\n",
        "        df_answers = pd.read_csv(\n",
        "            self.answers_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'ParentId',\n",
        "                'Score',\n",
        "                'Body'\n",
        "            ]\n",
        "        )\n",
        "        df_tags = pd.read_csv(\n",
        "            self.tags_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Tag'\n",
        "            ]\n",
        "        )\n",
        "        return df_questions, df_answers, df_tags\n",
        "\n",
        "\n",
        "    def qa_data(self):\n",
        "        \"\"\"Cleans, merges, and formats the question and answer data.\"\"\"\n",
        "        df_questions, df_answers, df_tags = self.load_data()\n",
        "        # Rename\n",
        "        df_questions.rename(\n",
        "            columns={\n",
        "                'Title': 'Question',\n",
        "                'Score': 'question_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        df_answers.rename(\n",
        "            columns={\n",
        "                'Body': 'Answer',\n",
        "                'ParentId':'Id',\n",
        "                'Score': 'answer_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        # Filter by score\n",
        "        df_questions = df_questions[df_questions['question_score'] > 5].copy()\n",
        "        # Sort and deduplicate answers\n",
        "        df_answers = df_answers.sort_values(\n",
        "            'answer_score',\n",
        "            ascending=False\n",
        "        ).drop_duplicates(subset=['Id'])\n",
        "        # Merge\n",
        "        df_qa = df_questions.merge(\n",
        "            df_answers,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        ).merge(\n",
        "            df_tags,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        )\n",
        "        # filter for python  questions\n",
        "        df_qa = df_qa[df_qa['answer_score'] > 5].copy()\n",
        "        df_qa = df_qa[df_qa['Tag']=='python'].copy()\n",
        "        df_qa['Answer'] = df_qa['Answer'].apply(\n",
        "            lambda x: re.sub(\n",
        "                self.regex,\n",
        "                \"\",\n",
        "                x\n",
        "            )\n",
        "        )\n",
        "        return df_qa\n",
        "\n",
        "    def get_fine_tunning_data(self):\n",
        "        \"\"\"Returns a list of formatted user-assistant strings.\"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        data = [\n",
        "            f\"<-change-of-interlocutor->user:\\n{row['Question']}\\n<-change-of-interlocutor->assistant:\\n{row['Answer']}\"\n",
        "            for index, row\n",
        "            in df_qa_data.iterrows()\n",
        "        ]\n",
        "        return data\n",
        "    def get_qa_data(self):\n",
        "        \"\"\"Returns a list of records dictionaries \"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        data = df_qa_data[\n",
        "            [\n",
        "                'Question',\n",
        "                'Answer'\n",
        "            ]\n",
        "        ].to_dict(orient='records')\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "bSU-ohmssoW5",
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:04.422646Z",
          "iopub.execute_input": "2024-04-12T03:33:04.422926Z",
          "iopub.status.idle": "2024-04-12T03:33:04.437593Z",
          "shell.execute_reply.started": "2024-04-12T03:33:04.422902Z",
          "shell.execute_reply": "2024-04-12T03:33:04.436688Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Enhancement Proposals data"
      ],
      "metadata": {
        "id": "jByErO6Tv-Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class Peps scrape all the oficial [pep](https://peps.python.org/) üîé to get all the best practices of this programming language üêç and their latest features ‚ú®, disposed to uses in the RAG system for the chatbot"
      ],
      "metadata": {
        "id": "sVuAO3XHv-Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Peps:\n",
        "    \"\"\"\n",
        "    Scrapes Python Enhancement Proposals (PEPs) from https://peps.python.org/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initiates the scraping process.\"\"\"\n",
        "        self.base_url = 'https://peps.python.org/'\n",
        "\n",
        "    def scraper(self, url):\n",
        "        \"\"\"\n",
        "        Fetches the HTML content of a given URL.\n",
        "\n",
        "        Args:\n",
        "            url (str): The URL to fetch.\n",
        "\n",
        "        Returns:\n",
        "            BeautifulSoup: A BeautifulSoup object representing the parsed HTML.\n",
        "        \"\"\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    def _fetch_pep_links(self):\n",
        "        \"\"\"Fetches links to individual PEP pages (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of PEP URLs.\n",
        "        \"\"\"\n",
        "        soup = self.scraper(self.base_url)\n",
        "        return list(\n",
        "            set(\n",
        "                [\n",
        "                    self.base_url + ref['href']\n",
        "                    for ref in soup.find_all('a', class_='pep reference internal')\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _download_peps(self):\n",
        "        \"\"\"Downloads the content of individual PEPs (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of BeautifulSoup objects representing individual PEPs.\n",
        "        \"\"\"\n",
        "        pep_links = self._fetch_pep_links()\n",
        "        return [self.scraper(pep_link) for pep_link in pep_links]\n",
        "\n",
        "    def scrape(self):\n",
        "        \"\"\"Extracts the relevant text content from each PEP.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of text strings, each representing the content of a PEP.\n",
        "        \"\"\"\n",
        "        pep_soups = self._download_peps()\n",
        "        return [\n",
        "            '\\n'.join([word.text.replace('\"\"\"',\"'\") for word in soup.find_all('section')])\n",
        "            for soup in pep_soups\n",
        "        ]\n",
        "\n",
        "    def jsonl_format(self):\n",
        "        \"\"\"Format the spcraped data to a jsonl format is it alist of dictionaries\n",
        "        Returns:\n",
        "            list: A list of dictionaries with the following:\n",
        "                page_content: The content of each section scraped\n",
        "                source: The linke where it was scraped\n",
        "        \"\"\"\n",
        "        source_links = self._fetch_pep_links()\n",
        "        pep_content = self.scrape()\n",
        "        pep_data = dict(zip(source_links,pep_content))\n",
        "        return [\n",
        "            {\n",
        "                'text': value,\n",
        "                'source': key\n",
        "            }\n",
        "            for key, value in pep_data.items()\n",
        "        ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:04.439931Z",
          "iopub.execute_input": "2024-04-12T03:33:04.440197Z",
          "iopub.status.idle": "2024-04-12T03:33:04.458601Z",
          "shell.execute_reply.started": "2024-04-12T03:33:04.440175Z",
          "shell.execute_reply": "2024-04-12T03:33:04.457786Z"
        },
        "trusted": true,
        "id": "VhlxYI82v-Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "S090QWlUv-Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embeder class is the method that lets you embed the data given as a list of strings ‚û°Ô∏èüì¶. It will support the retriever for the RAG system üîç"
      ],
      "metadata": {
        "id": "rd-XyCwdv-Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeder:\n",
        "    \"\"\"\n",
        "    Creates text document embeddings (numerical representations) for semantic search,\n",
        "    similarity comparison, and related natural language processing tasks. Offers the choice\n",
        "    between a larger model for richer embeddings or a smaller, more efficient model.\n",
        "\n",
        "    Attributes:\n",
        "        large (bool): Controls the size of the embedding model. When set to True, uses a\n",
        "                      larger model for richer embeddings (default: True).\n",
        "        model (str): The name of the embedding model to use. Changes based on the value of 'large'.\n",
        "        device (str): The device to use for computation. Options include \"cpu\" or, if available, \"cuda\"\n",
        "                      for GPU acceleration (default: \"cuda\").\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, large: bool = False):\n",
        "        \"\"\"\n",
        "        Initializes the Embeder instance with preferences for model size.\n",
        "\n",
        "        Args:\n",
        "            large (bool, optional): If True, initializes with the larger embedding model.\n",
        "                                    Defaults to False for the smaller, faster model.\n",
        "        \"\"\"\n",
        "        self.large = large\n",
        "        if self.large:\n",
        "            self.model = \"hkunlp/instructor-large\"\n",
        "            self.device = \"cuda\"  # If GPU is available\n",
        "        else:\n",
        "            self.model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "    def instructor(self):\n",
        "        \"\"\"\n",
        "        Configures the embedding pipeline based on the selected model size.\n",
        "\n",
        "        Returns:\n",
        "           An embedding object from either the HuggingFaceInstructEmbeddings class\n",
        "           (for the large model) or the SentenceTransformerEmbeddings class (for the smaller model).\n",
        "        \"\"\"\n",
        "        if self.large:\n",
        "            embed = HuggingFaceInstructEmbeddings(\n",
        "                model_name=self.model,\n",
        "                model_kwargs={\"device\": self.device}\n",
        "            )\n",
        "        else:\n",
        "            embed = SentenceTransformerEmbeddings(model_name=self.model)\n",
        "        return embed\n",
        "\n",
        "    def run(self, docs_list: list):\n",
        "        \"\"\"\n",
        "        Generates embeddings for a given list of text documents.\n",
        "\n",
        "        Args:\n",
        "            docs_list (list): A list of text documents to embed.\n",
        "\n",
        "        Returns:\n",
        "            List[List[float]]: A list of lists, where each inner list represents the\n",
        "                               numerical embedding for a corresponding document in the input.\n",
        "        \"\"\"\n",
        "        if docs_list is None:\n",
        "            docs_list = ['']  # Prevent errors with empty input\n",
        "\n",
        "        embed = self.instructor()  # Get the appropriate embedding object\n",
        "        return embed.embed_documents(docs_list)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:04.459701Z",
          "iopub.execute_input": "2024-04-12T03:33:04.459986Z",
          "iopub.status.idle": "2024-04-12T03:33:04.474111Z",
          "shell.execute_reply.started": "2024-04-12T03:33:04.459963Z",
          "shell.execute_reply": "2024-04-12T03:33:04.473315Z"
        },
        "trusted": true,
        "id": "sOH_Yiqfv-Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "AwdUk3Ruv-Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a good practice, I consider the prompt as the instruction you use to get what you need from an LLM üß†. It includes all the parameters the LLM needs in every attempt.  Any change in a parameter will generate a different output üîÑ.  Therefore, the input is the whole prompt instruction plus all its parameters.\n",
        "\n",
        "Additionally, it's defined in an external object (like a NoSQL database üóÑÔ∏è). This allows for easy updates outside of the code, making changes simpler for non-technical people üëç."
      ],
      "metadata": {
        "id": "45AXx8bdv-Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_config = [{\n",
        "    \"prompt_id\": \"0.0.1\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 1,\n",
        "    \"max_length\": 250,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.2\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "6. DO NOT repeat more than twice the same sentence!\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 1,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.3\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "6. DO NOT repeat more than once the same sentence!\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.4\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures in no more than 1000 characters\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "6. DO NOT repeat more than once the same sentence!\n",
        "7. If the user ask to you for a python script code, just return the code, do not EXPLAIN anything else\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "}]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:04.475385Z",
          "iopub.execute_input": "2024-04-12T03:33:04.475641Z",
          "iopub.status.idle": "2024-04-12T03:33:04.489476Z",
          "shell.execute_reply.started": "2024-04-12T03:33:04.47562Z",
          "shell.execute_reply": "2024-04-12T03:33:04.488692Z"
        },
        "trusted": true,
        "id": "grjNfB7Jv-Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T05:03:58.746391Z",
          "iopub.execute_input": "2024-04-04T05:03:58.746828Z",
          "iopub.status.idle": "2024-04-04T05:03:58.751668Z",
          "shell.execute_reply.started": "2024-04-04T05:03:58.746794Z",
          "shell.execute_reply": "2024-04-04T05:03:58.750527Z"
        },
        "id": "QsrMGEIWv-To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class aims to load the project's environment variables üîê (in this case, Kaggle secrets). Additionally, it disposes of the prompt, as previously defined, to use it according to the team's chosen version üéØ and for the method that needs those secrets to work ‚ú®."
      ],
      "metadata": {
        "id": "yA1L-4M7v-To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Settings:\n",
        "    \"\"\"\n",
        "    Manages and loads external configuration secrets for the application.  Utilizes a UserSecretsClient\n",
        "    to securely retrieve sensitive configuration values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Settings class and retrieves configuration secrets.\n",
        "        \"\"\"\n",
        "        self.user_secrets = UserSecretsClient()  # Create an instance for accessing secrets\n",
        "\n",
        "        # Load individual secrets (add descriptions for clarity)\n",
        "        self.CHUNK_SIZE = self.user_secrets.get_secret(\"CHUNK_SIZE\")  # The size of data chunks for processing\n",
        "        self.CHUNK_OVERLAP = self.user_secrets.get_secret(\"CHUNK_OVERLAP\")  # Ooverlap between data chunks\n",
        "        self.CHROMA_NAME_INDEX = self.user_secrets.get_secret(\"CHROMA_NAME_INDEX\")  # Vectorial Database identifier\n",
        "        self.MAX_MEMORY = self.user_secrets.get_secret(\"MAX_MEMORY\")  # Max memory limit to fit the max_lenght paramether\n",
        "        self.K = self.user_secrets.get_secret(\"K\")  # Could be a parameter for an algorithm\n",
        "        self.NN_THRESHOLD = self.user_secrets.get_secret(\"NN_THRESHOLD\")  # A threshold for a neural network or similarity metric\n",
        "        self.PROMPT_ID = self.user_secrets.get_secret(\"PROMPT_ID\")  # An identifier for a prompt (likely in a text-based task and configs)\n",
        "\n",
        "        # Given that 'prompt_config' is teh previous source of prompt definitions:\n",
        "        self.prompt = [prompt for prompt in prompt_config if prompt['prompt_id'] == self.PROMPT_ID][0]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:04.490469Z",
          "iopub.execute_input": "2024-04-12T03:33:04.490728Z",
          "iopub.status.idle": "2024-04-12T03:33:04.50457Z",
          "shell.execute_reply.started": "2024-04-12T03:33:04.490706Z",
          "shell.execute_reply": "2024-04-12T03:33:04.503751Z"
        },
        "trusted": true,
        "id": "4luakarIv-To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever"
      ],
      "metadata": {
        "id": "VBnQM3Wyv-To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The retriever is the engine of the RAG system ‚öôÔ∏è. It uses config settings stored on the Settings class üìÅ and the Embeder class to populate the Chroma vector database üöÄ. This setup needs to be done once and then can be loaded persistently.\n",
        "\n",
        "* when setting up the vectorial database, This process may take a few minutes the first time ‚è≥, so please be PATIENT!"
      ],
      "metadata": {
        "id": "EGk-a7c_v-To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeder = Embeder()\n",
        "settings = Settings()\n",
        "peps = Peps()\n",
        "class Retriever:\n",
        "    \"\"\"\n",
        "    Retrieves relevant text chunks from a Chroma vectorial database based on a query. Leverages external\n",
        "    configuration settings (Settings) and an embedding model (Embeder).\n",
        "\n",
        "    Handles text splitting, vector database loading, and querying.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Retriever class. Configures the text splitter using settings and stores the\n",
        "        index name for the Chroma database. Initializes the Chroma vectorial_db object.\n",
        "        \"\"\"\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=int(settings.CHUNK_SIZE),\n",
        "            length_function=len,\n",
        "            chunk_overlap=int(settings.CHUNK_OVERLAP)\n",
        "        )\n",
        "        self.index_name = settings.CHROMA_NAME_INDEX\n",
        "        self.data = peps.jsonl_format()\n",
        "        self.documents = self.text_splitter.split_documents(self._prepare_documents())\n",
        "\n",
        "        # Initialize the Chroma vectorial database with preprocessed data\n",
        "        self.vectorial_db = Chroma.from_documents(\n",
        "            documents=self.documents,\n",
        "            embedding=embeder.instructor()\n",
        "        )\n",
        "\n",
        "    def _prepare_documents(self):\n",
        "        \"\"\"\n",
        "        Loads and preprocesses data for storage in the Chroma database.\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "        for obj in self.data:\n",
        "            page_content = obj.get(\"text\", \"\")\n",
        "            metadata = {\n",
        "                \"source\": obj.get(\"source\", \"local\")\n",
        "            }\n",
        "            documents.append(Document(page_content=page_content, metadata=metadata))\n",
        "        return documents\n",
        "\n",
        "    def query(\n",
        "            self,\n",
        "            message: str,\n",
        "            k: int = int(settings.K),\n",
        "            threshold: float = float(settings.NN_THRESHOLD)\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Retrieves the most similar text chunks from the Chroma database based on a given query.\n",
        "        \"\"\"\n",
        "        # Perform the similarity search\n",
        "        try:\n",
        "            res = self.vectorial_db.similarity_search_with_score(message, k=k)\n",
        "            # Filter and return results\n",
        "            relevant_results = list(\n",
        "                set(  # Remove duplicates\n",
        "                    [\n",
        "                        vector[0].page_content\n",
        "                        for vector in res\n",
        "                        if vector[1] < threshold  # Apply similarity threshold\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            return [\n",
        "                result.replace('\"\"\"',\"'\") # Clean up possible docString retrieved from documented code\n",
        "                for result in relevant_results\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:04.505739Z",
          "iopub.execute_input": "2024-04-12T03:33:04.507697Z",
          "iopub.status.idle": "2024-04-12T03:33:05.596749Z",
          "shell.execute_reply.started": "2024-04-12T03:33:04.507668Z",
          "shell.execute_reply": "2024-04-12T03:33:05.595996Z"
        },
        "trusted": true,
        "id": "eeLOLCNUv-To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Message"
      ],
      "metadata": {
        "id": "um3Drd8av-To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The message class lets you store every system, user, and assistant message in an organized structure üí¨. This includes unique IDs and timestamps for each message generated during the conversation.\n",
        "\n",
        "It uses a local JSON file üìÑ to store chat history (including few-shot samples to guide the model). In a real-world scenario, this file would represent a NoSQL database üóÑÔ∏è  capable of handling chats from many users when the chatbot is deployed as a large-scale web service üåê.\n",
        "\n",
        "It use the Setting class to meet the main propmpt configs in the run time üòÉ üèÉ‚Äç‚ôÄÔ∏è  "
      ],
      "metadata": {
        "id": "3W9MqxErv-To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Message:\n",
        "    \"\"\"\n",
        "    Represents a message within a conversational context. Stores information about the message's role,\n",
        "    content, timestamp, and manages saving and loading conversation history.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        role: str,\n",
        "        content: str,\n",
        "        timestamp: str = int(time.time()),\n",
        "        prompt_id: str = settings.PROMPT_ID\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a Message object.\n",
        "\n",
        "        Args:\n",
        "            role (str): Indicates the role of the sender (e.g., 'user', 'system').\n",
        "            content (str): The text content of the message.\n",
        "            timestamp (str, optional): A timestamp for the message (defaults to the current time).\n",
        "            prompt_id (str, optional): Identifier relating to a specific prompt configuration (from settings).\n",
        "        \"\"\"\n",
        "\n",
        "        self.reply_id = str(uuid.uuid4())  # Generate a unique ID for the message\n",
        "        self.role = role\n",
        "        self.content = content\n",
        "        self.timestamp = timestamp\n",
        "        self.file = 'data/history.json'  # File for storing conversation history\n",
        "\n",
        "        # Ensure the history file exists\n",
        "        if not os.path.exists(self.file):\n",
        "            json.dump([], open(self.file, 'w'))  # Create an empty file if it doesn't exist\n",
        "\n",
        "    def reply(self):\n",
        "        \"\"\"\n",
        "        Formats a basic reply message structure.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the message's reply ID, role, content, and timestamp.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'reply_id': self.reply_id,\n",
        "            'role': self.role,\n",
        "            'content': self.content,\n",
        "            'timestamp': self.timestamp\n",
        "        }\n",
        "\n",
        "    def system_reply(self):\n",
        "        \"\"\"\n",
        "        Generates a system reply using the prompt configuration.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the system reply, including ID, role, timestamp,\n",
        "                  and text content derived from settings.\n",
        "        \"\"\"\n",
        "        prompt = settings.prompt\n",
        "        return {\n",
        "            'reply_id': self.reply_id,\n",
        "            'role': 'system',\n",
        "            'content': prompt['system'],\n",
        "            'timestamp': self.timestamp\n",
        "        }\n",
        "\n",
        "    def new_chat(self):\n",
        "        \"\"\"\n",
        "        Starts a new chat by initializing the history file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list with the initial system reply and the user's message.\n",
        "        \"\"\"\n",
        "        init_chat = [self.system_reply(), self.reply()]\n",
        "        json.dump(init_chat, open(self.file, 'w'))\n",
        "        return init_chat\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Updates the conversation history file by appending the current reply.\n",
        "        \"\"\"\n",
        "        dict_history = json.load(open(self.file))\n",
        "        dict_history.append(self.reply())\n",
        "        json.dump(dict_history, open(self.file, 'w'))\n",
        "\n",
        "    def restart_history(self):\n",
        "        \"\"\"\n",
        "        Clears the conversation history file.\n",
        "        \"\"\"\n",
        "        json.dump([], open(self.file, 'w'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:05.597946Z",
          "iopub.execute_input": "2024-04-12T03:33:05.598393Z",
          "iopub.status.idle": "2024-04-12T03:33:05.610359Z",
          "shell.execute_reply.started": "2024-04-12T03:33:05.59836Z",
          "shell.execute_reply": "2024-04-12T03:33:05.609453Z"
        },
        "trusted": true,
        "id": "KXIv6MGTv-To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Gemma"
      ],
      "metadata": {
        "id": "w9GPoEDhv-Tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the brain of the agent! üß† The Gemma class lets you load the model üí™ and tokenizer ‚öôÔ∏è using those smart settings from the Setting class. Then, you can chat with the model using the powerful GPU device üöÄ."
      ],
      "metadata": {
        "id": "QjOFrJdfv-Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gemma:\n",
        "    \"\"\"\n",
        "    Implements a conversational AI chatbot powered by Gemma large language model. Initializes the model,\n",
        "    tokenizer, and prepares settings from a configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Gemma chatbot instance.\n",
        "\n",
        "        Loads the language model and its corresponding tokenizer from the settings configuration.\n",
        "        Prepares the model for use on a GPU (if available).\n",
        "        \"\"\"\n",
        "        self.prompt = settings.prompt  # Load prompt settings\n",
        "\n",
        "        # Load language model and tokenizer\n",
        "        self.model_checkpoint = self.prompt['model']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
        "        self.gemma = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_checkpoint,\n",
        "            torch_dtype=torch.float16  # Use half-precision for efficiency (if supported)\n",
        "        ).cuda()  # Move model to GPU (if available)\n",
        "\n",
        "    def chat(self, context: str):\n",
        "        \"\"\"\n",
        "        Generates a chatbot response based on the provided conversational context.\n",
        "\n",
        "        Args:\n",
        "            context (str): The conversational input text.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text response from the chatbot.\n",
        "        \"\"\"\n",
        "        # Prepare input for the language model\n",
        "        input_text = context\n",
        "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        input_ids = {\n",
        "            k: v.to(\"cuda\") for k, v in input_ids.items()  # Move tensors to GPU\n",
        "        }\n",
        "\n",
        "        # Generate a response with the language model\n",
        "        outputs = self.gemma.generate(\n",
        "            **input_ids,\n",
        "            max_length=self.prompt['max_length']  # Control response length\n",
        "        )\n",
        "\n",
        "        # Decode and return the generated text\n",
        "        return self.tokenizer.decode(outputs[0])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:05.613889Z",
          "iopub.execute_input": "2024-04-12T03:33:05.614262Z",
          "iopub.status.idle": "2024-04-12T03:33:05.626313Z",
          "shell.execute_reply.started": "2024-04-12T03:33:05.614226Z",
          "shell.execute_reply": "2024-04-12T03:33:05.625521Z"
        },
        "trusted": true,
        "id": "BAh5anzOv-Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaSubtle:\n",
        "    \"\"\"\n",
        "    Implements a conversational AI chatbot powered by Gemma large language model. Initializes the model,\n",
        "    tokenizer, and prepares settings from a configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Gemma chatbot instance.\n",
        "\n",
        "        Loads the language model and its corresponding tokenizer from the settings configuration.\n",
        "        Prepares the model for use on a GPU (if available).\n",
        "        \"\"\"\n",
        "        self.prompt = settings.prompt  # Load prompt settings\n",
        "\n",
        "        # Load language model and tokenizer\n",
        "        self.model_checkpoint = self.prompt['model']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
        "        self.gemma = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_checkpoint,\n",
        "            torch_dtype=torch.float16  # Use half-precision for efficiency (if supported)\n",
        "        ).cuda()  # Move model to GPU (if available)\n",
        "\n",
        "    def chat(self, context: str):\n",
        "        \"\"\"\n",
        "        Generates a chatbot response based on the provided conversational context.\n",
        "\n",
        "        Args:\n",
        "            context (str): The conversational input text.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text response from the chatbot.\n",
        "        \"\"\"\n",
        "        # Prepare input for the language model\n",
        "        input_text = context\n",
        "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        input_ids = {\n",
        "            k: v.to(\"cuda\") for k, v in input_ids.items()  # Move tensors to GPU\n",
        "        }\n",
        "\n",
        "        # Generate a response with the language model\n",
        "        outputs = self.gemma.generate(\n",
        "            **input_ids,\n",
        "            max_length=self.prompt['max_length'],  # Control response length\n",
        "            temperature=1\n",
        "        )\n",
        "\n",
        "        # Decode and return the generated text\n",
        "        return self.tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T04:35:11.812377Z",
          "iopub.execute_input": "2024-04-12T04:35:11.812727Z",
          "iopub.status.idle": "2024-04-12T04:35:11.821715Z",
          "shell.execute_reply.started": "2024-04-12T04:35:11.812701Z",
          "shell.execute_reply": "2024-04-12T04:35:11.820828Z"
        },
        "trusted": true,
        "id": "EX8a9QJjv-Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expert Python Agent/Assistant"
      ],
      "metadata": {
        "id": "eGYUVBo8v-Tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the agent class brings it all together! It uses the retriever üîç to implement the RAG system (üí° Retrieval-Augmented Generation) and the Gemma class üß† as the powerful brain. Together, they answer questions ‚ùì and the agent performs some final preprocessing ‚ú® to deliver the polished answer\n",
        "\n",
        "When instancing the Retriver it is going to spend around 12 minutes, according to the different test, because it first scrape üï∏Ô∏è the peps documentation to be used to populate the vector stores  üíæ, so it is hard because of the document partition üìÅ and its embdedings  üíé while creating the index"
      ],
      "metadata": {
        "id": "-Lrb77huv-Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "retriever = Retriever()\n",
        "gemma = Gemma()\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Implements a conversational AI agent that leverages a knowledge database for information retrieval\n",
        "    and integrates with a generative language model (Gemma) for response generation. Manages conversation\n",
        "    history and question-answering logic.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Agent, loading prompt settings and ensuring the conversation history file exists.\n",
        "        \"\"\"\n",
        "        self.prompt = settings.prompt\n",
        "        self.file = 'data/history.json'\n",
        "        if not os.path.exists(self.file):\n",
        "            json.dump([], open(self.file, 'w'))  # Create an empty file if necessary\n",
        "\n",
        "        self.token = '\\n<-change-of-interlocutor->'  # Token to separate speakers in the chat history\n",
        "        self.memory_lenght = int(settings.MAX_MEMORY)\n",
        "\n",
        "    def augmented_question(self, question: str):\n",
        "        \"\"\"\n",
        "        Enhances the user's question with relevant technical documentation.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's original question.\n",
        "\n",
        "        Returns:\n",
        "            str: The question augmented with technical documentation (if found), otherwise the original question.\n",
        "        \"\"\"\n",
        "        tech_docs = retriever.query(question)\n",
        "        if len(tech_docs) > 0:\n",
        "            docs = self.prompt['technical_documentation']\n",
        "            tech_docs = '\\n* '.join(tech_docs)\n",
        "            docs = docs.format(technical_documentation=tech_docs)\n",
        "            augmented_reply = f\"\"\"{question}{docs}\"\"\"\n",
        "            return augmented_reply\n",
        "        else:\n",
        "            return question\n",
        "\n",
        "    def memory(self, question: str):\n",
        "        \"\"\"\n",
        "        Prepares the conversational context (memory) for the language model.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's current question.\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted conversation history with a clear separation between speakers, ready\n",
        "                 for input to the language model.\n",
        "        \"\"\"\n",
        "\n",
        "        dict_history = json.load(open(self.file))\n",
        "        message = Message(\n",
        "            role='user',\n",
        "            content=self.augmented_question(question)\n",
        "        )\n",
        "\n",
        "        if len(dict_history) > 0:\n",
        "            message.update()  # Add the latest message to history\n",
        "            full_chat = json.load(open(self.file))\n",
        "        else:\n",
        "            full_chat = message.new_chat()  # Start a new conversation\n",
        "\n",
        "        if len(full_chat) > self.memory_lenght:\n",
        "            full_chat = [full_chat[0]] + full_chat[-self.memory_lenght:] #limiting the few-shot prompt to fit max_lenght\n",
        "\n",
        "        return '\\n'.join(\n",
        "            [\n",
        "                self.token + reply['role'] + ': ' + reply['content']\n",
        "                for reply in full_chat\n",
        "            ]\n",
        "        ) + self.token + 'assistant:'\n",
        "\n",
        "    def get_answer(self, full_response: str, question: str):\n",
        "        \"\"\"\n",
        "        Extracts the relevant answer from the language model's generated response.\n",
        "\n",
        "        Args:\n",
        "            full_response (str): The complete response generated by the language model.\n",
        "            question (str): The user's original question.\n",
        "\n",
        "        Returns:\n",
        "            str: The extracted answer.\n",
        "        \"\"\"\n",
        "\n",
        "        answer_list = full_response.split(self.token)\n",
        "        pos_list = [\n",
        "            pos\n",
        "            for pos, answer\n",
        "            in enumerate(answer_list)\n",
        "            if question in answer\n",
        "        ]\n",
        "        tokened_answer = answer_list[pos_list[0] + 1]\n",
        "        answer = tokened_answer.split('assistant:')[1]\n",
        "        message = Message(\n",
        "            role='assistant',\n",
        "            content=answer\n",
        "        )\n",
        "        message.update()  # Update conversation history\n",
        "        return answer\n",
        "\n",
        "    def chat(self, question: str, verbose: bool = False):\n",
        "        \"\"\"\n",
        "        Manages the core interaction with the Agent.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's query.\n",
        "            verbose (bool, optional): If True, prints the language model's full response (default: False).\n",
        "\n",
        "        Returns:\n",
        "            str: The Agent's answer to the user's question.\n",
        "        \"\"\"\n",
        "\n",
        "        memory = self.memory(question)  # Build conversational context\n",
        "        full_response = gemma.chat(context=memory)  # Generate response\n",
        "        if verbose:\n",
        "            print(full_response)\n",
        "        answer = self.get_answer(full_response, question)  # Extract the answer\n",
        "        return display(Markdown(answer))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:33:05.627402Z",
          "iopub.execute_input": "2024-04-12T03:33:05.627696Z",
          "iopub.status.idle": "2024-04-12T03:44:38.695255Z",
          "shell.execute_reply.started": "2024-04-12T03:33:05.627673Z",
          "shell.execute_reply": "2024-04-12T03:44:38.694268Z"
        },
        "trusted": true,
        "id": "QgEJMzbUv-Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "r8OX7Xe-soW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tunning data"
      ],
      "metadata": {
        "id": "Vr-em4bmv-T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_qa = PythonQAData.get_qa_data()\n",
        "print(python_qa[1486])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:44:38.696312Z",
          "iopub.execute_input": "2024-04-12T03:44:38.696593Z",
          "iopub.status.idle": "2024-04-12T03:45:11.354921Z",
          "shell.execute_reply.started": "2024-04-12T03:44:38.696569Z",
          "shell.execute_reply": "2024-04-12T03:45:11.353934Z"
        },
        "trusted": true,
        "id": "y18eUG_av-T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "'\\n* Mean lenght of a python common question:',\n",
        "sum([len(q['Answer']) for q in python_qa])/len(python_qa),\n",
        "'\\n* Standard deviation of length a common Python question:',\n",
        "np.std([len(q['Answer']) for q in python_qa])\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T04:20:26.443565Z",
          "iopub.execute_input": "2024-04-12T04:20:26.443928Z",
          "iopub.status.idle": "2024-04-12T04:20:26.478842Z",
          "shell.execute_reply.started": "2024-04-12T04:20:26.443899Z",
          "shell.execute_reply": "2024-04-12T04:20:26.477908Z"
        },
        "trusted": true,
        "id": "cHWxe1jov-T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "'\\n* Mean lenght of a python common answer:',\n",
        "sum([len(q['Question']) for q in python_qa])/len(python_qa),\n",
        "'\\n* Standard deviation of length a common Python question:',\n",
        "np.std([len(q['Question']) for q in python_qa])\n",
        ")"
      ],
      "metadata": {
        "id": "rcx8wheAv-T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_context = PythonQAData.get_fine_tunning_data()\n",
        "print(\n",
        "    'Python Questions loaded:',\n",
        "    len(python_context),\n",
        "    '\\n',\n",
        "    '\\nSample question-answer:\\n',\n",
        "    python_context[-1][:1000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:45:11.383873Z",
          "iopub.execute_input": "2024-04-12T03:45:11.384548Z",
          "iopub.status.idle": "2024-04-12T03:45:29.178375Z",
          "shell.execute_reply.started": "2024-04-12T03:45:11.384511Z",
          "shell.execute_reply": "2024-04-12T03:45:29.177286Z"
        },
        "trusted": true,
        "id": "U3dNp92ev-T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## few-shot data"
      ],
      "metadata": {
        "id": "7bRhz1Dlv-T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_list=[7,6009,12,36,8034,130,141,537,1057,5042]\n",
        "for i, q in enumerate(few_shot_list):\n",
        "    print('_'*100,f'\\n{i}.',python_qa[q])"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-12T03:45:29.179639Z",
          "iopub.execute_input": "2024-04-12T03:45:29.179952Z",
          "iopub.status.idle": "2024-04-12T03:45:29.186521Z",
          "shell.execute_reply.started": "2024-04-12T03:45:29.179926Z",
          "shell.execute_reply": "2024-04-12T03:45:29.185603Z"
        },
        "trusted": true,
        "id": "U2LrBsGZv-T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG data"
      ],
      "metadata": {
        "id": "YQD7LZX5v-T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "peps = Peps()\n",
        "peps_corpus = peps.scrape()\n",
        "print(\n",
        "    'corpus lenght:',\n",
        "    len(peps_corpus),\n",
        "    '\\nsection <n> lenght:',\n",
        "    len(peps_corpus[0]),\n",
        "    '\\nsample text:\\n\\n',\n",
        "    peps_corpus[10][1306:2000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:45:29.187667Z",
          "iopub.execute_input": "2024-04-12T03:45:29.187985Z",
          "iopub.status.idle": "2024-04-12T03:47:30.309933Z",
          "shell.execute_reply.started": "2024-04-12T03:45:29.187961Z",
          "shell.execute_reply": "2024-04-12T03:47:30.309022Z"
        },
        "trusted": true,
        "id": "2EjLkhW4v-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "DJvR58fcsoW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the embeder class"
      ],
      "metadata": {
        "id": "QCTBjj0Zv-T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "embeder = Embeder()\n",
        "vec = embeder.run([\"Beautiful is better than ugly.\"])[0]\n",
        "print(len(vec))\n",
        "vec[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:47:30.311186Z",
          "iopub.execute_input": "2024-04-12T03:47:30.311896Z",
          "iopub.status.idle": "2024-04-12T03:47:32.598957Z",
          "shell.execute_reply.started": "2024-04-12T03:47:30.31186Z",
          "shell.execute_reply": "2024-04-12T03:47:32.598068Z"
        },
        "trusted": true,
        "id": "l3iVJP-8v-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Retriver"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T03:39:46.166323Z",
          "iopub.execute_input": "2024-04-09T03:39:46.167289Z",
          "iopub.status.idle": "2024-04-09T03:39:46.171461Z",
          "shell.execute_reply.started": "2024-04-09T03:39:46.167254Z",
          "shell.execute_reply": "2024-04-09T03:39:46.170554Z"
        },
        "id": "yOskowKev-T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question = \"\"\"\n",
        "Develop a Python code snippet to print the diamond structure with the specified number of rows.\n",
        "The program should follow the Fibonacci sequence for the number of characters per row\n",
        "and validate input to ensure it is an odd number.\n",
        "\"\"\"\n",
        "retriever.query(\n",
        "    question,\n",
        "    k=100,\n",
        "    threshold=15\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:47:32.600323Z",
          "iopub.execute_input": "2024-04-12T03:47:32.600766Z",
          "iopub.status.idle": "2024-04-12T03:47:32.632863Z",
          "shell.execute_reply.started": "2024-04-12T03:47:32.600726Z",
          "shell.execute_reply": "2024-04-12T03:47:32.63199Z"
        },
        "trusted": true,
        "id": "De2WP_Jdv-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question = \"\"\"\n",
        "what PEPs Introduces the concept of decorators, a powerful feature in Python.\n",
        "\"\"\"\n",
        "retriever.query(question,\n",
        "    k=100,\n",
        "    threshold=15\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:47:32.725843Z",
          "iopub.execute_input": "2024-04-12T03:47:32.726152Z",
          "iopub.status.idle": "2024-04-12T03:47:32.75525Z",
          "shell.execute_reply.started": "2024-04-12T03:47:32.726123Z",
          "shell.execute_reply": "2024-04-12T03:47:32.75442Z"
        },
        "trusted": true,
        "id": "GWuFJks0v-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question = \"\"\"\n",
        "what is python??\n",
        "\"\"\"\n",
        "retriever.query(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:47:32.756408Z",
          "iopub.execute_input": "2024-04-12T03:47:32.756663Z",
          "iopub.status.idle": "2024-04-12T03:47:32.784035Z",
          "shell.execute_reply.started": "2024-04-12T03:47:32.756642Z",
          "shell.execute_reply": "2024-04-12T03:47:32.783251Z"
        },
        "trusted": true,
        "id": "Qy3m_pkyv-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "R3xhjyW9soW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading few-shot to the memory"
      ],
      "metadata": {
        "id": "YZWzUxlGv-T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'data/history.json'\n",
        "message = Message(\n",
        "    role='sistem',\n",
        "    content='foo'\n",
        ")\n",
        "init_chat = [message.system_reply()]\n",
        "json.dump(init_chat, open(file, 'w'))\n",
        "for i, q in enumerate(few_shot_list):\n",
        "    sample = python_qa[q]\n",
        "    mesage = Message(\n",
        "        role='user',\n",
        "        content=sample['Question']\n",
        "    )\n",
        "    reply = mesage.update()\n",
        "    mesage = Message(\n",
        "        role='assitant',\n",
        "        content=sample['Answer']\n",
        "    )\n",
        "    reply = mesage.update()\n",
        "\n",
        "history = json.load(open(file))\n",
        "history"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-12T03:47:32.785042Z",
          "iopub.execute_input": "2024-04-12T03:47:32.785314Z",
          "iopub.status.idle": "2024-04-12T03:47:32.807345Z",
          "shell.execute_reply.started": "2024-04-12T03:47:32.785274Z",
          "shell.execute_reply": "2024-04-12T03:47:32.806576Z"
        },
        "_kg_hide-output": true,
        "trusted": true,
        "id": "nIDpPtEjv-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Agent system memory"
      ],
      "metadata": {
        "id": "txTcJjwZv-T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent()\n",
        "print(agent.memory(\"\"\"\n",
        "what PEPs Introduces the concept of decorators, a powerful feature in Python.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-12T03:47:32.80842Z",
          "iopub.execute_input": "2024-04-12T03:47:32.808726Z",
          "iopub.status.idle": "2024-04-12T03:47:32.832364Z",
          "shell.execute_reply.started": "2024-04-12T03:47:32.808697Z",
          "shell.execute_reply": "2024-04-12T03:47:32.831533Z"
        },
        "_kg_hide-output": true,
        "trusted": true,
        "id": "vQf7nc0Cv-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma Python Chat"
      ],
      "metadata": {
        "id": "Cc_e_HAgsoW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'what is python??'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:47:32.833324Z",
          "iopub.execute_input": "2024-04-12T03:47:32.833575Z",
          "iopub.status.idle": "2024-04-12T03:50:08.373767Z",
          "shell.execute_reply.started": "2024-04-12T03:47:32.833554Z",
          "shell.execute_reply": "2024-04-12T03:50:08.372866Z"
        },
        "trusted": true,
        "id": "qGMKErpjv-T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'who are you???'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:50:08.378256Z",
          "iopub.execute_input": "2024-04-12T03:50:08.378554Z",
          "iopub.status.idle": "2024-04-12T03:51:06.433057Z",
          "shell.execute_reply.started": "2024-04-12T03:50:08.378529Z",
          "shell.execute_reply": "2024-04-12T03:51:06.431861Z"
        },
        "trusted": true,
        "id": "P0Juudm7v-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'what is python and how long is it????'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:51:06.434647Z",
          "iopub.execute_input": "2024-04-12T03:51:06.435384Z",
          "iopub.status.idle": "2024-04-12T03:52:19.677203Z",
          "shell.execute_reply.started": "2024-04-12T03:51:06.435349Z",
          "shell.execute_reply": "2024-04-12T03:52:19.676334Z"
        },
        "trusted": true,
        "id": "ppjc7t3mv-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'my name is juan nice to meet you!!!'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:52:19.678531Z",
          "iopub.execute_input": "2024-04-12T03:52:19.67882Z",
          "iopub.status.idle": "2024-04-12T03:52:31.942936Z",
          "shell.execute_reply.started": "2024-04-12T03:52:19.678795Z",
          "shell.execute_reply": "2024-04-12T03:52:31.942037Z"
        },
        "trusted": true,
        "id": "k-TjZFQrv-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'who are you?!'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:52:31.944103Z",
          "iopub.execute_input": "2024-04-12T03:52:31.944428Z",
          "iopub.status.idle": "2024-04-12T03:52:52.196214Z",
          "shell.execute_reply.started": "2024-04-12T03:52:31.944403Z",
          "shell.execute_reply": "2024-04-12T03:52:52.195077Z"
        },
        "trusted": true,
        "id": "qVZnth7nv-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'what is my name??'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:52:52.197701Z",
          "iopub.execute_input": "2024-04-12T03:52:52.19805Z",
          "iopub.status.idle": "2024-04-12T03:53:13.072638Z",
          "shell.execute_reply.started": "2024-04-12T03:52:52.198017Z",
          "shell.execute_reply": "2024-04-12T03:53:13.071762Z"
        },
        "trusted": true,
        "id": "GIWzNa-Rv-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'Develop a Python code snippet to print the diamond structure with the specified number of rows. The program should follow the Fibonacci sequence for the number of characters per row and validate input to ensure it is an odd number.'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:53:13.073819Z",
          "iopub.execute_input": "2024-04-12T03:53:13.074103Z",
          "iopub.status.idle": "2024-04-12T03:55:24.734618Z",
          "shell.execute_reply.started": "2024-04-12T03:53:13.074078Z",
          "shell.execute_reply": "2024-04-12T03:55:24.733768Z"
        },
        "trusted": true,
        "id": "OdOWjgM0v-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'Create a Python dictionary that stores square roots of numbers from 1 to 15 and cube roots of numbers from 16 to 30, ensuring to keep a precision up to two decimal places.'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T03:55:24.735854Z",
          "iopub.execute_input": "2024-04-12T03:55:24.73624Z",
          "iopub.status.idle": "2024-04-12T03:57:35.085867Z",
          "shell.execute_reply.started": "2024-04-12T03:55:24.736207Z",
          "shell.execute_reply": "2024-04-12T03:57:35.084946Z"
        },
        "trusted": true,
        "id": "yTvbTgmmv-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'give me the python class that do a list comprehension format'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T04:03:51.577591Z",
          "iopub.execute_input": "2024-04-12T04:03:51.577989Z",
          "iopub.status.idle": "2024-04-12T04:06:40.01186Z",
          "shell.execute_reply.started": "2024-04-12T04:03:51.57796Z",
          "shell.execute_reply": "2024-04-12T04:06:40.010944Z"
        },
        "trusted": true,
        "id": "28zdU7_vv-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'tell me a python joke!'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T04:08:12.539575Z",
          "iopub.execute_input": "2024-04-12T04:08:12.539935Z",
          "iopub.status.idle": "2024-04-12T04:11:01.824677Z",
          "shell.execute_reply.started": "2024-04-12T04:08:12.539906Z",
          "shell.execute_reply": "2024-04-12T04:11:01.823812Z"
        },
        "trusted": true,
        "id": "MhsomaX7v-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'how to print on hello world?'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T04:15:27.892207Z",
          "iopub.execute_input": "2024-04-12T04:15:27.892638Z",
          "iopub.status.idle": "2024-04-12T04:18:17.037221Z",
          "shell.execute_reply.started": "2024-04-12T04:15:27.892607Z",
          "shell.execute_reply": "2024-04-12T04:18:17.036346Z"
        },
        "trusted": true,
        "id": "vJLftJjev-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'what is the zen of python?'\n",
        "print(agent.chat(question))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T04:58:11.618008Z",
          "iopub.execute_input": "2024-04-12T04:58:11.618739Z",
          "iopub.status.idle": "2024-04-12T05:01:01.337998Z",
          "shell.execute_reply.started": "2024-04-12T04:58:11.618705Z",
          "shell.execute_reply": "2024-04-12T05:01:01.337085Z"
        },
        "trusted": true,
        "id": "WIXlKtwCv-T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = json.load(open(file))\n",
        "history"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-12T04:07:11.019206Z",
          "iopub.execute_input": "2024-04-12T04:07:11.01957Z",
          "iopub.status.idle": "2024-04-12T04:07:11.034196Z",
          "shell.execute_reply.started": "2024-04-12T04:07:11.019543Z",
          "shell.execute_reply": "2024-04-12T04:07:11.033186Z"
        },
        "trusted": true,
        "id": "h9DMATPCv-T2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}