{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 726715,
          "sourceType": "datasetVersion",
          "datasetId": 262
        },
        {
          "sourceId": 11384,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 6216
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Gemma Python Chat (GPC)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/Gemma_Python_Chat_(GPC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240404%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240404T061645Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da886914e40c0cb34745087de0dbc21cda4e3c814bde561617c0673740e502364117e23e5f43029f75dc8dbfe1bc5f1410c765400364f85ed170d06788a2acff56a892da9894fd3af3ef4051e88c91093eda3f21e516a024b33a86295fbe3667de6533b0ad782c53ed14c1a0a0fc5f9c5eea75907717ff211b0459421003d883225cad9cb893b1eb3431e0040446a5729e930203bb2f5f19ccbdd167c2d907b248cab6c0a98c39cff3fe45295ff0b661c27ec99cf133558aa695f1344337992cce2b3c94df68a31bd2341a02d154f59a5648dd415363a85fafba066ad8cfdc9bac7111b4927038bb26e3fb5265999a3ab823f4b5641c35e9769b188869671c542,pythonquestions:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F262%2F726715%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240404%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240404T061645Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1e927f5cf108286d4500752cb64d1956097d02a59ebd6e835727ad1a819410fa10a88a603e79c188d13a5f5b54fdcaa16ec101d823a014bee0ce206c21d66945d16d3520735110412c2b10b15df0efe8d51fd9ec0f0630e9b4489d3b1e148502ce46d7f2f7edf4bae66704acb247c49d02c9036f61b1b477223c15a706b94d54f4309b85f771b32c48db068d67a3e9cf582a69d6e33a14a35fae8d810316832e105fef0b0b5c71020bbba2af7405cb500fd34af3459091839b6efb5695a055ed4effd13f4c677ca2613b903057c64839ca4bcd78fdab85ababc5c30325d38a749416e0f2bfdcb3a7a6c1aafd5bd44d9a5f15b230e02479a2ed0dcbf6622cd098,gemma/transformers/2b/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F6216%2F11384%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240404%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240404T061645Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D57126f0c83f0076b180b2e1341b5eea217ee5b5da42c89c3e39e4344ef45212bf0e8cbe90fcae310d79ac1dd1a22573202b2d3605b873205b5eadaeeddbacb873b40136e9a3b9e2d724d174f2e44843a66844a5f7180ae4e155f2f61d9e92ec1405d0694723d32a8496a5beba2ba50b96811f7eaaf584f7807d07290ccb8bf7d69ac3afb954023d6f44a7cf4abc8933d65c83a1e3135ff35070d19b74dd39d7875ea36a3ca02f326fc0e48db7d6a45faa432e4a2f26ab6965b1ac021ca97e90daef68391df2bb68dc92496d447165f9826eceb42bbd129e728674fca0759c28f0dc72aed5ea89474d7576c9e7b92e6213ab37a77ded83932fdcdd05d8e84144d'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "h128mpxawLyk"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/GPC_(Gemma_Python_Chat).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP ü§ñ Gemma Python Chatbot"
      ],
      "metadata": {
        "id": "Qx8hbihKsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/8xZNc32/Gemma.png)"
      ],
      "metadata": {
        "id": "B0RdGIyrsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gemma Python Chatbot üöÄüöÄ help you to answer common questions about the üêç Python programming language, powered by [Gemma 2B IT](https://blog.google/technology/developers/gemma-open-models/) updated with the Python Enhancement Proposal ([PEPs](https://peps.python.org/)) documentation using a Retrival-Augmented Generation ([RAG](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)) sponsored by a Chroma vectorial database using the open source embeddings [hkunlp/instructor-large](https://huggingface.co/hkunlp/instructor-large) of 768 entries, orchested by [langchaing](https://python.langchain.com/docs/use_cases/chatbots/) that is a framework that let you use differen models y les code changes"
      ],
      "metadata": {
        "id": "5C9kSlAVsoW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "5GhJQPvLsoW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "# !pip install ydata-profiling\n",
        "!pip install langchain\n",
        "#!pip install pydantic\n",
        "!pip install sentence-transformers\n",
        "!pip install InstructorEmbedding"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-04T03:17:26.210459Z",
          "iopub.execute_input": "2024-04-04T03:17:26.211704Z",
          "iopub.status.idle": "2024-04-04T03:18:56.7299Z",
          "shell.execute_reply.started": "2024-04-04T03:17:26.211655Z",
          "shell.execute_reply": "2024-04-04T03:18:56.728684Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "eJR3_LJ1wLyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##!pip install langchain-core==0.1.31\n",
        "#!pip install chromadb==0.3.26\n",
        "#!pip install ydata-profiling==4.6.1\n",
        "#!pip install langchain==0.0.345\n",
        "#!pip install pydantic==1.10.14\n",
        "#!pip install sentence-transformers==2.6.1\n",
        "#!pip install InstructorEmbedding==1.0.1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T03:18:56.732368Z",
          "iopub.execute_input": "2024-04-04T03:18:56.732835Z",
          "iopub.status.idle": "2024-04-04T03:18:56.738321Z",
          "shell.execute_reply.started": "2024-04-04T03:18:56.732792Z",
          "shell.execute_reply": "2024-04-04T03:18:56.737132Z"
        },
        "trusted": true,
        "id": "vlsWVRDkwLyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "# variables\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "# model\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "# data\n",
        "from datasets import load_dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# embeddings\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "# vector database\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "Vq5ZFhX7soW4",
        "execution": {
          "iopub.status.busy": "2024-04-04T03:18:56.739774Z",
          "iopub.execute_input": "2024-04-04T03:18:56.740086Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "m_GPx-U-wLyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions and answers data"
      ],
      "metadata": {
        "id": "eN9RM-YawLyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pythonQAData:\n",
        "    \"\"\"\n",
        "    Processes data from Questions and Answers CSV files to provide a structured Q&A format.\n",
        "\n",
        "    Attributes:\n",
        "        questions_path (str): Path to the Questions CSV file\n",
        "        answers_path (str): Path to the Answers CSV file\n",
        "        tags_path (str): Path to the tags CSV file\n",
        "\n",
        "    Methods:\n",
        "        load_data(): Loads the CSV data into DataFrames.\n",
        "        merge(): Cleans, merges, and formats the question and answer data.\n",
        "        get_formatted_qa(): Returns a list of formatted question-answer strings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.questions_path = '../input/pythonquestions/Questions.csv'\n",
        "        self.answers_path = '../input/pythonquestions/Answers.csv'\n",
        "        self.tags_path = '../input/pythonquestions/Tags.csv'\n",
        "        self.regex = r\"<\\/?[\\w\\s]*>\"\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Loads Questions and Answers data from CSV files.\"\"\"\n",
        "        df_questions = pd.read_csv(\n",
        "            self.questions_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Score',\n",
        "                'Title'\n",
        "            ]\n",
        "        )\n",
        "        df_answers = pd.read_csv(\n",
        "            self.answers_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'ParentId',\n",
        "                'Score',\n",
        "                'Body'\n",
        "            ]\n",
        "        )\n",
        "        df_tags = pd.read_csv(\n",
        "            self.tags_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Tag'\n",
        "            ]\n",
        "        )\n",
        "        return df_questions, df_answers, df_tags\n",
        "\n",
        "\n",
        "    def qa_data(self):\n",
        "        \"\"\"Cleans, merges, and formats the question and answer data.\"\"\"\n",
        "        df_questions, df_answers, df_tags = self.load_data()\n",
        "        # Rename\n",
        "        df_questions.rename(\n",
        "            columns={\n",
        "                'Title': 'Question',\n",
        "                'Score': 'question_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        df_answers.rename(\n",
        "            columns={\n",
        "                'Body': 'Answer',\n",
        "                'ParentId':'Id',\n",
        "                'Score': 'answer_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        # Filter by score\n",
        "        df_questions = df_questions[df_questions['question_score'] > 5].copy()\n",
        "        # Sort and deduplicate answers\n",
        "        df_answers = df_answers.sort_values(\n",
        "            'answer_score',\n",
        "            ascending=False\n",
        "        ).drop_duplicates(subset=['Id'])\n",
        "        # Merge\n",
        "        df_qa = df_questions.merge(\n",
        "            df_answers,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        ).merge(\n",
        "            df_tags,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        )\n",
        "        # filter for python  questions\n",
        "        df_qa = df_qa[df_qa['answer_score'] > 5].copy()\n",
        "        df_qa = df_qa[df_qa['Tag']=='python'].copy()\n",
        "        df_qa['Answer'] = df_qa['Answer'].apply(\n",
        "            lambda x: re.sub(\n",
        "                self.regex,\n",
        "                \"\",\n",
        "                x\n",
        "            )\n",
        "        )\n",
        "        return df_qa\n",
        "\n",
        "    def get_fine_tunning_data(self):\n",
        "        \"\"\"Returns a list of formatted user-assistant strings.\"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        data = [\n",
        "            f\"<-change-of-interlocutor->user:\\n{row['Question']}\\n<-change-of-interlocutor->assistant:\\n{row['Answer']}\"\n",
        "            for index, row\n",
        "            in df_qa_data.iterrows()\n",
        "        ]\n",
        "        return data\n",
        "    def get_qa_data(self):\n",
        "        \"\"\"Returns a list of records dictionaries \"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        data = df_qa_data[\n",
        "            [\n",
        "                'Question',\n",
        "                'Answer'\n",
        "            ]\n",
        "        ].to_dict(orient='records')\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "bSU-ohmssoW5",
        "execution": {
          "iopub.status.busy": "2024-04-04T05:25:09.079827Z",
          "iopub.execute_input": "2024-04-04T05:25:09.080594Z",
          "iopub.status.idle": "2024-04-04T05:25:09.097851Z",
          "shell.execute_reply.started": "2024-04-04T05:25:09.080558Z",
          "shell.execute_reply": "2024-04-04T05:25:09.096654Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Enhancement Proposals data"
      ],
      "metadata": {
        "id": "_7zrWMLCwLyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Peps:\n",
        "    \"\"\"\n",
        "    Scrapes Python Enhancement Proposals (PEPs) from https://peps.python.org/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initiates the scraping process.\"\"\"\n",
        "        self.base_url = 'https://peps.python.org/'\n",
        "\n",
        "    def scraper(self, url):\n",
        "        \"\"\"\n",
        "        Fetches the HTML content of a given URL.\n",
        "\n",
        "        Args:\n",
        "            url (str): The URL to fetch.\n",
        "\n",
        "        Returns:\n",
        "            BeautifulSoup: A BeautifulSoup object representing the parsed HTML.\n",
        "        \"\"\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    def _fetch_pep_links(self):\n",
        "        \"\"\"Fetches links to individual PEP pages (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of PEP URLs.\n",
        "        \"\"\"\n",
        "        soup = self.scraper(self.base_url)\n",
        "        return list(\n",
        "            set(\n",
        "                [\n",
        "                    self.base_url + ref['href']\n",
        "                    for ref in soup.find_all('a', class_='pep reference internal')\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _download_peps(self):\n",
        "        \"\"\"Downloads the content of individual PEPs (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of BeautifulSoup objects representing individual PEPs.\n",
        "        \"\"\"\n",
        "        pep_links = self._fetch_pep_links()\n",
        "        return [self.scraper(pep_link) for pep_link in pep_links]\n",
        "\n",
        "    def scrape(self):\n",
        "        \"\"\"Extracts the relevant text content from each PEP.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of text strings, each representing the content of a PEP.\n",
        "        \"\"\"\n",
        "        pep_soups = self._download_peps()\n",
        "        return [\n",
        "            '\\n'.join([word.text.replace('\"\"\"',\"'\") for word in soup.find_all('section')])\n",
        "            for soup in pep_soups\n",
        "        ]\n",
        "\n",
        "    def jsonl_format(self):\n",
        "        \"\"\"Format the spcraped data to a jsonl format is it alist of dictionaries\n",
        "        Returns:\n",
        "            list: A list of dictionaries with the following:\n",
        "                page_content: The content of each section scraped\n",
        "                source: The linke where it was scraped\n",
        "        \"\"\"\n",
        "        source_links = self._fetch_pep_links()\n",
        "        pep_content = self.scrape()\n",
        "        pep_data = dict(zip(source_links,pep_content))\n",
        "        return [\n",
        "            {\n",
        "                'text': value,\n",
        "                'source': key\n",
        "            }\n",
        "            for key, value in pep_data.items()\n",
        "        ]"
      ],
      "metadata": {
        "trusted": true,
        "id": "fgP86R4UwLyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "CF53bxyLwLyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeder:\n",
        "    \"\"\"\n",
        "    Creates embeddings (numerical representations) of text documents for various tasks like semantic search and\n",
        "    similarity comparison. Provides flexibility to choose between a large model for more comprehensive\n",
        "    embeddings or a smaller, faster model.\n",
        "\n",
        "    Attributes:\n",
        "\n",
        "    large (bool): Flag indicating whether to use the large embedding model (default: True).\n",
        "    model (str): Name of the embedding model to use.\n",
        "    device (str): Device to use for computations (default: \"cuda\").\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            large: bool = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        init(self, large: bool = True) -> None\n",
        "            Initializes the class with the specified model size preference.\n",
        "        \"\"\"\n",
        "        self.large = large\n",
        "        if self.large:\n",
        "            self.model = \"hkunlp/instructor-large\"\n",
        "            self.device: str = \"cuda\"\n",
        "        else:\n",
        "            self.model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "    def instructor(self):\n",
        "        if self.large:\n",
        "            embed = HuggingFaceInstructEmbeddings(\n",
        "                model_name=self.model,\n",
        "                model_kwargs={\"device\": self.device}\n",
        "            )\n",
        "        else:\n",
        "            embed = SentenceTransformerEmbeddings(\n",
        "                model_name=self.model\n",
        "            )\n",
        "        return embed\n",
        "\n",
        "    def run(\n",
        "            self,\n",
        "            docs_list: list,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        run(self, docs_list: list) -> List[List[float]]:\n",
        "            Generates embeddings for the provided list of documents.\n",
        "            Returns a list of lists, where each inner list represents the embedding vector for a document.\n",
        "        \"\"\"\n",
        "        if docs_list is None:\n",
        "            docs_list = ['']\n",
        "        embed = self.instructor()\n",
        "\n",
        "        return embed.embed_documents(docs_list)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UXeMWXLPwLyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "nMVhmZGJwLyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_config = [{\n",
        "    \"prompt_id\": \"0.0.1\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "}]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T03:31:14.039481Z",
          "iopub.execute_input": "2024-04-04T03:31:14.039973Z",
          "iopub.status.idle": "2024-04-04T03:31:14.047054Z",
          "shell.execute_reply.started": "2024-04-04T03:31:14.03994Z",
          "shell.execute_reply": "2024-04-04T03:31:14.045991Z"
        },
        "trusted": true,
        "id": "ZdbE7ftnwLyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T05:03:58.746391Z",
          "iopub.execute_input": "2024-04-04T05:03:58.746828Z",
          "iopub.status.idle": "2024-04-04T05:03:58.751668Z",
          "shell.execute_reply.started": "2024-04-04T05:03:58.746794Z",
          "shell.execute_reply": "2024-04-04T05:03:58.750527Z"
        },
        "id": "MdJqhGxDwLyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Settings():\n",
        "    def __init__(self):\n",
        "        self.user_secrets = UserSecretsClient()\n",
        "        self.CHUNK_SIZE = self.user_secrets.get_secret(\"CHUNK_SIZE\")\n",
        "        self.CHUNK_OVERLAP = self.user_secrets.get_secret(\"CHUNK_OVERLAP\")\n",
        "        self.CHROMA_NAME_INDEX = self.user_secrets.get_secret(\"CHROMA_NAME_INDEX\")\n",
        "        self.K = self.user_secrets.get_secret(\"K\")\n",
        "        self.NN_THRESHOLD = self.user_secrets.get_secret(\"NN_THRESHOLD\")\n",
        "        self.PROMPT_ID = self.user_secrets.get_secret(\"PROMPT_ID\")\n",
        "        self.prompt = [prompt for prompt in prompt_config if prompt['prompt_id']==self.PROMPT_ID][0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T03:50:04.694077Z",
          "iopub.execute_input": "2024-04-04T03:50:04.694475Z",
          "iopub.status.idle": "2024-04-04T03:50:04.702Z",
          "shell.execute_reply.started": "2024-04-04T03:50:04.694446Z",
          "shell.execute_reply": "2024-04-04T03:50:04.700964Z"
        },
        "trusted": true,
        "id": "syKgCwYlwLys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever"
      ],
      "metadata": {
        "id": "1DyUiayewLys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeder = Embeder()\n",
        "settings = Settings()\n",
        "class Retriever:\n",
        "    \"\"\" retriever\n",
        "    Retrieves the embeddings from a vectorial database\n",
        "    \"\"\"\n",
        "    def __init__(self, data: list[dict]):\n",
        "        \"\"\" initialize the retriever\n",
        "        \"\"\"\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=int(settings.CHUNK_SIZE),\n",
        "            length_function=len,\n",
        "            chunk_overlap=int(settings.CHUNK_OVERLAP)\n",
        "        )\n",
        "        self.index_name = settings.CHROMA_NAME_INDEX\n",
        "        self.data = data\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"\n",
        "        Loads documents from the initialized data and returns them as a list of Document objects.\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "        for obj in self.data:\n",
        "            page_content = obj.get(\"text\", \"\")\n",
        "            metadata = {\n",
        "                \"source\": obj.get(\"source\", \"local\")\n",
        "            }\n",
        "            documents.append(Document(page_content=page_content, metadata=metadata))\n",
        "        return documents\n",
        "\n",
        "    def set(self):\n",
        "        \"\"\"\n",
        "         update the vectorial database\n",
        "        \"\"\"\n",
        "        data = self.load()\n",
        "        splitter = self.text_splitter\n",
        "        documents = splitter.split_documents(data)\n",
        "        instructor = embeder.instructor()\n",
        "        vector_db = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=instructor,\n",
        "            persist_directory=self.index_name\n",
        "        )\n",
        "        vector_db.persist()\n",
        "\n",
        "    def query(\n",
        "            self,\n",
        "            message: str,\n",
        "            k: int = int(settings.K),\n",
        "            threshold: float = float(settings.NN_THRESHOLD)\n",
        "    ):\n",
        "        \"\"\" retrieve the 2 more similar text chunks based on the message given\n",
        "        \"\"\"\n",
        "        vectorial_db = Chroma(\n",
        "            embedding_function=embeder.instructor(),\n",
        "            persist_directory=self.index_name\n",
        "        )\n",
        "        res = vectorial_db.similarity_search_with_score(message, k=k)\n",
        "        return [vector[0].page_content for vector in res if vector[1] < threshold]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T03:31:53.642778Z",
          "iopub.execute_input": "2024-04-04T03:31:53.643491Z",
          "iopub.status.idle": "2024-04-04T03:31:54.127495Z",
          "shell.execute_reply.started": "2024-04-04T03:31:53.643434Z",
          "shell.execute_reply": "2024-04-04T03:31:54.126605Z"
        },
        "trusted": true,
        "id": "kH5Fs_BfwLys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Message"
      ],
      "metadata": {
        "id": "BUYT2x_xwLys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "settings = Settings()\n",
        "class Message:\n",
        "    def __init__(\n",
        "            self,\n",
        "            role: str,\n",
        "            content: str,\n",
        "            timestamp: str = int(time.time()),\n",
        "            prompt_id: str = settings.PROMPT_ID\n",
        "    ):\n",
        "        self.reply_id = str(uuid.uuid4())\n",
        "        self.role = role\n",
        "        self.content = content\n",
        "        self.timestamp = timestamp\n",
        "        self.file = 'data/history.json'\n",
        "        if not os.path.exists(self.file):\n",
        "            json.dump([], open(self.file, 'w'))\n",
        "\n",
        "    def reply(self):\n",
        "        return {\n",
        "            'reply_id': self.reply_id,\n",
        "            'role': self.role,\n",
        "            'content': self.content,\n",
        "            'timestamp': self.timestamp\n",
        "        }\n",
        "\n",
        "    def system_reply(self):\n",
        "        prompt = settings.prompt\n",
        "        return {\n",
        "            'reply_id': self.reply_id,\n",
        "            'role': 'system',\n",
        "            'content': prompt['system'],\n",
        "            'timestamp': self.timestamp\n",
        "        }\n",
        "    def new_chat(self):\n",
        "        init_chat = [self.system_reply, self.reply]\n",
        "        json.dump(init_chat, open(self.file, 'w'))\n",
        "        return init_chat\n",
        "    def update(self):\n",
        "        dict_history = json.load(open(self.file))\n",
        "        dict_history.append(self.reply())\n",
        "        json.dump(dict_history, open(self.file, 'w'))\n",
        "    def restart_history(self):\n",
        "        json.dump([], open(self.file, 'w'))"
      ],
      "metadata": {
        "id": "tZ082Q90wLys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Gemma"
      ],
      "metadata": {
        "id": "UaGU4mJBwLys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gemma:\n",
        "    def __init__(self):\n",
        "        self.prompt = settings.prompt\n",
        "        self.model_checkpoint = self.prompt['model']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
        "        self.gemma = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_checkpoint,\n",
        "            torch_dtype=torch.float16\n",
        "        ).cuda()\n",
        "    def chat(self, context: str):\n",
        "        # Use the model\n",
        "        input_text = context\n",
        "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        input_ids = {k: v.to(\"cuda\") for k, v in input_ids.items()}  if torch.cuda.is_available() else {k: v.to(\"cpu\")}\n",
        "        outputs = self.gemma.generate(**input_ids, max_length=self.prompt['max_length'])\n",
        "        return self.tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "id": "qbyyUM2swLys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expert Python Agent/Assistant"
      ],
      "metadata": {
        "id": "KYZWQYaKwLys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = Retriever(data)\n",
        "gemma = Gemma()\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self):\n",
        "        self.prompt = settings.prompt\n",
        "        self.file = 'data/history.json'\n",
        "        if not os.path.exists(self.file):\n",
        "            json.dump([], open(self.file, 'w'))\n",
        "    def augmented_question(self, question: str):\n",
        "        technical_documentation = retriever.query(question)\n",
        "        if len(technical_documentation)>0:\n",
        "            docs = self.prompt['technical_documentation']\n",
        "            technical_documentation = '\\n* '.join(technical_documentation)\n",
        "            technical_documentation = technical_documentation[0]\n",
        "            docs = docs.format(technical_documentation=technical_documentation)\n",
        "            augmented_reply = f\"\"\"{question}{docs}\"\"\"\n",
        "            return augmented_reply\n",
        "        else:\n",
        "            return question\n",
        "    def memory(self, question: str):\n",
        "        dict_history = json.load(open(self.file))\n",
        "        message = Message(\n",
        "                role='user',\n",
        "                content=self.augmented_question(question)\n",
        "            )\n",
        "        if len(dict_history)>0:\n",
        "            message.update()\n",
        "            full_chat = json.load(open(self.file))\n",
        "        else:\n",
        "            full_chat = message.new_chat()\n",
        "        return '\\n'.join(\n",
        "            [\n",
        "                '\\n<-change-of-interlocutor->'+reply['role']+': '+reply['content']\n",
        "                for reply\n",
        "                in full_chat\n",
        "            ]\n",
        "        )+'\\n<-change-of-interlocutor->assistant:'\n",
        "    def chat(self, question: str):\n",
        "        memory = self.memory(question)\n",
        "        full_answer = gemma.chat(context=memory)\n",
        "        print(full_answer)\n",
        "        answer_list = full_answer.split('<-change-of-interlocutor->')\n",
        "        pos_list = [\n",
        "            pos\n",
        "            for\n",
        "            pos, answer\n",
        "            in enumerate(answer_list)\n",
        "            if question in answer\n",
        "        ]\n",
        "        answer = answer_list[pos_list[0]+1]\n",
        "        return answer\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T06:10:20.879227Z",
          "iopub.execute_input": "2024-04-04T06:10:20.880064Z",
          "iopub.status.idle": "2024-04-04T06:10:27.675946Z",
          "shell.execute_reply.started": "2024-04-04T06:10:20.880022Z",
          "shell.execute_reply": "2024-04-04T06:10:27.67492Z"
        },
        "trusted": true,
        "id": "9K9aOWEDwLys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "r8OX7Xe-soW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fine-tunning data"
      ],
      "metadata": {
        "id": "N_NOUHaCwLyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_qa = PythonQAData.get_qa_data()\n",
        "print(python_qa[1486])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T05:26:56.580063Z",
          "iopub.execute_input": "2024-04-04T05:26:56.58051Z",
          "iopub.status.idle": "2024-04-04T05:27:15.158737Z",
          "shell.execute_reply.started": "2024-04-04T05:26:56.580475Z",
          "shell.execute_reply": "2024-04-04T05:27:15.157684Z"
        },
        "trusted": true,
        "id": "95Qx_Al4wLyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_context = PythonQAData.get_fine_tunning_data()\n",
        "print(\n",
        "    'Python Questions loaded:',\n",
        "    len(python_context),\n",
        "    '\\n',\n",
        "    '\\nSample question-answer:\\n',\n",
        "    python_context[-1][:1000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T03:34:57.730787Z",
          "iopub.execute_input": "2024-04-04T03:34:57.731226Z",
          "iopub.status.idle": "2024-04-04T03:35:42.339181Z",
          "shell.execute_reply.started": "2024-04-04T03:34:57.731187Z",
          "shell.execute_reply": "2024-04-04T03:35:42.338181Z"
        },
        "trusted": true,
        "id": "5HdbmkZbwLyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## few-shot data"
      ],
      "metadata": {
        "id": "-8xy_qcmwLyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_list=[7,6009,12,36,8034,130,141,537,1057,5042]\n",
        "for i, q in enumerate(few_shot_list):\n",
        "    print('_'*100,f'\\n{i}.',python_qa[q])"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-04T05:29:20.99811Z",
          "iopub.execute_input": "2024-04-04T05:29:20.998584Z",
          "iopub.status.idle": "2024-04-04T05:29:21.006764Z",
          "shell.execute_reply.started": "2024-04-04T05:29:20.998547Z",
          "shell.execute_reply": "2024-04-04T05:29:21.005491Z"
        },
        "trusted": true,
        "id": "A5hCMowawLyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG data"
      ],
      "metadata": {
        "id": "FxFp53iqwLyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "peps = Peps()\n",
        "peps_corpus = peps.scrape()\n",
        "print(\n",
        "    'corpus lenght:',\n",
        "    len(peps_corpus),\n",
        "    '\\nsection <n> lenght:',\n",
        "    len(peps_corpus[0]),\n",
        "    '\\nsample text:\\n\\n',\n",
        "    peps_corpus[10][1306:2000]\n",
        ")"
      ],
      "metadata": {
        "id": "Z70gd-KhwLyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "peps = Peps()\n",
        "data = peps.jsonl_format()\n",
        "data[-1]['text'][:2000]"
      ],
      "metadata": {
        "id": "oDsHqmbKwLyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "DJvR58fcsoW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "embeder = Embeder()\n",
        "vec = embeder.run([\"hello I'm goku\"])[0]\n",
        "print(len(vec))\n",
        "vec[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T03:35:42.348359Z",
          "iopub.execute_input": "2024-04-04T03:35:42.348712Z",
          "iopub.status.idle": "2024-04-04T03:35:50.815084Z",
          "shell.execute_reply.started": "2024-04-04T03:35:42.348668Z",
          "shell.execute_reply": "2024-04-04T03:35:50.814027Z"
        },
        "trusted": true,
        "id": "gjN90cXywLyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "retriever = Retriever(data)\n",
        "retriever.set()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T03:35:50.816534Z",
          "iopub.execute_input": "2024-04-04T03:35:50.816879Z",
          "iopub.status.idle": "2024-04-04T03:39:32.610082Z",
          "shell.execute_reply.started": "2024-04-04T03:35:50.81685Z",
          "shell.execute_reply": "2024-04-04T03:39:32.609037Z"
        },
        "trusted": true,
        "id": "tp73Q5c3wLyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = Retriever(data)\n",
        "question = \"\"\"\n",
        "Version Control System\n",
        "Currently the CPython and supporting repositories use Mercurial. As a modern distributed version control system, it has served us well since the migration from Subversion. However, when evaluating the VCS we must consider the capabilities of the VCS itself as well as the network effect and mindshare of the community around that VCS.\n",
        "\n",
        "There are really only two real options for this, Mercurial and Git. The technical capabilities of the two systems are largely equivalent, therefore this PEP instead focuses on their social aspects.\n",
        "\n",
        "It is not possible to get exact numbers for the number of projects or people which are using a particular VCS, however we can infer this by looking at several sources of information for what VCS projects are using.\n",
        "\n",
        "The Open Hub (previously Ohloh) statistics [1] show that 37% of the repositories indexed by The Open Hub are using Git (second only to Subversion which has 48%) while Mercurial has just 2%,\n",
        "Does the community advantage of Git outweigh the familiarity of Mercurial for CPython?\n",
        "\"\"\"\n",
        "retriever.query(question)[1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T04:42:09.780437Z",
          "iopub.execute_input": "2024-04-04T04:42:09.781391Z",
          "iopub.status.idle": "2024-04-04T04:42:12.334769Z",
          "shell.execute_reply.started": "2024-04-04T04:42:09.78135Z",
          "shell.execute_reply": "2024-04-04T04:42:12.333676Z"
        },
        "trusted": true,
        "id": "JoWTgVDdwLyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "R3xhjyW9soW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading few-shot to the memory"
      ],
      "metadata": {
        "id": "Ckz9vn-nwLyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'data/history.json'\n",
        "init_chat = [message.system_reply()]\n",
        "json.dump(init_chat, open(file, 'w'))\n",
        "for i, q in enumerate(few_shot_list):\n",
        "    sample = python_qa[q]\n",
        "    mesage = Message(\n",
        "        role='user',\n",
        "        content=sample['Question']\n",
        "    )\n",
        "    reply = mesage.update()\n",
        "    mesage = Message(\n",
        "        role='assitant',\n",
        "        content=sample['Answer']\n",
        "    )\n",
        "    reply = mesage.update()\n",
        "\n",
        "history = json.load(open(file))\n",
        "history"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-04T05:47:23.257687Z",
          "iopub.execute_input": "2024-04-04T05:47:23.258149Z",
          "iopub.status.idle": "2024-04-04T05:47:23.28537Z",
          "shell.execute_reply.started": "2024-04-04T05:47:23.258114Z",
          "shell.execute_reply": "2024-04-04T05:47:23.284271Z"
        },
        "trusted": true,
        "id": "ht19CJH3wLy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the RAG system"
      ],
      "metadata": {
        "id": "NVrxPChLwLy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent()\n",
        "print(agent.memory(\"\"\"\n",
        "Version Control System\n",
        "Currently the CPython and supporting repositories use Mercurial. As a modern distributed version control system, it has served us well since the migration from Subversion. However, when evaluating the VCS we must consider the capabilities of the VCS itself as well as the network effect and mindshare of the community around that VCS.\n",
        "\n",
        "There are really only two real options for this, Mercurial and Git. The technical capabilities of the two systems are largely equivalent, therefore this PEP instead focuses on their social aspects.\n",
        "\n",
        "It is not possible to get exact numbers for the number of projects or people which are using a particular VCS, however we can infer this by looking at several sources of information for what VCS projects are using.\n",
        "\n",
        "The Open Hub (previously Ohloh) statistics [1] show that 37% of the repositories indexed by The Open Hub are using Git (second only to Subversion which has 48%) while Mercurial has just 2%,\n",
        "\"\"\"))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T04:32:58.441392Z",
          "iopub.execute_input": "2024-04-04T04:32:58.442425Z",
          "iopub.status.idle": "2024-04-04T04:33:00.6218Z",
          "shell.execute_reply.started": "2024-04-04T04:32:58.442386Z",
          "shell.execute_reply": "2024-04-04T04:33:00.62058Z"
        },
        "trusted": true,
        "id": "W41GGMcewLy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPC - 0.0.1"
      ],
      "metadata": {
        "id": "Cc_e_HAgsoW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T05:00:19.510655Z",
          "iopub.execute_input": "2024-04-04T05:00:19.511798Z",
          "iopub.status.idle": "2024-04-04T05:00:19.519551Z",
          "shell.execute_reply.started": "2024-04-04T05:00:19.51175Z",
          "shell.execute_reply": "2024-04-04T05:00:19.518346Z"
        },
        "trusted": true,
        "id": "D1qL6UxkwLy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "print('*'*2000,agent.chat('what is the lates python version that is currently lunched'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T06:10:43.368184Z",
          "iopub.execute_input": "2024-04-04T06:10:43.369145Z",
          "iopub.status.idle": "2024-04-04T06:12:01.585753Z",
          "shell.execute_reply.started": "2024-04-04T06:10:43.369108Z",
          "shell.execute_reply": "2024-04-04T06:12:01.58453Z"
        },
        "trusted": true,
        "id": "MCnV7arBwLy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tunning"
      ],
      "metadata": {
        "id": "5Jl33mfjsoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPC - 0.1.0"
      ],
      "metadata": {
        "id": "xUdtzcyTsoW6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FEFtiHQBwLy0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}