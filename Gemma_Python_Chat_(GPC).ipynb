{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 726715,
          "sourceType": "datasetVersion",
          "datasetId": 262
        },
        {
          "sourceId": 11384,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 6216
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "name": "Gemma Python Chat (GPC)",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/Gemma_Python_Chat_(GPC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240402%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240402T055605Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dbed536196ddbefc905539fcc2633adf752808daebae3699d48e1b14ce81b341b2c99b5aed28ab13187be50fb3bef07b6d811ad2321b1b0e28e1d11c3960199db000778d01c35644b5f55eeac4e4f62b8d1d7d784e7bb61dba49337965a60183409c5391d480c197faff0d897494bfb3fbf09be02d7d70412f45a02b32a3cd8d44bbd12c7a8aeeea18bfa46f193caf0ff1f1faac2bf69bf196f75d929b6c775a36850732c2a33413cf5a6b1615c68fb01fc2eedcdd07c1cf668c33638d9c46425315bd34ab212b1d757ed45590dc9495f535b84f383d737930870d9510ee0d87b074afffe9e3e1a6ef6abe5dacc12aa29738da3301bade988ed92a27e0a9b817c,pythonquestions:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F262%2F726715%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240402%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240402T055605Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5207095bf44aec03b51c2537e12b23e82acd42a1a59e5dab728d33273324df2c81d1c720c5911129b54b02736e49bd27bdae3740f84495d666e1a20b3cc13c7a7135b035c31b6424ac1bfe99dd7e139ad38563fcada9091fc0cf2698d6ca6355110970910c767996fed8c0f2ae47e0be6f08ae0bb922ab12277fba24bd4ea607d0e7c49931f9fae14da925ec3faab4773fda9132d4c09291ca81cb161976da37e46b8a6b82cc4c8135dfc98c83e44942428138044014d976b7ef6790784c47efba80be8f9149620dd6bafa7064ee8e48a5eefd916f2e8553d29c52b592b41ecdae5bdca1f5b88a163562f243b585ffe3e17f9b76800d5c3e4d2e49b0ffc2ddf5,gemma/transformers/2b/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F6216%2F11384%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240402%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240402T055605Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dce56110ac732e93976aec6db4b90a6fcc6a8deb1921aaed6692ab4a6b9d9d86d9da41639e0b137e352750f61dd19b7579ff458764f89da70f60680cb00c52eeb9d64e755eebc477724cee5a3bb730cdf32cc2bb0963b17a4f2c4ce342db6b6e209202cec8bbc22ab0b57f0f9971327982b426e2b00afb34b7fc6fcff3381e08d3674b170375aeb14bd8aed6e14a58ef228dc1598b954e87fff6499ec41a719ea13449cb0e07777ad467715356a30c4fc85c4aeee0bdfbc59a3cb1932975fd926e0d5d24ddeeeddaabc3b826cd21a49fb4bb2d70a685d195bca92e549258a2829222f68ffa9b8511653c288a3deafe7770b7f2414f3e06a8c83e99ecb80d063a1'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "KfO7yD8TYURQ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/GPC_(Gemma_Python_Chat).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP ü§ñ Gemma Python Chatbot"
      ],
      "metadata": {
        "id": "Qx8hbihKsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/8xZNc32/Gemma.png)"
      ],
      "metadata": {
        "id": "B0RdGIyrsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gemma Python Chatbot üöÄüöÄ help you to answer common questions about the üêç Python programming language, powered by [Gemma 2B IT](https://blog.google/technology/developers/gemma-open-models/) updated with the Python Enhancement Proposal ([PEPs](https://peps.python.org/)) documentation using a Retrival-Augmented Generation ([RAG](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)) sponsored by a Chroma vectorial database using the open source embeddings [hkunlp/instructor-large](https://huggingface.co/hkunlp/instructor-large) of 768 entries, orchested by [langchaing](https://python.langchain.com/docs/use_cases/chatbots/) that is a framework that let you use differen models y les code changes"
      ],
      "metadata": {
        "id": "5C9kSlAVsoW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "5GhJQPvLsoW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.209\n",
        "!pip install chromadb==0.3.26\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install InstructorEmbedding==1.0.1"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-02T04:26:18.336254Z",
          "iopub.execute_input": "2024-04-02T04:26:18.33709Z",
          "iopub.status.idle": "2024-04-02T04:27:57.369757Z",
          "shell.execute_reply.started": "2024-04-02T04:26:18.337054Z",
          "shell.execute_reply": "2024-04-02T04:27:57.36858Z"
        },
        "trusted": true,
        "id": "DKDfTSZcYURV",
        "outputId": "bd5b1b08-b0d8-4f94-963b-110e895e0d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting langchain==0.0.209\n  Downloading langchain-0.0.209-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.209) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.209) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.209) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.209) (4.0.3)\nCollecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.209)\n  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\nCollecting langchainplus-sdk>=0.0.13 (from langchain==0.0.209)\n  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.209) (2.9.0)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.209) (1.26.4)\nCollecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.209)\n  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata (8.5 kB)\nCollecting pydantic<2,>=1 (from langchain==0.0.209)\n  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.2/150.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.209) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.209) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.209) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.209) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.209) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.209) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.209) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.209) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.209) (0.9.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain==0.0.209) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.209) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.209) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.209) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.209) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.209) (3.0.3)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.209) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.209) (1.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.209) (3.1.1)\nDownloading langchain-0.0.209-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\nDownloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\nDownloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic, openapi-schema-pydantic, langchainplus-sdk, dataclasses-json, langchain\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.3\n    Uninstalling pydantic-2.5.3:\n      Successfully uninstalled pydantic-2.5.3\n  Attempting uninstall: dataclasses-json\n    Found existing installation: dataclasses-json 0.6.4\n    Uninstalling dataclasses-json-0.6.4:\n      Successfully uninstalled dataclasses-json-0.6.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires pydantic>=2, but you have pydantic 1.10.14 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dataclasses-json-0.5.14 langchain-0.0.209 langchainplus-sdk-0.0.20 openapi-schema-pydantic-1.2.4 pydantic-1.10.14\nCollecting chromadb==0.3.26\n  Downloading chromadb-0.3.26-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: pandas>=1.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (2.1.4)\nRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (2.31.0)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (1.10.14)\nCollecting hnswlib>=0.7 (from chromadb==0.3.26)\n  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting clickhouse-connect>=0.5.7 (from chromadb==0.3.26)\n  Downloading clickhouse_connect-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\nCollecting duckdb>=0.7.1 (from chromadb==0.3.26)\n  Downloading duckdb-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (763 bytes)\nRequirement already satisfied: fastapi>=0.85.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.25.0)\nRequirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (1.26.4)\nCollecting posthog>=2.4.0 (from chromadb==0.3.26)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (4.9.0)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.3.26)\n  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb==0.3.26)\n  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (0.15.2)\nRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (4.66.1)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.3.26) (7.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2024.2.2)\nRequirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (1.26.18)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2023.3.post1)\nRequirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (0.22.0)\nRequirement already satisfied: lz4 in /opt/conda/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (4.3.3)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb==0.3.26) (0.32.0.post1)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.3.26)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (23.5.26)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (21.3)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (1.12)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.26) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.26) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.26) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.3.26)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.26) (2.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.26) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.26) (3.6)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb==0.3.26) (0.21.4)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.26) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (1.0.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (6.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26) (2024.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxruntime>=1.14.1->chromadb==0.3.26) (3.1.1)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.33.0,>=0.29.0->fastapi>=0.85.1->chromadb==0.3.26) (4.2.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.26)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.26) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.85.1->chromadb==0.3.26) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.85.1->chromadb==0.3.26) (1.2.0)\nDownloading chromadb-0.3.26-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading clickhouse_connect-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m965.0/965.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading duckdb-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: hnswlib\n  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=196380 sha256=2a016c54732dc189422187cc247132e1436916f77e4921a3f706bfa505ce3f16\n  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\nSuccessfully built hnswlib\nInstalling collected packages: monotonic, pulsar-client, humanfriendly, hnswlib, duckdb, clickhouse-connect, posthog, coloredlogs, onnxruntime, chromadb\nSuccessfully installed chromadb-0.3.26 clickhouse-connect-0.7.6 coloredlogs-15.0.1 duckdb-0.10.1 hnswlib-0.8.0 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.17.1 posthog-3.5.0 pulsar-client-3.4.0\nCollecting sentence-transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.38.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.16.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.2.0)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.21.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=5ff6fff94aa7f0b9df69b3f57eecccb997e3c950f0336ccd8f4aa20061578a77\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\nCollecting InstructorEmbedding==1.0.1\n  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\nDownloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\nInstalling collected packages: InstructorEmbedding\nSuccessfully installed InstructorEmbedding-1.0.1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#base\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "# variables\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "# model\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "# data\n",
        "from datasets import load_dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# embeddings\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "# vector database\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "Vq5ZFhX7soW4",
        "execution": {
          "iopub.status.busy": "2024-04-02T04:29:50.774285Z",
          "iopub.execute_input": "2024-04-02T04:29:50.775135Z",
          "iopub.status.idle": "2024-04-02T04:30:12.477292Z",
          "shell.execute_reply.started": "2024-04-02T04:29:50.775102Z",
          "shell.execute_reply": "2024-04-02T04:30:12.47651Z"
        },
        "trusted": true,
        "outputId": "7f08a964-725e-4742-bca0-52c90288d215"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-04-02 04:30:01.239549: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-02 04:30:01.239678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-02 04:30:01.370325: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"/kaggle/input/gemma/transformers/2b/2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, torch_dtype=torch.float16).cuda()"
      ],
      "metadata": {
        "id": "Q1QizKYCsoW4",
        "outputId": "9b505082-176e-4ec8-c951-63e124418a80",
        "execution": {
          "iopub.status.busy": "2024-04-02T04:31:53.734608Z",
          "iopub.execute_input": "2024-04-02T04:31:53.736175Z",
          "iopub.status.idle": "2024-04-02T04:32:27.954373Z",
          "shell.execute_reply.started": "2024-04-02T04:31:53.736137Z",
          "shell.execute_reply": "2024-04-02T04:32:27.953545Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "8b79b72468e949da926b63fc4b6b504f"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b79b72468e949da926b63fc4b6b504f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "TLZ7I9soYURW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions and answers data"
      ],
      "metadata": {
        "id": "cGv0BqcqYURX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pythonQAData:\n",
        "    \"\"\"\n",
        "    Processes data from Questions and Answers CSV files to provide a structured Q&A format.\n",
        "\n",
        "    Attributes:\n",
        "        questions_path (str): Path to the Questions CSV file\n",
        "        answers_path (str): Path to the Answers CSV file\n",
        "        tags_path (str): Path to the tags CSV file\n",
        "\n",
        "    Methods:\n",
        "        load_data(): Loads the CSV data into DataFrames.\n",
        "        merge(): Cleans, merges, and formats the question and answer data.\n",
        "        get_formatted_qa(): Returns a list of formatted question-answer strings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.questions_path = '../input/pythonquestions/Questions.csv'\n",
        "        self.answers_path = '../input/pythonquestions/Answers.csv'\n",
        "        self.tags_path = '../input/pythonquestions/Tags.csv'\n",
        "        self.regex = r\"<\\/?[\\w\\s]*>\"\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Loads Questions and Answers data from CSV files.\"\"\"\n",
        "        df_questions = pd.read_csv(\n",
        "            self.questions_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Score',\n",
        "                'Title'\n",
        "            ]\n",
        "        )\n",
        "        df_answers = pd.read_csv(\n",
        "            self.answers_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'ParentId',\n",
        "                'Score',\n",
        "                'Body'\n",
        "            ]\n",
        "        )\n",
        "        df_tags = pd.read_csv(\n",
        "            self.tags_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Tag'\n",
        "            ]\n",
        "        )\n",
        "        return df_questions, df_answers, df_tags\n",
        "\n",
        "\n",
        "    def qa_data(self):\n",
        "        \"\"\"Cleans, merges, and formats the question and answer data.\"\"\"\n",
        "        df_questions, df_answers, df_tags = self.load_data()\n",
        "        # Rename\n",
        "        df_questions.rename(\n",
        "            columns={\n",
        "                'Title': 'Question',\n",
        "                'Score': 'question_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        df_answers.rename(\n",
        "            columns={\n",
        "                'Body': 'Answer',\n",
        "                'ParentId':'Id',\n",
        "                'Score': 'answer_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        # Filter by score\n",
        "        df_questions = df_questions[df_questions['question_score'] > 5].copy()\n",
        "        # Sort and deduplicate answers\n",
        "        df_answers = df_answers.sort_values(\n",
        "            'answer_score',\n",
        "            ascending=False\n",
        "        ).drop_duplicates(subset=['Id'])\n",
        "        # Merge\n",
        "        qa_data = df_questions.merge(\n",
        "            df_answers,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        ).merge(\n",
        "            df_tags,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        )\n",
        "        # filter for python  questions\n",
        "        qa_data = qa_data[qa_data['answer_score'] > 5]\n",
        "        df_qa_data = qa_data[qa_data['Tag']=='python']\n",
        "        return df_qa_data\n",
        "\n",
        "    def get_formatted_qa(self):\n",
        "        \"\"\"Returns a list of formatted question-answer strings.\"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        df_qa_data['Answer'] = df_qa_data['Answer'].apply(\n",
        "            lambda x: re.sub(\n",
        "                self.regex,\n",
        "                \"\",\n",
        "                x\n",
        "            )\n",
        "        )\n",
        "        data = [\n",
        "            f\"Question:\\n{row['Question']}\\n\\nAnswer:\\n{row['Answer']}\"\n",
        "            for index, row\n",
        "            in df_qa_data.iterrows()\n",
        "        ]\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "bSU-ohmssoW5",
        "execution": {
          "iopub.status.busy": "2024-04-02T04:34:14.367423Z",
          "iopub.execute_input": "2024-04-02T04:34:14.368076Z",
          "iopub.status.idle": "2024-04-02T04:34:14.381811Z",
          "shell.execute_reply.started": "2024-04-02T04:34:14.368043Z",
          "shell.execute_reply": "2024-04-02T04:34:14.38082Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Enhancement Proposals data"
      ],
      "metadata": {
        "id": "eT5dLrFXYURX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Peps:\n",
        "    \"\"\"\n",
        "    Scrapes Python Enhancement Proposals (PEPs) from https://peps.python.org/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initiates the scraping process.\"\"\"\n",
        "        self.base_url = 'https://peps.python.org/'\n",
        "\n",
        "    def scraper(self, url):\n",
        "        \"\"\"\n",
        "        Fetches the HTML content of a given URL.\n",
        "\n",
        "        Args:\n",
        "            url (str): The URL to fetch.\n",
        "\n",
        "        Returns:\n",
        "            BeautifulSoup: A BeautifulSoup object representing the parsed HTML.\n",
        "        \"\"\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    def _fetch_pep_links(self):\n",
        "        \"\"\"Fetches links to individual PEP pages (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of PEP URLs.\n",
        "        \"\"\"\n",
        "        soup = self.scraper(self.base_url)\n",
        "        return list(\n",
        "            set(\n",
        "                [\n",
        "                    self.base_url + ref['href']\n",
        "                    for ref in soup.find_all('a', class_='pep reference internal')\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _download_peps(self):\n",
        "        \"\"\"Downloads the content of individual PEPs (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of BeautifulSoup objects representing individual PEPs.\n",
        "        \"\"\"\n",
        "        pep_links = self._fetch_pep_links()\n",
        "        return [self.scraper(pep_link) for pep_link in pep_links]\n",
        "\n",
        "    def scrape(self):\n",
        "        \"\"\"Extracts the relevant text content from each PEP.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of text strings, each representing the content of a PEP.\n",
        "        \"\"\"\n",
        "        pep_soups = self._download_peps()\n",
        "        return [\n",
        "            '\\n'.join([word.text for word in soup.find_all('section')])\n",
        "            for soup in pep_soups\n",
        "        ]\n",
        "\n",
        "    def jsonl_format(self):\n",
        "        \"\"\"Format the spcraped data to a jsonl format is it alist of dictionaries\n",
        "        Returns:\n",
        "            list: A list of dictionaries with the following:\n",
        "                page_content: The content of each section scraped\n",
        "                source: The linke where it was scraped\n",
        "        \"\"\"\n",
        "        source_links = self._fetch_pep_links()\n",
        "        pep_content = self.scrape()\n",
        "        pep_data = dict(zip(source_links,pep_content))\n",
        "        return [\n",
        "            {\n",
        "                'text': value,\n",
        "                'source': key\n",
        "            }\n",
        "            for key, value in pep_data.items()\n",
        "        ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:34:22.152627Z",
          "iopub.execute_input": "2024-04-02T04:34:22.152979Z",
          "iopub.status.idle": "2024-04-02T04:34:22.163947Z",
          "shell.execute_reply.started": "2024-04-02T04:34:22.152953Z",
          "shell.execute_reply": "2024-04-02T04:34:22.16275Z"
        },
        "trusted": true,
        "id": "gKPpPW8bYURX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "Z_u7X8tlYURX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeder:\n",
        "    \"\"\"\n",
        "    Creates embeddings (numerical representations) of text documents for various tasks like semantic search and\n",
        "    similarity comparison. Provides flexibility to choose between a large model for more comprehensive\n",
        "    embeddings or a smaller, faster model.\n",
        "\n",
        "    Attributes:\n",
        "\n",
        "    large (bool): Flag indicating whether to use the large embedding model (default: True).\n",
        "    model (str): Name of the embedding model to use.\n",
        "    device (str): Device to use for computations (default: \"cuda\").\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            large: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        init(self, large: bool = True) -> None\n",
        "            Initializes the class with the specified model size preference.\n",
        "        \"\"\"\n",
        "        self.large = large\n",
        "        if self.large:\n",
        "            self.model = \"hkunlp/instructor-large\"\n",
        "            self.device: str = \"cuda\"\n",
        "        else:\n",
        "            self.model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "    def instructor(self):\n",
        "        if self.large:\n",
        "            embed = HuggingFaceInstructEmbeddings(\n",
        "                model_name=self.model,\n",
        "                model_kwargs={\"device\": self.device}\n",
        "            )\n",
        "        else:\n",
        "            embed = SentenceTransformerEmbeddings(\n",
        "                model_name=self.model\n",
        "            )\n",
        "        return embed\n",
        "\n",
        "    def run(\n",
        "            self,\n",
        "            docs_list: list,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        run(self, docs_list: list) -> List[List[float]]:\n",
        "            Generates embeddings for the provided list of documents.\n",
        "            Returns a list of lists, where each inner list represents the embedding vector for a document.\n",
        "        \"\"\"\n",
        "        if docs_list is None:\n",
        "            docs_list = ['']\n",
        "        embed = self.instructor()\n",
        "\n",
        "        return embed.embed_documents(docs_list)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:34:26.303781Z",
          "iopub.execute_input": "2024-04-02T04:34:26.304644Z",
          "iopub.status.idle": "2024-04-02T04:34:26.312921Z",
          "shell.execute_reply.started": "2024-04-02T04:34:26.304609Z",
          "shell.execute_reply": "2024-04-02T04:34:26.311962Z"
        },
        "trusted": true,
        "id": "ibycVvSPYURY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Settings:\n",
        "    user_secrets = UserSecretsClient()\n",
        "    CHUNK_SIZE = user_secrets.get_secret(\"CHUNK_SIZE\")\n",
        "    CHUNK_OVERLAP = user_secrets.get_secret(\"CHUNK_OVERLAP\")\n",
        "    CHROMA_NAME_INDEX = user_secrets.get_secret(\"CHROMA_NAME_INDEX\")\n",
        "    K = user_secrets.get_secret(\"K\")\n",
        "    NN_THRESHOLD = user_secrets.get_secret(\"NN_THRESHOLD\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:34:29.913111Z",
          "iopub.execute_input": "2024-04-02T04:34:29.913824Z",
          "iopub.status.idle": "2024-04-02T04:34:30.472223Z",
          "shell.execute_reply.started": "2024-04-02T04:34:29.913793Z",
          "shell.execute_reply": "2024-04-02T04:34:30.47146Z"
        },
        "trusted": true,
        "id": "rx4nvak0YURY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeder = Embeder()\n",
        "settings = Settings()\n",
        "class Retriever:\n",
        "    \"\"\" retriever\n",
        "    Retrieves the embeddings from a vectorial database\n",
        "    \"\"\"\n",
        "    def __init__(self, data: list[dict]):\n",
        "        \"\"\" initialize the retriever\n",
        "        \"\"\"\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=int(settings.CHUNK_SIZE),\n",
        "            length_function=len,\n",
        "            chunk_overlap=int(settings.CHUNK_OVERLAP)\n",
        "        )\n",
        "        self.index_name = settings.CHROMA_NAME_INDEX\n",
        "        self.data = data\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"\n",
        "        Loads documents from the initialized data and returns them as a list of Document objects.\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "        for obj in self.data:\n",
        "            page_content = obj.get(\"text\", \"\")\n",
        "            metadata = {\n",
        "                \"source\": obj.get(\"source\", \"local\")\n",
        "            }\n",
        "            documents.append(Document(page_content=page_content, metadata=metadata))\n",
        "        return documents\n",
        "\n",
        "    def set(self):\n",
        "        \"\"\"\n",
        "         update the vectorial database\n",
        "        \"\"\"\n",
        "        data = self.load()\n",
        "        splitter = self.text_splitter\n",
        "        documents = splitter.split_documents(data)\n",
        "        instructor = embeder.instructor()\n",
        "        vector_db = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=instructor,\n",
        "            persist_directory=self.index_name\n",
        "        )\n",
        "        vector_db.persist()\n",
        "\n",
        "    def query(\n",
        "            self,\n",
        "            message: str,\n",
        "            k: int = int(settings.K),\n",
        "            threshold: float = float(settings.NN_THRESHOLD)\n",
        "    ):\n",
        "        \"\"\" retrieve the 2 more similar text chunks based on the message given\n",
        "        \"\"\"\n",
        "        vectorial_db = Chroma(\n",
        "            embedding_function=embeder.instructor(),\n",
        "            persist_directory=self.index_name\n",
        "        )\n",
        "        res = vectorial_db.similarity_search_with_score(message, k=k)\n",
        "        return [vector[0].page_content for vector in res if vector[1] < threshold]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:52:19.644213Z",
          "iopub.execute_input": "2024-04-02T04:52:19.645139Z",
          "iopub.status.idle": "2024-04-02T04:52:19.656097Z",
          "shell.execute_reply.started": "2024-04-02T04:52:19.645107Z",
          "shell.execute_reply": "2024-04-02T04:52:19.655142Z"
        },
        "trusted": true,
        "id": "xTtW4KShYURY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "r8OX7Xe-soW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Data"
      ],
      "metadata": {
        "id": "AsIR-djKYURY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "peps = Peps()\n",
        "peps_corpus = peps.scrape()\n",
        "print(\n",
        "    'corpus lenght:',\n",
        "    len(peps_corpus),\n",
        "    '\\nsection <n> lenght:',\n",
        "    len(peps_corpus[0]),\n",
        "    '\\nsample text:\\n\\n',\n",
        "    peps_corpus[10][1306:2000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:38:26.852098Z",
          "iopub.execute_input": "2024-04-02T04:38:26.853036Z",
          "iopub.status.idle": "2024-04-02T04:39:56.683426Z",
          "shell.execute_reply.started": "2024-04-02T04:38:26.853003Z",
          "shell.execute_reply": "2024-04-02T04:39:56.682496Z"
        },
        "trusted": true,
        "id": "tSzE78xEYURY",
        "outputId": "37c117f8-94ad-4b7b-94df-f6fdd0f2647c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "corpus lenght: 644 \nsection <n> lenght: 85988 \nsample text:\n\n ing and distributing wheels with this platform tag, and that pip\nsupport downloading and installing these packages on compatible platforms.\n\n\nRationale\nCurrently, distribution of binary Python extensions for Windows and OS X is\nstraightforward. Developers and packagers build wheels (PEP 427, PEP 491),\nwhich are\nassigned platform tags such as win32 or macosx_10_6_intel, and upload\nthese wheels to PyPI. Users can download and install these wheels using tools\nsuch as pip.\nFor Linux, the situation is much more delicate. In general, compiled Python\nextension modules built on one Linux distribution will not work on other Linux\ndistributions, or even on different machines running the same Lin\nCPU times: user 49.8 s, sys: 687 ms, total: 50.5 s\nWall time: 1min 29s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "peps = Peps()\n",
        "data = peps.jsonl_format()\n",
        "data[-1]['text'][:2000]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:40:48.003271Z",
          "iopub.execute_input": "2024-04-02T04:40:48.003662Z",
          "iopub.status.idle": "2024-04-02T04:42:08.115039Z",
          "shell.execute_reply.started": "2024-04-02T04:40:48.003634Z",
          "shell.execute_reply": "2024-04-02T04:42:08.113982Z"
        },
        "trusted": true,
        "id": "c7kdSc-vYURY",
        "outputId": "66a52390-9422-427d-efe4-83017f0c7ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "CPU times: user 51.4 s, sys: 740 ms, total: 52.2 s\nWall time: 1min 20s\n",
          "output_type": "stream"
        },
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'\\n\\nPython Enhancement Proposals\\n\\nPython ¬ª \\nPEP Index ¬ª \\nPEP 507\\n\\n\\n\\n\\n\\nToggle light / dark / auto colour theme\\n\\n\\n\\n\\nPEP 507 ‚Äì Migrate CPython to Git and GitLab\\n\\nAuthor:\\nBarry Warsaw <barry at python.org>\\nStatus:\\nRejected\\nType:\\nProcess\\nCreated:\\n30-Sep-2015\\nPost-History:\\n\\nResolution:\\nCore-Workflow message\\n\\n\\n\\nTable of Contents\\nAbstract\\nRationale\\nVersion Control System\\nRepository Hosting\\nCode Review\\nGitLab merge requests\\n\\n\\nCriticism\\nX is not written in Python\\nMercurial is better than Git\\nCPython Workflow is too Complicated\\n\\n\\nOpen issues\\nReferences\\nCopyright\\n\\n\\n\\nAbstract\\nThis PEP proposes migrating the repository hosting of CPython and the\\nsupporting repositories to Git.  Further, it proposes adopting a\\nhosted GitLab instance as the primary way of handling merge requests,\\ncode reviews, and code hosting.  It is similar in intent to PEP 481\\nbut proposes an open source alternative to GitHub and omits the\\nproposal to run Phabricator.  As with PEP 481, this particular PEP is\\noffered as an alternative to PEP 474 and PEP 462.\\n\\n\\nRationale\\nCPython is an open source project which relies on a number of\\nvolunteers donating their time.  As with any healthy, vibrant open\\nsource project, it relies on attracting new volunteers as well as\\nretaining existing developers.  Given that volunteer time is the most\\nscarce resource, providing a process that maximizes the efficiency of\\ncontributors and reduces the friction for contributions, is of vital\\nimportance for the long-term health of the project.\\nThe current tool chain of the CPython project is a custom and unique\\ncombination of tools.  This has two critical implications:\\n\\nThe unique nature of the tool chain means that contributors must\\nremember or relearn, the process, workflow, and tools whenever they\\ncontribute to CPython, without the advantage of leveraging long-term\\nmemory and familiarity they retain by working with other projects in\\nthe FLOSS ecosystem.  The knowledge they gain in working with\\nCPython is unlikely to be applicable to other'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fine-tunning data"
      ],
      "metadata": {
        "id": "QD1QTKiqYURZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_context = PythonQAData.get_formatted_qa()\n",
        "print(\n",
        "    'Python Questions loaded:',\n",
        "    len(python_context),\n",
        "    '\\n',\n",
        "    '\\nSample question-answer:\\n',\n",
        "    python_context[-1][1306:2000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:43:16.279448Z",
          "iopub.execute_input": "2024-04-02T04:43:16.280393Z",
          "iopub.status.idle": "2024-04-02T04:43:48.343612Z",
          "shell.execute_reply.started": "2024-04-02T04:43:16.280357Z",
          "shell.execute_reply": "2024-04-02T04:43:48.342642Z"
        },
        "trusted": true,
        "id": "9-W4aryKYURZ",
        "outputId": "e6cd4dec-538d-4ad3-e794-f2eb483e6fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Python Questions loaded: 33313 \n \nSample question-answer:\n \nCPU times: user 18.5 s, sys: 1.11 s, total: 19.6 s\nWall time: 32.1 s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "DJvR58fcsoW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "embeder = Embeder()\n",
        "vec = embeder.run([\"hello I'm goku\"])[0]\n",
        "print(len(vec))\n",
        "vec[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:56:43.783893Z",
          "iopub.execute_input": "2024-04-01T20:56:43.784946Z",
          "iopub.status.idle": "2024-04-01T20:56:45.229391Z",
          "shell.execute_reply.started": "2024-04-01T20:56:43.784896Z",
          "shell.execute_reply": "2024-04-01T20:56:45.228368Z"
        },
        "trusted": true,
        "id": "9rdbZQZiYURZ",
        "outputId": "2ee8779a-ab89-49e5-db38-ece2a08ef2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "load INSTRUCTOR_Transformer\nmax_seq_length  512\n768\n",
          "output_type": "stream"
        },
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[-0.011055625043809414,\n -0.01609027571976185,\n -0.010001139715313911,\n -0.01644662767648697,\n 0.030971292406320572,\n 0.024415701627731323,\n 0.011769121512770653,\n -0.0039945864118635654,\n 0.0033679513726383448,\n 0.01626904122531414]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "retriever = Retriever(data)\n",
        "retriever.set()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T05:13:53.98249Z",
          "iopub.execute_input": "2024-04-02T05:13:53.982901Z",
          "iopub.status.idle": "2024-04-02T05:45:29.235927Z",
          "shell.execute_reply.started": "2024-04-02T05:13:53.982871Z",
          "shell.execute_reply": "2024-04-02T05:45:29.23471Z"
        },
        "trusted": true,
        "id": "jxX6LZOLYURZ",
        "outputId": "7453b898-a440-49cb-b04e-e56766e115b7",
        "colab": {
          "referenced_widgets": [
            "ab2938e68eff4b8da84cb9351207a5e6"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "load INSTRUCTOR_Transformer\nmax_seq_length  512\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab2938e68eff4b8da84cb9351207a5e6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = Retriever(data)\n",
        "question = \"what is a List comprehensions and what is it's relation with map() and filter()?\"\n",
        "retriever.query(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "3hjvbnYcYURZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "R3xhjyW9soW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Use the model\n",
        "input_text = \"What is the data cut-off date for the Gemma Google models training dataset?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = {k: v.to(\"cuda\") for k, v in input_ids.items()}  if torch.cuda.is_available() else {k: v.to(\"cpu\")}\n",
        "outputs = model.generate(**input_ids, max_length=200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T04:13:29.90822Z",
          "iopub.execute_input": "2024-03-21T04:13:29.90861Z",
          "iopub.status.idle": "2024-03-21T04:13:35.388771Z",
          "shell.execute_reply.started": "2024-03-21T04:13:29.908582Z",
          "shell.execute_reply": "2024-03-21T04:13:35.387839Z"
        },
        "id": "P5DbvMzHsoW6",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Use the model\n",
        "input_text = \"if 5 t-shirts get dry in 10 hour ho long spend 30 t-shirts to get dry?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = {k: v.to(\"cuda\") for k, v in input_ids.items()}  if torch.cuda.is_available() else {k: v.to(\"cpu\")}\n",
        "outputs = model.generate(**input_ids, max_length=500)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "egQtE4VtsoW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.0.1"
      ],
      "metadata": {
        "id": "Cc_e_HAgsoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tunning"
      ],
      "metadata": {
        "id": "5Jl33mfjsoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.1.0"
      ],
      "metadata": {
        "id": "xUdtzcyTsoW6"
      }
    }
  ]
}