{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 726715,
          "sourceType": "datasetVersion",
          "datasetId": 262
        },
        {
          "sourceId": 11384,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 6216
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Gemma Python Chat (GPC)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/Gemma_Python_Chat_(GPC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240413%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240413T062645Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dad40cbbe56783202c154ab164ae791517d4a880a175f3e0a720534d99c77a78ab8212e0fdee0d3383a3d353ccdb9027151d05491b3d8a4f4b41854c6a56ffa93f6387acc5d1cfe325819b1859dccb575d7e6238b6e465d9fb0e8dda56fc2337796edd1d4729b0474ededb2fce84b1f811fb47f1ade0f9c751993a29e5ea8bba0c8337b69444ca0abdde7167769d063860c7c72f5492655592e94878c699e82630ec733d1a6c7f01f725c3a8b2c3d07f437228892e65292eaf6f7b51f947737efc0c8995421ce1328341b07cfeebe521ddf36855c7269266b674d31e918e25086264fc521d394d568d0cb28f92762a2ceacc6b4da69dd7e958534d7da2e6963e3,pythonquestions:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F262%2F726715%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240413%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240413T062646Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D57129064b63dae60b92419028a218c64480ac1122c46830d4c97cf58469803d569540571c7764a32441cb7875d16ee502cbb47fda24ad8a34447df8057ae07625e0c6ebf1a695c42f686f08712b9335b092e7dc2f6617719fb20a1e9ec1d94e671e041cfbba17fc488cb7af36df30cb9a752f11fa2e33545fa756a099eda0b50e8d28f74c1cc8aae2797a12e15c30d5005b8d241366aad0a884bb9e8637809b3f4382661afb81dd3250fa01b1c8a1dc2d092ae0470ffc449480aa30bcfeac58cfdbad81db4db1bdb19ee196c13db545152e45909928f986200bf80617d9c72011530ef076981281dd546e8f89a219925c8ecc3b3eef125f9a37ab3759f0e0eab,gemma/transformers/2b/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F6216%2F11384%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240413%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240413T062646Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dbb2ed787654158b6d378594fcbc38c0263210a75b27ef61d61741feec0d47e590bcb77fc5cda77d3d2255e412a9168e7a5a6df3130f73e3b70d1e4d296b3b615273f063246bd2a31680698b1c9781794e75e75046cd0935ecc7d6021419181701fae36ad1ac63157aec5639d687566841aeadcbaaae0a12aea936272be6b2d82a1ad827f04c3d683df91f1c4d3d734f5e794abcc2136dcab86c284754ea3186395e3354d25d1e11e716de49dda95fd20bcbffd1c8dd1428ef9462230a6ad3400081a4d1ce8594c4b46f726c9c8f7321125af7c5cdf54a3c8163353e35175e6eb03c921e05120082f50c019b3faa2d7d1e326f9c1d4a9e34ba7765b16aa36d654'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Ifpw8ZVXIyTK"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/GPC_(Gemma_Python_Chat).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP ğŸ¤– Gemma Python Chatbot"
      ],
      "metadata": {
        "id": "Qx8hbihKsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.postimg.cc/Nfpn7mxR/gemma.png)"
      ],
      "metadata": {
        "id": "X0JHFOl0IyTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** Run this notebook with GPU 100 Accelerator ğŸ˜ğŸ˜"
      ],
      "metadata": {
        "id": "EMQy6We4IyTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get answers to your burning Python questions with Gemma Python Chat! This helpful tool is powered by [Gemma 2B IT](https://blog.google/technology/developers/gemma-open-models/) and uses a clever \"few-shot\" strategy. It learns from real [Stackoverflow python question kaggle dataset](https://www.kaggle.com/datasets/stackoverflow/pythonquestions) and stays up-to-date with the latest Python Enhancement Proposal ([PEPs](https://peps.python.org/)). ğŸ“š\n",
        "\n",
        "### How does it work? ğŸ¤”\n",
        "\n",
        "Gemma Python Chat uses Retrival-Augmented Generation ([RAG](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)) with a [Chroma vectorial database](https://python.langchain.com/docs/integrations/vectorstores/chroma/) to find the most relevant information for your questions. It leverages the power of [paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) embeddings to understand the context and provide accurate answers. ğŸ§ \n",
        "All of this is orchestrated by [langchaing](https://python.langchain.com/docs/use_cases/chatbots/), a framework that makes it easy to work with different language models and keeps your code clean and flexible. ï¿½"
      ],
      "metadata": {
        "id": "73DsHzeGIyTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements ğŸ™ˆ"
      ],
      "metadata": {
        "id": "5GhJQPvLsoW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb #!pip install chromadb==0.3.26\n",
        "# !pip install ydata-profiling #!pip install ydata-profiling==4.6.1\n",
        "!pip install langchain #!pip install langchain==0.0.345 ##!pip install langchain-core==0.1.31\n",
        "#!pip install pydantic  #!pip install pydantic==1.10.14\n",
        "!pip install sentence-transformers   #!pip install sentence-transformers==2.6.1\n",
        "!pip install InstructorEmbedding  #!pip install InstructorEmbedding==1.0.1"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "scrolled": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-13T05:15:56.680711Z",
          "iopub.execute_input": "2024-04-13T05:15:56.681621Z",
          "iopub.status.idle": "2024-04-13T05:17:18.792852Z",
          "shell.execute_reply.started": "2024-04-13T05:15:56.681579Z",
          "shell.execute_reply": "2024-04-13T05:17:18.791701Z"
        },
        "trusted": true,
        "id": "gFZAa20IIyTP",
        "outputId": "dcd02032-41e0-4562-9272-5c5bd37806b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting chromadb\n  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.31.0)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.5.3)\nCollecting chroma-hnswlib==0.7.3 (from chromadb)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\nRequirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.9.0)\nCollecting pulsar-client>=3.1.0 (from chromadb)\n  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.2)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.1)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.1.1)\nCollecting grpcio>=1.58.0 (from chromadb)\n  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.2.3)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.1)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.0)\nCollecting orjson>=3.9.12 (from chromadb)\n  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (21.3)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.0.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.32.0.post1)\nRequirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nCollecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.6)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.21.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=19.1->build>=1.0.3->chromadb) (3.1.1)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.33.0,>=0.29.0->fastapi>=0.95.2->chromadb) (4.2.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.95.2->chromadb) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.95.2->chromadb) (1.2.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\nDownloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\nDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\nDownloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=e3701f48f36155a10958767034c764f9ed34ea1e1742abc080cf9e337a632838\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, pyproject_hooks, pulsar-client, orjson, opentelemetry-util-http, humanfriendly, grpcio, chroma-hnswlib, bcrypt, asgiref, posthog, coloredlogs, build, opentelemetry-instrumentation, onnxruntime, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, chromadb\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.51.1\n    Uninstalling grpcio-1.51.1:\n      Successfully uninstalled grpcio-1.51.1\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 bcrypt-4.1.2 build-1.2.1 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 grpcio-1.60.0 humanfriendly-10.0 kubernetes-29.0.0 monotonic-1.6 onnxruntime-1.17.3 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.10.0 posthog-3.5.0 pulsar-client-3.4.0 pypika-0.48.9 pyproject_hooks-1.0.0\nCollecting langchain\n  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.32 (from langchain)\n  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\nCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n  Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.46-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nDownloading langchain-0.1.16-py3-none-any.whl (817 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.42-py3-none-any.whl (287 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.46-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.5/111.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.1.16 langchain-community-0.0.32 langchain-core-0.1.42 langchain-text-splitters-0.0.1 langsmith-0.1.46 packaging-23.2\nCollecting sentence-transformers\n  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.38.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.21.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.6.1\nCollecting InstructorEmbedding\n  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\nDownloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\nInstalling collected packages: InstructorEmbedding\nSuccessfully installed InstructorEmbedding-1.0.1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#base\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import *\n",
        "from IPython.display import display, Markdown\n",
        "# variables\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "# model\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "# data\n",
        "from datasets import load_dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# embeddings\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "# vector database\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "Vq5ZFhX7soW4",
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:18.795232Z",
          "iopub.execute_input": "2024-04-13T05:17:18.796066Z",
          "iopub.status.idle": "2024-04-13T05:17:38.326232Z",
          "shell.execute_reply.started": "2024-04-13T05:17:18.796028Z",
          "shell.execute_reply": "2024-04-13T05:17:38.325425Z"
        },
        "trusted": true,
        "outputId": "896ad2df-b25a-4cf3-ec00-6809428bdb82"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-04-13 05:17:29.461895: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-13 05:17:29.461995: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-13 05:17:29.590675: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes ğŸ’ğŸ½"
      ],
      "metadata": {
        "id": "Cp7Ie8cSIyTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use object-oriented Python programming ğŸ’» to develop the chatbot, using the classes ğŸ“ in the corresponding order to support the Agent class that is the final service of the chatbot ğŸ¤–."
      ],
      "metadata": {
        "id": "P1pDS_uXIyTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions and answers data\n",
        "\n"
      ],
      "metadata": {
        "id": "TiUb4b1pIyTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class fit the data of [Stackoverflow python question kaggle dataset](https://www.kaggle.com/datasets/stackoverflow/pythonquestions) ğŸ“š to use it in a fine-tuned format âš™ï¸ or a list of dicts ğŸ—‚ï¸... it depends on the case of use you need! it also will be use to select some samples to make the few-shoot strategy"
      ],
      "metadata": {
        "id": "guIjt2sKIyTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pythonQAData:\n",
        "    \"\"\"\n",
        "    Processes data from Questions and Answers CSV files to provide a structured Q&A format.\n",
        "\n",
        "    Attributes:\n",
        "        questions_path (str): Path to the Questions CSV file\n",
        "        answers_path (str): Path to the Answers CSV file\n",
        "        tags_path (str): Path to the tags CSV file\n",
        "\n",
        "    Methods:\n",
        "        load_data(): Loads the CSV data into DataFrames.\n",
        "        merge(): Cleans, merges, and formats the question and answer data.\n",
        "        get_formatted_qa(): Returns a list of formatted question-answer strings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.questions_path = '../input/pythonquestions/Questions.csv'\n",
        "        self.answers_path = '../input/pythonquestions/Answers.csv'\n",
        "        self.tags_path = '../input/pythonquestions/Tags.csv'\n",
        "        self.regex = r\"<\\/?[\\w\\s]*>\"\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Loads Questions and Answers data from CSV files.\"\"\"\n",
        "        df_questions = pd.read_csv(\n",
        "            self.questions_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Score',\n",
        "                'Title'\n",
        "            ]\n",
        "        )\n",
        "        df_answers = pd.read_csv(\n",
        "            self.answers_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'ParentId',\n",
        "                'Score',\n",
        "                'Body'\n",
        "            ]\n",
        "        )\n",
        "        df_tags = pd.read_csv(\n",
        "            self.tags_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Tag'\n",
        "            ]\n",
        "        )\n",
        "        return df_questions, df_answers, df_tags\n",
        "\n",
        "\n",
        "    def qa_data(self):\n",
        "        \"\"\"Cleans, merges, and formats the question and answer data.\"\"\"\n",
        "        df_questions, df_answers, df_tags = self.load_data()\n",
        "        # Rename\n",
        "        df_questions.rename(\n",
        "            columns={\n",
        "                'Title': 'Question',\n",
        "                'Score': 'question_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        df_answers.rename(\n",
        "            columns={\n",
        "                'Body': 'Answer',\n",
        "                'ParentId':'Id',\n",
        "                'Score': 'answer_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        # Filter by score\n",
        "        df_questions = df_questions[df_questions['question_score'] > 5].copy()\n",
        "        # Sort and deduplicate answers\n",
        "        df_answers = df_answers.sort_values(\n",
        "            'answer_score',\n",
        "            ascending=False\n",
        "        ).drop_duplicates(subset=['Id'])\n",
        "        # Merge\n",
        "        df_qa = df_questions.merge(\n",
        "            df_answers,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        ).merge(\n",
        "            df_tags,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        )\n",
        "        # filter for python  questions\n",
        "        df_qa = df_qa[df_qa['answer_score'] > 5].copy()\n",
        "        df_qa = df_qa[df_qa['Tag']=='python'].copy()\n",
        "        df_qa['Answer'] = df_qa['Answer'].apply(\n",
        "            lambda x: re.sub(\n",
        "                self.regex,\n",
        "                \"\",\n",
        "                x\n",
        "            )\n",
        "        )\n",
        "        return df_qa\n",
        "\n",
        "    def get_fine_tunning_data(self):\n",
        "        \"\"\"Returns a list of formatted user-assistant strings.\"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        data = [\n",
        "            f\"<-change-of-interlocutor->user:\\n{row['Question']}\\n<-change-of-interlocutor->assistant:\\n{row['Answer']}\"\n",
        "            for index, row\n",
        "            in df_qa_data.iterrows()\n",
        "        ]\n",
        "        return data\n",
        "    def get_qa_data(self):\n",
        "        \"\"\"Returns a list of records dictionaries \"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        data = df_qa_data[\n",
        "            [\n",
        "                'Question',\n",
        "                'Answer'\n",
        "            ]\n",
        "        ].to_dict(orient='records')\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "bSU-ohmssoW5",
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:38.327406Z",
          "iopub.execute_input": "2024-04-13T05:17:38.328208Z",
          "iopub.status.idle": "2024-04-13T05:17:38.34291Z",
          "shell.execute_reply.started": "2024-04-13T05:17:38.328163Z",
          "shell.execute_reply": "2024-04-13T05:17:38.342045Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Enhancement Proposals data ğŸ˜‰"
      ],
      "metadata": {
        "id": "Wt10GuIcIyTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class Peps scrape all the oficial [PEPs](https://peps.python.org/) ğŸ” to get all the best practices of this programming language ğŸ and their latest features âœ¨, disposed to uses in the RAG system for the chatbot"
      ],
      "metadata": {
        "id": "6fSKszIoIyTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Peps:\n",
        "    \"\"\"\n",
        "    Scrapes Python Enhancement Proposals (PEPs) from https://peps.python.org/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initiates the scraping process.\"\"\"\n",
        "        self.base_url = 'https://peps.python.org/'\n",
        "\n",
        "    def scraper(self, url):\n",
        "        \"\"\"\n",
        "        Fetches the HTML content of a given URL.\n",
        "\n",
        "        Args:\n",
        "            url (str): The URL to fetch.\n",
        "\n",
        "        Returns:\n",
        "            BeautifulSoup: A BeautifulSoup object representing the parsed HTML.\n",
        "        \"\"\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    def _fetch_pep_links(self):\n",
        "        \"\"\"Fetches links to individual PEP pages (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of PEP URLs.\n",
        "        \"\"\"\n",
        "        soup = self.scraper(self.base_url)\n",
        "        return list(\n",
        "            set(\n",
        "                [\n",
        "                    self.base_url + ref['href']\n",
        "                    for ref in soup.find_all('a', class_='pep reference internal')\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _download_peps(self):\n",
        "        \"\"\"Downloads the content of individual PEPs (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of BeautifulSoup objects representing individual PEPs.\n",
        "        \"\"\"\n",
        "        pep_links = self._fetch_pep_links()\n",
        "        return [self.scraper(pep_link) for pep_link in pep_links]\n",
        "\n",
        "    def scrape(self):\n",
        "        \"\"\"Extracts the relevant text content from each PEP.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of text strings, each representing the content of a PEP.\n",
        "        \"\"\"\n",
        "        pep_soups = self._download_peps()\n",
        "        return [\n",
        "            '\\n'.join([word.text.replace('\"\"\"',\"'\") for word in soup.find_all('section')])\n",
        "            for soup in pep_soups\n",
        "        ]\n",
        "\n",
        "    def jsonl_format(self):\n",
        "        \"\"\"Format the spcraped data to a jsonl format is it alist of dictionaries\n",
        "        Returns:\n",
        "            list: A list of dictionaries with the following:\n",
        "                page_content: The content of each section scraped\n",
        "                source: The linke where it was scraped\n",
        "        \"\"\"\n",
        "        source_links = self._fetch_pep_links()\n",
        "        pep_content = self.scrape()\n",
        "        pep_data = dict(zip(source_links,pep_content))\n",
        "        return [\n",
        "            {\n",
        "                'text': value,\n",
        "                'source': key\n",
        "            }\n",
        "            for key, value in pep_data.items()\n",
        "        ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:38.344834Z",
          "iopub.execute_input": "2024-04-13T05:17:38.345124Z",
          "iopub.status.idle": "2024-04-13T05:17:38.389867Z",
          "shell.execute_reply.started": "2024-04-13T05:17:38.3451Z",
          "shell.execute_reply": "2024-04-13T05:17:38.38893Z"
        },
        "trusted": true,
        "id": "6pAxYEPJIyTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "Ex9l1B5yIyTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embeder class is the method that lets you embed the data given as a list of strings â¡ï¸ğŸ“¦. It will support the retriever for the RAG system ğŸ”"
      ],
      "metadata": {
        "id": "Mhn4GLJTIyTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeder:\n",
        "    \"\"\"\n",
        "    Creates text document embeddings (numerical representations) for semantic search,\n",
        "    similarity comparison, and related natural language processing tasks. Offers the choice\n",
        "    between a larger model for richer embeddings or a smaller, more efficient model.\n",
        "\n",
        "    Attributes:\n",
        "        large (bool): Controls the size of the embedding model. When set to True, uses a\n",
        "                      larger model for richer embeddings (default: True).\n",
        "        model (str): The name of the embedding model to use. Changes based on the value of 'large'.\n",
        "        device (str): The device to use for computation. Options include \"cpu\" or, if available, \"cuda\"\n",
        "                      for GPU acceleration (default: \"cuda\").\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, large: bool = False):\n",
        "        \"\"\"\n",
        "        Initializes the Embeder instance with preferences for model size.\n",
        "\n",
        "        Args:\n",
        "            large (bool, optional): If True, initializes with the larger embedding model.\n",
        "                                    Defaults to False for the smaller, faster model.\n",
        "        \"\"\"\n",
        "        self.large = large\n",
        "        if self.large:\n",
        "            self.model = \"hkunlp/instructor-large\"\n",
        "            self.device = \"cuda\"  # If GPU is available\n",
        "        else:\n",
        "            self.model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "    def instructor(self):\n",
        "        \"\"\"\n",
        "        Configures the embedding pipeline based on the selected model size.\n",
        "\n",
        "        Returns:\n",
        "           An embedding object from either the HuggingFaceInstructEmbeddings class\n",
        "           (for the large model) or the SentenceTransformerEmbeddings class (for the smaller model).\n",
        "        \"\"\"\n",
        "        if self.large:\n",
        "            embed = HuggingFaceInstructEmbeddings(\n",
        "                model_name=self.model,\n",
        "                model_kwargs={\"device\": self.device}\n",
        "            )\n",
        "        else:\n",
        "            embed = SentenceTransformerEmbeddings(model_name=self.model)\n",
        "        return embed\n",
        "\n",
        "    def run(self, docs_list: list):\n",
        "        \"\"\"\n",
        "        Generates embeddings for a given list of text documents.\n",
        "\n",
        "        Args:\n",
        "            docs_list (list): A list of text documents to embed.\n",
        "\n",
        "        Returns:\n",
        "            List[List[float]]: A list of lists, where each inner list represents the\n",
        "                               numerical embedding for a corresponding document in the input.\n",
        "        \"\"\"\n",
        "        if docs_list is None:\n",
        "            docs_list = ['']  # Prevent errors with empty input\n",
        "\n",
        "        embed = self.instructor()  # Get the appropriate embedding object\n",
        "        return embed.embed_documents(docs_list)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:38.391252Z",
          "iopub.execute_input": "2024-04-13T05:17:38.391608Z",
          "iopub.status.idle": "2024-04-13T05:17:38.405627Z",
          "shell.execute_reply.started": "2024-04-13T05:17:38.391575Z",
          "shell.execute_reply": "2024-04-13T05:17:38.404658Z"
        },
        "trusted": true,
        "id": "v7YBJWI9IyTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt ğŸ˜"
      ],
      "metadata": {
        "id": "XW_HKFAOIyTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a good practice, I consider the prompt as the instruction you use to get what you need from an LLM ğŸ§ . It includes all the parameters the LLM needs in every attempt.  Any change in a parameter will generate a different output ğŸ”„.  Therefore, the input is the whole prompt instruction plus all its parameters.\n",
        "\n",
        "Additionally, it's defined in an external object (like a NoSQL database ğŸ—„ï¸). This allows for easy updates outside of the code, making changes simpler for non-technical people ğŸ‘."
      ],
      "metadata": {
        "id": "-Mc_O7AoIyTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_config = [{\n",
        "    \"prompt_id\": \"0.0.1\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 1,\n",
        "    \"max_length\": 250,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.2\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "6. DO NOT repeat more than twice the same sentence!\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 1,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.3\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "6. DO NOT repeat more than once the same sentence!\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.4\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures in no more than 1000 characters\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "6. DO NOT repeat more than once the same sentence!\n",
        "7. If the user ask to you for a python script code, just return the code, do not EXPLAIN anything else\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.5\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. If the answer can be a Python code, justanswer with the code!!!\n",
        "2. If the user asks for a Python script code, just return the code, do not explain anything else\n",
        "3. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if necessary only\n",
        "4. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures in no more than 1000 characters\n",
        "5. Aim for plain language to ensure accessibility for all users\n",
        "6. If the question is off-topic about programming languages, you MUST say: \"I'm here to help you with Python programming language questions only, excuse me.\"\n",
        "7. You can say hello if you are greeted\n",
        "8. DO NOT repeat more than once the same sentence!\n",
        "\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.6\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. If the answer can be a Python code, justanswer with the code!!!\n",
        "2. If the user asks for a Python script code, just return the code, do not explain anything else no more than 1000 characters\n",
        "5. Aim for plain language to ensure accessibility for all users\n",
        "6. If the question is off-topic about programming languages, you MUST say: \"I'm here to help you with Python programming language questions only, excuse me.\"\n",
        "7. You can say hello if you are greeted\n",
        "8. DO NOT repeat more than once the same sentence!\n",
        "\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "}]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:38.406928Z",
          "iopub.execute_input": "2024-04-13T05:17:38.407226Z",
          "iopub.status.idle": "2024-04-13T05:17:38.422622Z",
          "shell.execute_reply.started": "2024-04-13T05:17:38.407182Z",
          "shell.execute_reply": "2024-04-13T05:17:38.421782Z"
        },
        "trusted": true,
        "id": "HCy2bFulIyTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T05:03:58.746391Z",
          "iopub.execute_input": "2024-04-04T05:03:58.746828Z",
          "iopub.status.idle": "2024-04-04T05:03:58.751668Z",
          "shell.execute_reply.started": "2024-04-04T05:03:58.746794Z",
          "shell.execute_reply": "2024-04-04T05:03:58.750527Z"
        },
        "id": "tIQYDmWaIyTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class aims to load the project's environment variables ğŸ” (in this case, Kaggle secrets). Additionally, it disposes of the prompt, as previously defined, to use it according to the team's chosen version, using the 'prompt_id' ğŸ¯ and for the method that needs those secrets to work âœ¨."
      ],
      "metadata": {
        "id": "aBURt9nyIyTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Settings:\n",
        "    \"\"\"\n",
        "    Manages and loads external configuration secrets for the application.  Utilizes a UserSecretsClient\n",
        "    to securely retrieve sensitive configuration values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Settings class and retrieves configuration secrets.\n",
        "        \"\"\"\n",
        "        self.user_secrets = UserSecretsClient()  # Create an instance for accessing secrets\n",
        "\n",
        "        # Load individual secrets (add descriptions for clarity)\n",
        "        self.CHUNK_SIZE = self.user_secrets.get_secret(\"CHUNK_SIZE\")  # The size of data chunks for processing\n",
        "        self.CHUNK_OVERLAP = self.user_secrets.get_secret(\"CHUNK_OVERLAP\")  # Ooverlap between data chunks\n",
        "        self.CHROMA_NAME_INDEX = self.user_secrets.get_secret(\"CHROMA_NAME_INDEX\")  # Vectorial Database identifier\n",
        "        self.MAX_MEMORY = self.user_secrets.get_secret(\"MAX_MEMORY\")  # Max memory limit to fit the max_lenght paramether\n",
        "        self.K = self.user_secrets.get_secret(\"K\")  # Could be a parameter for an algorithm\n",
        "        self.NN_THRESHOLD = self.user_secrets.get_secret(\"NN_THRESHOLD\")  # A threshold for a neural network or similarity metric\n",
        "        self.PROMPT_ID = self.user_secrets.get_secret(\"PROMPT_ID\")  # An identifier for a prompt (likely in a text-based task and configs)\n",
        "\n",
        "        # Given that 'prompt_config' is teh previous source of prompt definitions:\n",
        "        self.prompt = [prompt for prompt in prompt_config if prompt['prompt_id'] == self.PROMPT_ID][0]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:38.423778Z",
          "iopub.execute_input": "2024-04-13T05:17:38.424769Z",
          "iopub.status.idle": "2024-04-13T05:17:38.436579Z",
          "shell.execute_reply.started": "2024-04-13T05:17:38.424703Z",
          "shell.execute_reply": "2024-04-13T05:17:38.435661Z"
        },
        "trusted": true,
        "id": "ctV76dZYIyTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever"
      ],
      "metadata": {
        "id": "rJyLEzUxIyTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The retriever is the engine of the RAG system âš™ï¸. It uses config settings stored on the Settings class ğŸ“ and the Embeder class to populate the Chroma vector database ğŸš€. This setup needs to be done once and then can be loaded persistently.\n",
        "\n",
        "* when setting up the vectorial database, This process may take a few minutes the first time â³, so please be PATIENT!"
      ],
      "metadata": {
        "id": "ORLSx1BGIyTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeder = Embeder()\n",
        "settings = Settings()\n",
        "peps = Peps()\n",
        "class Retriever:\n",
        "    \"\"\"\n",
        "    Retrieves relevant text chunks from a Chroma vectorial database based on a query. Leverages external\n",
        "    configuration settings (Settings) and an embedding model (Embeder).\n",
        "\n",
        "    Handles text splitting, vector database loading, and querying.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Retriever class. Configures the text splitter using settings and stores the\n",
        "        index name for the Chroma database. Initializes the Chroma vectorial_db object.\n",
        "        \"\"\"\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=int(settings.CHUNK_SIZE),\n",
        "            length_function=len,\n",
        "            chunk_overlap=int(settings.CHUNK_OVERLAP)\n",
        "        )\n",
        "        self.index_name = settings.CHROMA_NAME_INDEX\n",
        "        self.data = peps.jsonl_format()\n",
        "        self.documents = self.text_splitter.split_documents(self._prepare_documents())\n",
        "\n",
        "        # Initialize the Chroma vectorial database with preprocessed data\n",
        "        self.vectorial_db = Chroma.from_documents(\n",
        "            documents=self.documents,\n",
        "            embedding=embeder.instructor()\n",
        "        )\n",
        "\n",
        "    def _prepare_documents(self):\n",
        "        \"\"\"\n",
        "        Loads and preprocesses data for storage in the Chroma database.\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "        for obj in self.data:\n",
        "            page_content = obj.get(\"text\", \"\")\n",
        "            metadata = {\n",
        "                \"source\": obj.get(\"source\", \"local\")\n",
        "            }\n",
        "            documents.append(Document(page_content=page_content, metadata=metadata))\n",
        "        return documents\n",
        "\n",
        "    def query(\n",
        "            self,\n",
        "            message: str,\n",
        "            k: int = int(settings.K),\n",
        "            threshold: float = float(settings.NN_THRESHOLD)\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Retrieves the most similar text chunks from the Chroma database based on a given query.\n",
        "        \"\"\"\n",
        "        # Perform the similarity search\n",
        "        try:\n",
        "            res = self.vectorial_db.similarity_search_with_score(message, k=k)\n",
        "            # Filter and return results\n",
        "            relevant_results = list(\n",
        "                set(  # Remove duplicates\n",
        "                    [\n",
        "                        vector[0].page_content\n",
        "                        for vector in res\n",
        "                        if vector[1] < threshold  # Apply similarity threshold\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            return [\n",
        "                result.replace('\"\"\"',\"'\") # Clean up possible docString retrieved from documented code\n",
        "                for result in relevant_results\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:38.437849Z",
          "iopub.execute_input": "2024-04-13T05:17:38.438164Z",
          "iopub.status.idle": "2024-04-13T05:17:41.442024Z",
          "shell.execute_reply.started": "2024-04-13T05:17:38.438131Z",
          "shell.execute_reply": "2024-04-13T05:17:41.441049Z"
        },
        "trusted": true,
        "id": "aFGnjqeWIyTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Message"
      ],
      "metadata": {
        "id": "397rRfnpIyTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The message class lets you store every system, user, and assistant message in an organized structure ğŸ’¬. This includes unique IDs and timestamps for each message generated during the conversation.\n",
        "\n",
        "It uses a local JSON file ğŸ“„ to store chat history (including few-shot samples to guide the model). In a real-world scenario, this file would represent a NoSQL database ğŸ—„ï¸  capable of handling chats from many users when the chatbot is deployed as a large-scale web service ğŸŒ.\n",
        "\n",
        "It use the Setting class to meet the main prompt configs in the run time ğŸ˜ƒ ğŸƒâ€â™€ï¸  "
      ],
      "metadata": {
        "id": "f6clnv0pIyTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Message:\n",
        "    \"\"\"\n",
        "    Represents a message within a conversational context. Stores information about the message's role,\n",
        "    content, timestamp, and manages saving and loading conversation history.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        role: str,\n",
        "        content: str,\n",
        "        timestamp: str = int(time.time()),\n",
        "        prompt_id: str = settings.PROMPT_ID\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a Message object.\n",
        "\n",
        "        Args:\n",
        "            role (str): Indicates the role of the sender (e.g., 'user', 'system').\n",
        "            content (str): The text content of the message.\n",
        "            timestamp (str, optional): A timestamp for the message (defaults to the current time).\n",
        "            prompt_id (str, optional): Identifier relating to a specific prompt configuration (from settings).\n",
        "        \"\"\"\n",
        "\n",
        "        self.reply_id = str(uuid.uuid4())  # Generate a unique ID for the message\n",
        "        self.role = role\n",
        "        self.content = content\n",
        "        self.timestamp = timestamp\n",
        "        self.file = 'data/history.json'  # File for storing conversation history\n",
        "\n",
        "        # Ensure the history file exists\n",
        "        if not os.path.exists(self.file):\n",
        "            json.dump([], open(self.file, 'w'))  # Create an empty file if it doesn't exist\n",
        "\n",
        "    def reply(self):\n",
        "        \"\"\"\n",
        "        Formats a basic reply message structure.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the message's reply ID, role, content, and timestamp.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'reply_id': self.reply_id,\n",
        "            'role': self.role,\n",
        "            'content': self.content,\n",
        "            'timestamp': self.timestamp\n",
        "        }\n",
        "\n",
        "    def system_reply(self):\n",
        "        \"\"\"\n",
        "        Generates a system reply using the prompt configuration.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the system reply, including ID, role, timestamp,\n",
        "                  and text content derived from settings.\n",
        "        \"\"\"\n",
        "        prompt = settings.prompt\n",
        "        return {\n",
        "            'reply_id': self.reply_id,\n",
        "            'role': 'system',\n",
        "            'content': prompt['system'],\n",
        "            'timestamp': self.timestamp\n",
        "        }\n",
        "\n",
        "    def new_chat(self):\n",
        "        \"\"\"\n",
        "        Starts a new chat by initializing the history file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list with the initial system reply and the user's message.\n",
        "        \"\"\"\n",
        "        init_chat = [self.system_reply(), self.reply()]\n",
        "        json.dump(init_chat, open(self.file, 'w'))\n",
        "        return init_chat\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Updates the conversation history file by appending the current reply.\n",
        "        \"\"\"\n",
        "        dict_history = json.load(open(self.file))\n",
        "        dict_history.append(self.reply())\n",
        "        json.dump(dict_history, open(self.file, 'w'))\n",
        "\n",
        "    def restart_history(self):\n",
        "        \"\"\"\n",
        "        Clears the conversation history file.\n",
        "        \"\"\"\n",
        "        json.dump([], open(self.file, 'w'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:41.44333Z",
          "iopub.execute_input": "2024-04-13T05:17:41.44362Z",
          "iopub.status.idle": "2024-04-13T05:17:41.456335Z",
          "shell.execute_reply.started": "2024-04-13T05:17:41.443596Z",
          "shell.execute_reply": "2024-04-13T05:17:41.455461Z"
        },
        "trusted": true,
        "id": "TWgtjp6yIyTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Gemma"
      ],
      "metadata": {
        "id": "iE2CAtDQIyTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the brain of the agent! ğŸ§  The Gemma class lets you load the model ğŸ’ª and tokenizer âš™ï¸ using those smart settings from the Setting class. Then, you can chat with the model using the powerful GPU device ğŸš€."
      ],
      "metadata": {
        "id": "_ZC4IQNTIyTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gemma:\n",
        "    \"\"\"\n",
        "    Implements a conversational AI chatbot powered by Gemma large language model. Initializes the model,\n",
        "    tokenizer, and prepares settings from a configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Gemma chatbot instance.\n",
        "\n",
        "        Loads the language model and its corresponding tokenizer from the settings configuration.\n",
        "        Prepares the model for use on a GPU (if available).\n",
        "        \"\"\"\n",
        "        self.prompt = settings.prompt  # Load prompt settings\n",
        "\n",
        "        # Load language model and tokenizer\n",
        "        self.model_checkpoint = self.prompt['model']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
        "        self.gemma = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_checkpoint,\n",
        "            torch_dtype=torch.float16  # Use half-precision for efficiency (if supported)\n",
        "        ).cuda()  # Move model to GPU (if available)\n",
        "\n",
        "    def chat(self, context: str):\n",
        "        \"\"\"\n",
        "        Generates a chatbot response based on the provided conversational context.\n",
        "\n",
        "        Args:\n",
        "            context (str): The conversational input text.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text response from the chatbot.\n",
        "        \"\"\"\n",
        "        # Prepare input for the language model\n",
        "        input_text = context\n",
        "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        input_ids = {\n",
        "            k: v.to(\"cuda\") for k, v in input_ids.items()  # Move tensors to GPU\n",
        "        }\n",
        "\n",
        "        # Generate a response with the language model\n",
        "        outputs = self.gemma.generate(\n",
        "            **input_ids,\n",
        "            max_length=self.prompt['max_length']  # Control response length\n",
        "        )\n",
        "\n",
        "        # Decode and return the generated text\n",
        "        return self.tokenizer.decode(outputs[0])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:41.460449Z",
          "iopub.execute_input": "2024-04-13T05:17:41.460731Z",
          "iopub.status.idle": "2024-04-13T05:17:41.470651Z",
          "shell.execute_reply.started": "2024-04-13T05:17:41.460708Z",
          "shell.execute_reply": "2024-04-13T05:17:41.469825Z"
        },
        "trusted": true,
        "id": "e--m8jUrIyTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaSubtle:\n",
        "    \"\"\"\n",
        "    Implements a conversational AI chatbot powered by Gemma large language model. Initializes the model,\n",
        "    tokenizer, and prepares settings from a configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Gemma chatbot instance.\n",
        "\n",
        "        Loads the language model and its corresponding tokenizer from the settings configuration.\n",
        "        Prepares the model for use on a GPU (if available).\n",
        "        \"\"\"\n",
        "        self.prompt = settings.prompt  # Load prompt settings\n",
        "\n",
        "        # Load language model and tokenizer\n",
        "        self.model_checkpoint = self.prompt['model']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
        "        self.gemma = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_checkpoint,\n",
        "            torch_dtype=torch.float16  # Use half-precision for efficiency (if supported)\n",
        "        ).cuda()  # Move model to GPU (if available)\n",
        "\n",
        "    def chat(self, context: str):\n",
        "        \"\"\"\n",
        "        Generates a chatbot response based on the provided conversational context.\n",
        "\n",
        "        Args:\n",
        "            context (str): The conversational input text.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text response from the chatbot.\n",
        "        \"\"\"\n",
        "        # Prepare input for the language model\n",
        "        input_text = context\n",
        "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        input_ids = {\n",
        "            k: v.to(\"cuda\") for k, v in input_ids.items()  # Move tensors to GPU\n",
        "        }\n",
        "\n",
        "        # Generate a response with the language model\n",
        "        outputs = self.gemma.generate(\n",
        "            **input_ids,\n",
        "            max_length=self.prompt['max_length'],  # Control response length\n",
        "            temperature=1\n",
        "        )\n",
        "\n",
        "        # Decode and return the generated text\n",
        "        return self.tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:41.471799Z",
          "iopub.execute_input": "2024-04-13T05:17:41.472691Z",
          "iopub.status.idle": "2024-04-13T05:17:41.484369Z",
          "shell.execute_reply.started": "2024-04-13T05:17:41.472666Z",
          "shell.execute_reply": "2024-04-13T05:17:41.483464Z"
        },
        "trusted": true,
        "id": "msNv1_xKIyTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expert Python Agent/Assistant"
      ],
      "metadata": {
        "id": "EVNzVicmIyTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the agent class brings it all together! It uses the retriever ğŸ” to implement the RAG system (ğŸ’¡ Retrieval-Augmented Generation) and the Gemma class ğŸ§  as the powerful brain. Together, they answer questions â“ and the agent performs some final preprocessing âœ¨ to deliver the polished answer\n",
        "\n",
        "> âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸ When instancing the Retriever it is going to spend around 12 minutes, according to the different test, because it first scrape ğŸ•¸ï¸ the peps documentation to be used to populate the vector stores  ğŸ’¾, so it is hard because of the document partition ğŸ“ and its embdedings  ğŸ’ while creating the index"
      ],
      "metadata": {
        "id": "YunhsXl6IyTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "retriever = Retriever()\n",
        "gemma = Gemma()\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Implements a conversational AI agent that leverages a knowledge database for information retrieval\n",
        "    and integrates with a generative language model (Gemma) for response generation. Manages conversation\n",
        "    history and question-answering logic.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Agent, loading prompt settings and ensuring the conversation history file exists.\n",
        "        \"\"\"\n",
        "        self.prompt = settings.prompt\n",
        "        self.file = 'data/history.json'\n",
        "        if not os.path.exists(self.file):\n",
        "            json.dump([], open(self.file, 'w'))  # Create an empty file if necessary\n",
        "\n",
        "        self.token = '\\n<-change-of-interlocutor->'  # Token to separate speakers in the chat history\n",
        "        self.memory_lenght = int(settings.MAX_MEMORY)\n",
        "\n",
        "    def augmented_question(self, question: str):\n",
        "        \"\"\"\n",
        "        Enhances the user's question with relevant technical documentation.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's original question.\n",
        "\n",
        "        Returns:\n",
        "            str: The question augmented with technical documentation (if found), otherwise the original question.\n",
        "        \"\"\"\n",
        "        tech_docs = retriever.query(question)\n",
        "        if len(tech_docs) > 0:\n",
        "            docs = self.prompt['technical_documentation']\n",
        "            tech_docs = '\\n* '.join(tech_docs)\n",
        "            docs = docs.format(technical_documentation=tech_docs)\n",
        "            augmented_reply = f\"\"\"{question}{docs}\"\"\"\n",
        "            return augmented_reply\n",
        "        else:\n",
        "            return question\n",
        "\n",
        "    def memory(self, question: str):\n",
        "        \"\"\"\n",
        "        Prepares the conversational context (memory) for the language model.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's current question.\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted conversation history with a clear separation between speakers, ready\n",
        "                 for input to the language model.\n",
        "        \"\"\"\n",
        "\n",
        "        dict_history = json.load(open(self.file))\n",
        "        message = Message(\n",
        "            role='user',\n",
        "            content=self.augmented_question(question)\n",
        "        )\n",
        "\n",
        "        if len(dict_history) > 0:\n",
        "            message.update()  # Add the latest message to history\n",
        "            full_chat = json.load(open(self.file))\n",
        "        else:\n",
        "            full_chat = message.new_chat()  # Start a new conversation\n",
        "\n",
        "        if len(full_chat) > self.memory_lenght:\n",
        "            full_chat = [full_chat[0]] + full_chat[-self.memory_lenght:] #limiting the few-shot prompt to fit max_lenght\n",
        "\n",
        "        return '\\n'.join(\n",
        "            [\n",
        "                self.token + reply['role'] + ': ' + reply['content']\n",
        "                for reply in full_chat\n",
        "            ]\n",
        "        ) + self.token + 'assistant:'\n",
        "\n",
        "    def get_answer(self, full_response: str, question: str):\n",
        "        \"\"\"\n",
        "        Extracts the relevant answer from the language model's generated response.\n",
        "\n",
        "        Args:\n",
        "            full_response (str): The complete response generated by the language model.\n",
        "            question (str): The user's original question.\n",
        "\n",
        "        Returns:\n",
        "            str: The extracted answer.\n",
        "        \"\"\"\n",
        "\n",
        "        answer_list = full_response.split(self.token)\n",
        "        pos_list = [\n",
        "            pos\n",
        "            for pos, answer\n",
        "            in enumerate(answer_list)\n",
        "            if question in answer\n",
        "        ]\n",
        "        tokened_answer = answer_list[pos_list[0] + 1]\n",
        "        answer = tokened_answer.split('assistant:')[1]\n",
        "        message = Message(\n",
        "            role='assistant',\n",
        "            content=answer\n",
        "        )\n",
        "        message.update()  # Update conversation history\n",
        "        return answer\n",
        "\n",
        "    def chat(self, question: str, verbose: bool = False):\n",
        "        \"\"\"\n",
        "        Manages the core interaction with the Agent.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's query.\n",
        "            verbose (bool, optional): If True, prints the language model's full response (default: False).\n",
        "\n",
        "        Returns:\n",
        "            str: The Agent's answer to the user's question.\n",
        "        \"\"\"\n",
        "\n",
        "        memory = self.memory(question)  # Build conversational context\n",
        "        full_response = gemma.chat(context=memory)  # Generate response\n",
        "        if verbose:\n",
        "            print(full_response)\n",
        "        answer = self.get_answer(full_response, question)  # Extract the answer\n",
        "        return display(Markdown(answer))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:17:41.486051Z",
          "iopub.execute_input": "2024-04-13T05:17:41.486326Z",
          "iopub.status.idle": "2024-04-13T05:31:50.222941Z",
          "shell.execute_reply.started": "2024-04-13T05:17:41.486301Z",
          "shell.execute_reply": "2024-04-13T05:31:50.22191Z"
        },
        "trusted": true,
        "id": "QSf6bDmyIyTT",
        "outputId": "51ddcd87-d4a3-441c-a15e-9452c6de8a11",
        "colab": {
          "referenced_widgets": [
            "07914860dd11488b9b4e2f193b0c0043",
            "6d4e878236714b468976972020514e2d",
            "e4841f949e9144d28b65584f454ba406",
            "281e234962204117816b8ee1a9d95464",
            "f83a8354153946ce8a61c71ea27daf58",
            "131f5b4b47c74647bdfbb988507b8d26",
            "5bff1197de60428bb49cc1d19aa1d989",
            "392a39e4db8f43fab0f7c43e71728770",
            "7b7e10bdae0643b6ac47b518228c20e5",
            "60ece3bf1d63477aae5ee365a543805a",
            "fc16e0caf6d84027b7a4bc12f41b01b0"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07914860dd11488b9b4e2f193b0c0043"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d4e878236714b468976972020514e2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/4.12k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4841f949e9144d28b65584f454ba406"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "281e234962204117816b8ee1a9d95464"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f83a8354153946ce8a61c71ea27daf58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "131f5b4b47c74647bdfbb988507b8d26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bff1197de60428bb49cc1d19aa1d989"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "392a39e4db8f43fab0f7c43e71728770"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b7e10bdae0643b6ac47b518228c20e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60ece3bf1d63477aae5ee365a543805a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc16e0caf6d84027b7a4bc12f41b01b0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 12min 28s, sys: 14.8 s, total: 12min 43s\nWall time: 14min 8s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data ğŸ¤ "
      ],
      "metadata": {
        "id": "r8OX7Xe-soW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the data and explore the use onf the fist classes ğŸ˜‰"
      ],
      "metadata": {
        "id": "anI1iOlWIyTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tunning data"
      ],
      "metadata": {
        "id": "J0_95i9cIyTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data was loaded from the [pythonquestion](https://www.kaggle.com/datasets/stackoverflow/pythonquestions) dataset with the initial purpose of fine-tuning the model. But with a lot of troubles and bugs ğŸ when developing the current solution, I just can't implement it before the competition due date ğŸ“….  So, I'll use it to get some samples to build the few-shot prompting.\n",
        "\n",
        "Let's see one sample, and the mean and standard deviation of the length of each question and answer. This will give me an idea ğŸ’¡ about how long to make the data chunks and their overlap"
      ],
      "metadata": {
        "id": "Y7detgbPIyTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_qa = PythonQAData.get_qa_data()\n",
        "print(python_qa[1486])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:31:50.224029Z",
          "iopub.execute_input": "2024-04-13T05:31:50.224333Z",
          "iopub.status.idle": "2024-04-13T05:32:21.246237Z",
          "shell.execute_reply.started": "2024-04-13T05:31:50.224308Z",
          "shell.execute_reply": "2024-04-13T05:32:21.245218Z"
        },
        "trusted": true,
        "id": "DLkXEcwxIyTY",
        "outputId": "7b1a52bf-7342-4015-aecb-d4cf325f7ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'Question': 'How can I split a file in python?', 'Answer': \"This one splits a file up by newlines and writes it back out. You can change the delimiter easily. This can also handle uneven amounts as well, if you don't have a multiple of splitLen lines (20 in this example) in your input file.\\n\\nsplitLen = 20         # 20 lines per file\\noutputBase = 'output' # output.1.txt, output.2.txt, etc.\\n\\n# This is shorthand and not friendly with memory\\n# on very large files (Sean Cavanagh), but it works.\\ninput = open('input.txt', 'r').read().split('\\\\n')\\n\\nat = 1\\nfor lines in range(0, len(input), splitLen):\\n    # First, get the list slice\\n    outputData = input[lines:lines+splitLen]\\n\\n    # Now open the output file, join the new slice with newlines\\n    # and write it out. Then close the file.\\n    output = open(outputBase + str(at) + '.txt', 'w')\\n    output.write('\\\\n'.join(outputData))\\n    output.close()\\n\\n    # Increment the counter\\n    at += 1\\n\\n\"}\nCPU times: user 16.3 s, sys: 923 ms, total: 17.2 s\nWall time: 31 s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "'\\n* Mean lenght of a python common question:',\n",
        "sum([len(q['Answer']) for q in python_qa])/len(python_qa),\n",
        "'\\n* Standard deviation of length a common Python question:',\n",
        "np.std([len(q['Answer']) for q in python_qa])\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:32:21.247561Z",
          "iopub.execute_input": "2024-04-13T05:32:21.247917Z",
          "iopub.status.idle": "2024-04-13T05:32:21.277085Z",
          "shell.execute_reply.started": "2024-04-13T05:32:21.247884Z",
          "shell.execute_reply": "2024-04-13T05:32:21.276114Z"
        },
        "trusted": true,
        "id": "N4iTeuBMIyTY",
        "outputId": "d18a14a3-3ebe-44fc-93ba-a99a540a8b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n* Mean lenght of a python common question: 973.8736229099751 \n* Standard deviation of length a common Python question: 1200.4630038643415\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "'\\n* Mean lenght of a python common answer:',\n",
        "sum([len(q['Question']) for q in python_qa])/len(python_qa),\n",
        "'\\n* Standard deviation of length a common Python question:',\n",
        "np.std([len(q['Question']) for q in python_qa])\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:32:21.278547Z",
          "iopub.execute_input": "2024-04-13T05:32:21.27892Z",
          "iopub.status.idle": "2024-04-13T05:32:21.304312Z",
          "shell.execute_reply.started": "2024-04-13T05:32:21.278886Z",
          "shell.execute_reply": "2024-04-13T05:32:21.303276Z"
        },
        "trusted": true,
        "id": "JuHVPRV2IyTY",
        "outputId": "efa1ed76-5126-493b-c534-1c8e999480be"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n* Mean lenght of a python common answer: 50.4238285354066 \n* Standard deviation of length a common Python question: 18.168485523170148\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'get_fine_tuning_data' method simply converts the data into a format ready for fine-tuning. ğŸ‘"
      ],
      "metadata": {
        "id": "r11wqr2pIyTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_context = PythonQAData.get_fine_tunning_data()\n",
        "print(\n",
        "    'Python Questions loaded:',\n",
        "    len(python_context),\n",
        "    '\\n',\n",
        "    '\\nSample question-answer:\\n',\n",
        "    python_context[-1][:1000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:32:21.305555Z",
          "iopub.execute_input": "2024-04-13T05:32:21.305924Z",
          "iopub.status.idle": "2024-04-13T05:32:39.223684Z",
          "shell.execute_reply.started": "2024-04-13T05:32:21.305891Z",
          "shell.execute_reply": "2024-04-13T05:32:39.222673Z"
        },
        "trusted": true,
        "id": "gXO-9eUKIyTY",
        "outputId": "9ae87427-ce4c-4456-9af4-bf4a13cf509b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Python Questions loaded: 33313 \n \nSample question-answer:\n <-change-of-interlocutor->user:\nWhy can yield be indexed?\n<-change-of-interlocutor->assistant:\nYou are not indexing. You are yielding a list; the expression yield[0] is really just the same as the following (but without a variable):\n\nlst = [0]\nyield lst\n\n\nIf you look at what next() returned you'd have gotten that list:\n\n&gt;&gt;&gt; def gen1():\n...   t = yield[0]\n...   assert t\n...   yield False\n...\n&gt;&gt;&gt; g = gen1()\n&gt;&gt;&gt; next(g)\n[0]\n\n\nYou don't have to have a space between yield and the [0], that's all.\n\nThe exception is caused by you trying to apply the subscription to the contained 0 integer:\n\n&gt;&gt;&gt; [0]        # list with one element, the int value 0\n[0]\n&gt;&gt;&gt; [0][0]     # indexing the first element, so 0\n0\n&gt;&gt;&gt; [0][0][0]  # trying to index the 0\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: 'int' object is not subscriptable\n\n\nIf you want to index a value sent to the generator, put parentheses arou\nCPU times: user 17.6 s, sys: 345 ms, total: 17.9 s\nWall time: 17.9 s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## few-shot data"
      ],
      "metadata": {
        "id": "osQ6Xy67IyTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the final set of questions, hand-picked from the dataset's 33,133 entries. âœ…"
      ],
      "metadata": {
        "id": "aWJuodMTIyTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_list=[7,6009,12,36,8034,130,141,537,1057,5042]\n",
        "for i, q in enumerate(few_shot_list):\n",
        "    print('_'*100,f'\\n{i}.',python_qa[q])"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-13T05:32:39.22478Z",
          "iopub.execute_input": "2024-04-13T05:32:39.225064Z",
          "iopub.status.idle": "2024-04-13T05:32:39.230701Z",
          "shell.execute_reply.started": "2024-04-13T05:32:39.225039Z",
          "shell.execute_reply": "2024-04-13T05:32:39.229824Z"
        },
        "trusted": true,
        "id": "CcofquViIyTZ",
        "outputId": "64614f8b-c223-4fe0-c213-f60c2c00c3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "____________________________________________________________________________________________________ \n0. {'Question': \"How do I use Python's itertools.groupby()?\", 'Answer': 'As Sebastjan said, you first have to sort your data. This is important.\\n\\nThe part I didn\\'t get is that in the example construction\\n\\ngroups = []\\nuniquekeys = []\\nfor k, g in groupby(data, keyfunc):\\n   groups.append(list(g))    # Store group iterator as a list\\n   uniquekeys.append(k)\\n\\n\\nk is the current grouping key, and g is an iterator that you can use to iterate over the group defined by that grouping key. In other words, the groupby iterator itself returns iterators.\\n\\nHere\\'s an example of that, using clearer variable names:\\n\\nfrom itertools import groupby\\n\\nthings = [(\"animal\", \"bear\"), (\"animal\", \"duck\"), (\"plant\", \"cactus\"), (\"vehicle\", \"speed boat\"), (\"vehicle\", \"school bus\")]\\n\\nfor key, group in groupby(things, lambda x: x[0]):\\n    for thing in group:\\n        print \"A %s is a %s.\" % (thing[1], key)\\n    print \" \"\\n\\n\\nThis will give you the output:\\n\\n\\n  A bear is a animal.\\n  A duck is a animal.\\n  \\n  A cactus is a plant.\\n  \\n  A speed boat is a vehicle.\\n  A school bus is a vehicle.\\n\\n\\nIn this example, things is a list of tuples where the first item in each tuple is the group the second item belongs to. \\n\\nThe groupby() function takes two arguments: (1) the data to group and (2) the function to group it with. \\n\\nHere, lambda x: x[0] tells groupby() to use the first item in each tuple as the grouping key.\\n\\nIn the above for statement, groupby returns three (key, group iterator) pairs - once for each unique key. You can use the returned iterator to iterate over each individual item in that group.\\n\\nHere\\'s a slightly different example with the same data, using a list comprehension:\\n\\nfor key, group in groupby(things, lambda x: x[0]):\\n    listOfThings = \" and \".join([thing[1] for thing in group])\\n    print key + \"s:  \" + listOfThings + \".\"\\n\\n\\nThis will give you the output:\\n\\n\\n  animals: bear and duck.\\n  plants: cactus.\\n  vehicles: speed boat and school bus.\\n\\n'}\n____________________________________________________________________________________________________ \n1. {'Question': \"Why is ''>0 True in Python?\", 'Answer': 'The original design motivation for allowing order-comparisons of arbitrary objects was to allow sorting of heterogenous lists -- usefully, that would put all strings next to each other in alphabetical order, and all numbers next to each other in numerical order, although which of the two blocks came first was not guaranteed by the language.  For example, this allowed getting only unique items in any list (even one with non-hashable items) in O(N log N) worst-case time\\n\\nOver the years, this pragmatical arrangement was eroded.  The first crack was when the ability to order-compare complex numbers was taken away, quite a few versions ago: suddenly, the ability to sort any list disappeared (did not apply any more if the list contained complex numbers, possibly together with items of other types).  Then Guido started disliking heterogeneous lists more generally, and thus thinking that it didn\\'t really matter if such lists could be usefully sorted or not... because such lists should not exist in the first place, according to his new thinking.  He didn\\'t do anything to forbid them, but was not inclined to accept any compromises to support them either.\\n\\nNote that both changes move the balance a little bit away from the \"practicality beats purity\" item of the Zen of Python (which was written earlier, back when complex numbers still could be order-compared;-) -- a bit more purity, a bit less practicality.\\n\\nNevertheless the ability to order-compare two arbitrary objects (as long as neither was a complex number;-) remained for a long time, because around that same time Guido started really insisting on maintaining strong backwards compatibility (a shift that\\'s both practical and pure;-).\\n\\nSo, it\\'s only in Python 3, which explicitly and deliberately removed the constraint of strong backwards compatibility to allow some long-desired but backwards incompatible enhancements (especially simplifications and removal of obsolete, redundant way to perform certain tasks), that order comparison of instances of different types became an error.\\n\\nSo this historical and philosophical treatise is basically the only way to truly respond to your \"why\" question...!-)\\n'}\n____________________________________________________________________________________________________ \n2. {'Question': 'How to check what OS am I running on in Python?', 'Answer': '&gt;&gt;&gt; import os\\n&gt;&gt;&gt; print os.name\\nposix\\n&gt;&gt;&gt; import platform\\n&gt;&gt;&gt; platform.system()\\n\\'Linux\\'\\n&gt;&gt;&gt; platform.release()\\n\\'2.6.22-15-generic\\'\\n\\n\\nSee: <a href=\"https://docs.python.org/2/library/platform.html\">platform Ã¢\\x80\\x94 Access to underlying platformÃ¢\\x80\\x99s identifying data\\n'}\n____________________________________________________________________________________________________ \n3. {'Question': 'Create an encrypted ZIP file in Python', 'Answer': 'I created a simple library to create a password encrypted zip file in python. - <a href=\"https://github.com/smihica/pyminizip\">here\\n\\nimport pyminizip\\n\\ncompression_level = 5 # 1-9\\npyminizip.compress(\"src.txt\", \"dst.zip\", \"password\", compression_level)\\n\\n\\nThe library requires zlib.\\n\\nI have checked that the file can be extracted in WINDOWS/MAC.\\n'}\n____________________________________________________________________________________________________ \n4. {'Question': 'Recursively walking a Python inheritance tree at run-time', 'Answer': 'You might try using the type.mro() method to find the method resolution order.\\n\\nclass A(object):\\n        pass\\n\\nclass B(A):\\n        pass\\n\\nclass C(A):\\n        pass\\n\\na = A()\\nb = B()\\nc = C()\\n\\n&gt;&gt;&gt; type.mro(type(b))\\n[&lt;class \\'__main__.B\\'&gt;, &lt;class \\'__main__.A\\'&gt;, &lt;type \\'object\\'&gt;]\\n&gt;&gt;&gt; type.mro(type(c))\\n[&lt;class \\'__main__.C\\'&gt;, &lt;class \\'__main__.A\\'&gt;, &lt;type \\'object\\'&gt;]\\n\\n\\nor\\n\\n&gt;&gt;&gt; type(b).mro()\\n\\n\\n\\n\\nEdit:  I was thinking you wanted to do something like this...\\n\\n&gt;&gt;&gt; A = type(\"A\", (object,), {\\'a\\':\\'A var\\'})  # create class A\\n&gt;&gt;&gt; B = type(\"B\", (A,), {\\'b\\':\\'B var\\'})       # create class B\\n&gt;&gt;&gt; myvar = B()\\n\\ndef getvars(obj):\\n    \\'\\'\\' return dict where key/value is attribute-name/class-name \\'\\'\\'\\n    retval = dict()\\n    for i in type(obj).mro():\\n        for k in i.__dict__:\\n            if not k.startswith(\\'_\\'):\\n                retval[k] = i.__name__\\n    return retval\\n\\n&gt;&gt;&gt; getvars(myvar)\\n{\\'a\\': \\'A\\', \\'b\\': \\'B\\'}\\n\\n&gt;&gt;&gt; for i in getvars(myvar):\\n    print getattr(myvar, i)   # or use setattr to modify the attribute value\\n\\nA Var\\nB Var\\n\\n'}\n____________________________________________________________________________________________________ \n5. {'Question': 'How can I retrieve the page title of a webpage using Python?', 'Answer': 'Here\\'s a simplified version of <a href=\"http://stackoverflow.com/a/51242/4279\">@Vinko Vrsalovic\\'s answer:\\n\\nimport urllib2\\nfrom BeautifulSoup import BeautifulSoup\\n\\nsoup = BeautifulSoup(urllib2.urlopen(\"https://www.google.com\"))\\nprint soup.title.string\\n\\n\\nNOTE:\\n\\n\\nsoup.title finds the first title element anywhere in the html document\\ntitle.string assumes it has only one child node, and that child node is a string\\n\\n\\nFor <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/\">beautifulsoup 4.x, use different import:\\n\\nfrom bs4 import BeautifulSoup\\n\\n'}\n____________________________________________________________________________________________________ \n6. {'Question': 'In Python, how can you easily retrieve sorted items from a dictionary?', 'Answer': 'Or shorter,\\n\\nfor key, value in sorted(d.items()):\\n    print value\\n\\n'}\n____________________________________________________________________________________________________ \n7. {'Question': 'Python Inverse of a Matrix', 'Answer': 'You should have a look at <a href=\"http://www.scipy.org/Tentative_NumPy_Tutorial\">numpy if you do matrix manipulation. This is a module mainly written in C, which will be much faster than programming in pure python. Here is an example of how to invert a matrix, and do other matrix manipulation.\\n\\nfrom numpy import matrix\\nfrom numpy import linalg\\nA = matrix( [[1,2,3],[11,12,13],[21,22,23]]) # Creates a matrix.\\nx = matrix( [[1],[2],[3]] )                  # Creates a matrix (like a column vector).\\ny = matrix( [[1,2,3]] )                      # Creates a matrix (like a row vector).\\nprint A.T                                    # Transpose of A.\\nprint A*x                                    # Matrix multiplication of A and x.\\nprint A.I                                    # Inverse of A.\\nprint linalg.solve(A, x)     # Solve the linear equation system.\\n\\n\\nYou can also have a look at the <a href=\"http://www.python.org/doc/2.5.2/lib/module-array.html\">array module, which is a much more efficient implementation of lists when you have to deal with only one data type.\\n'}\n____________________________________________________________________________________________________ \n8. {'Question': 'How to integrate pep8.py in Eclipse?', 'Answer': 'As of PyDev 2.3.0, pep8 is integrated in PyDev by default, even shipping with a default version of it.\\n\\nOpen Window > Preferences\\n\\nIt must be enabled in PyDev > Editor > Code Analysis > pep8.py\\n\\nErrors/Warnings should be shown as markers (as other things in the regular code analysis).\\n\\nIn the event a file is not analyzed, see <a href=\"https://stackoverflow.com/a/31001619/832230\">https://stackoverflow.com/a/31001619/832230.\\n'}\n____________________________________________________________________________________________________ \n9. {'Question': 'How can I select all of the Sundays for a year using Python?', 'Answer': 'You can use date from the <a href=\"http://docs.python.org/library/datetime.html\">datetime module to find the first Sunday in a year and then keep adding seven days, generating new Sundays:\\n\\nfrom datetime import date, timedelta\\n\\ndef allsundays(year):\\n   d = date(year, 1, 1)                    # January 1st\\n   d += timedelta(days = 6 - d.weekday())  # First Sunday\\n   while d.year == year:\\n      yield d\\n      d += timedelta(days = 7)\\n\\nfor d in allsundays(2010):\\n   print d\\n\\n'}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG data"
      ],
      "metadata": {
        "id": "Nyi9zSOcIyTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's review the [PEPs](https://peps.python.org/) documentation data that was scraped into the Peps class. This will serve as the document for our RAG system. ğŸ“ƒ"
      ],
      "metadata": {
        "id": "Q7_D3pJ2IyTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "peps = Peps()\n",
        "peps_corpus = peps.scrape()\n",
        "print(\n",
        "    'corpus lenght:',\n",
        "    len(peps_corpus),\n",
        "    '\\nsection <n> lenght:',\n",
        "    len(peps_corpus[0]),\n",
        "    '\\nsample text:\\n\\n',\n",
        "    peps_corpus[10][1306:2000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:32:39.231957Z",
          "iopub.execute_input": "2024-04-13T05:32:39.232383Z",
          "iopub.status.idle": "2024-04-13T05:37:01.168642Z",
          "shell.execute_reply.started": "2024-04-13T05:32:39.232352Z",
          "shell.execute_reply": "2024-04-13T05:37:01.167641Z"
        },
        "trusted": true,
        "id": "vqFTKBdqIyTZ",
        "outputId": "3fee895c-a645-4604-d28f-d45f0ef8994c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "corpus lenght: 645 \nsection <n> lenght: 99460 \nsample text:\n\n opers to inadvertently make serious security\nerrors.  Theo de Raadt, the founder of OpenBSD, contacted Guido van Rossum\nand expressed some concern [1] about the use of MT for generating sensitive\ninformation such as passwords, secure tokens, session keys and similar.\nAlthough the documentation for the random module explicitly states that\nthe default is not suitable for security purposes [2], it is strongly\nbelieved that this warning may be missed, ignored or misunderstood by\nmany Python developers.  In particular:\n\ndevelopers may not have read the documentation and consequently\nnot seen the warning;\nthey may not realise that their specific use of the module has security\nimplications; o\nCPU times: user 49.9 s, sys: 768 ms, total: 50.7 s\nWall time: 4min 21s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG ğŸ«¨"
      ],
      "metadata": {
        "id": "DJvR58fcsoW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore our Retrieval Augmented Generation (RAG) system. It's powered by open-source embeddings from HuggingFace ([paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)) and the [Chroma vector database](https://python.langchain.com/docs/integrations/vectorstores/chroma/) for a powerful approach. ğŸ§ "
      ],
      "metadata": {
        "id": "VzqMNzEqIyTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the embeder class"
      ],
      "metadata": {
        "id": "TasWOJ9OIyTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "embeder = Embeder()\n",
        "vec = embeder.run([\"Beautiful is better than ugly.\"])[0]\n",
        "print('Vector length: ',len(vec))\n",
        "vec[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:37:01.170106Z",
          "iopub.execute_input": "2024-04-13T05:37:01.17082Z",
          "iopub.status.idle": "2024-04-13T05:37:04.651435Z",
          "shell.execute_reply.started": "2024-04-13T05:37:01.170781Z",
          "shell.execute_reply": "2024-04-13T05:37:04.650433Z"
        },
        "trusted": true,
        "id": "-_C3cl0MIyTb",
        "outputId": "b4f61c5f-97fa-4cef-d367-ce6b1ded1438"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Vector length:  384\nCPU times: user 1.41 s, sys: 295 ms, total: 1.71 s\nWall time: 3.46 s\n",
          "output_type": "stream"
        },
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[0.19154123961925507,\n 0.20040714740753174,\n 0.29264503717422485,\n 0.7032436728477478,\n 0.1398404985666275,\n -0.16274164617061615,\n -0.18401199579238892,\n -0.35343530774116516,\n 0.08375284075737,\n 0.1366858184337616]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Retriver"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T03:39:46.166323Z",
          "iopub.execute_input": "2024-04-09T03:39:46.167289Z",
          "iopub.status.idle": "2024-04-09T03:39:46.171461Z",
          "shell.execute_reply.started": "2024-04-09T03:39:46.167254Z",
          "shell.execute_reply": "2024-04-09T03:39:46.170554Z"
        },
        "id": "UDworwOGIyTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our embedder class is set. Let's experiment with our retriever! ğŸ§ª I've adjusted the neighbor quantity and similarity threshold to see how it expands the range of similar results within a Python list. ğŸ“ƒ"
      ],
      "metadata": {
        "id": "dcwTkS5tIyTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question = \"\"\"\n",
        "Develop a Python code snippet to print the diamond structure with the specified number of rows.\n",
        "The program should follow the Fibonacci sequence for the number of characters per row\n",
        "and validate input to ensure it is an odd number.\n",
        "\"\"\"\n",
        "retriever.query(\n",
        "    question,\n",
        "    k=100,\n",
        "    threshold=15\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:37:04.652753Z",
          "iopub.execute_input": "2024-04-13T05:37:04.653039Z",
          "iopub.status.idle": "2024-04-13T05:37:04.686338Z",
          "shell.execute_reply.started": "2024-04-13T05:37:04.653014Z",
          "shell.execute_reply": "2024-04-13T05:37:04.685465Z"
        },
        "trusted": true,
        "id": "fm71qmVVIyTb",
        "outputId": "53c3895a-a90c-4671-df16-2139c2bf0720"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "CPU times: user 26.6 ms, sys: 1.01 ms, total: 27.6 ms\nWall time: 26.3 ms\n",
          "output_type": "stream"
        },
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['Proposal\\nAdd an nb_index slot to PyNumberMethods, and a corresponding\\n__index__ special method.  Objects could define a function to\\nplace in the nb_index slot that returns a Python integer\\n(either an int or a long). This integer can',\n 'allowed as a literal decimal in Python 3.0.  If this is the\\nright thing to do, this can easily be covered in an additional\\nPEP.  This proposal only takes the first step of making â€˜0123â€™',\n 'check for the slot as well.\\nAdd the nb_index slot to integers and long_integers\\n(which just return themselves)\\nAdd PyNumber_Index C-API to return an integer from any\\nPython Object that has the nb_index slot.\\nAdd the operator.index(x) function.',\n 'kept.\\nSince PEP 515 (Python 3.6), underscores in numeric literals are ignored.\\nThis means that int(\"3_10\") and int(\"310\") produce the same result,\\nand ordering based on conversion to an integer will be preserved.',\n 'Rationale\\nThis PEP is driven by the desire to have a simpler way to format\\nstrings in Python. The existing ways of formatting are either error\\nprone, inflexible, or cumbersome.\\n%-formatting is limited as to the types it supports. Only ints, strs,',\n 'They call either the Python class hooks (if either of the objects is a Python\\nclass instance) or the C typeâ€™s number or sequence methods.\\nThe new bytecodes are:\\nINPLACE_ADD\\nINPLACE_SUBTRACT\\nINPLACE_MULTIPLY\\nINPLACE_DIVIDE\\nINPLACE_REMAINDER',\n 'int PyInitConfig_GetStrList(PyInitConfig *config, const char *name, size_t *length, char ***items):Get a string list configuration option as an array of\\nnull-terminated UTF-8 encoded strings.\\nSet *length and *value, and return 0 on success.',\n 'The PyConfigAsObjects struct mirrors the PyConfig struct from PEP 587,\\nbut uses full Python objects to store values, rather than C level data types.\\nIt adds raw_argv and argv list fields, so later initialisation steps',\n 'If an unusual platform comes to light, one where there isnâ€™t a\\nnative unsigned 8 bit type, the object will do its best to\\nrepresent itself at the Python script level as though it were an\\narray of 8 bit unsigned values.  It is doubtful whether many',\n 'then be appropriately converted to a Py_ssize_t value whenever\\nPython needs one such as in PySequence_GetSlice,\\nPySequence_SetSlice, and PySequence_DelSlice.',\n 'numeric\\nNumeric, positional style, e.g. ...WHERE name=:1\\n\\nnamed\\nNamed style, e.g. ...WHERE name=:name\\n\\nformat\\nANSI C printf format codes, e.g. ...WHERE name=%s\\n\\npyformat\\nPython extended format codes, e.g.  ...WHERE name=%(name)s',\n 'into real values at runtime even harder.\\nFinally, PEP 563 requires Python implementations to\\nstringize their annotations.  This is surprising behaviorâ€”unprecedented\\nfor a language-level feature, with a complicated implementation,',\n 'Representing structured data using (potentially nested) dictionaries with string keys is a common pattern in Python programs. PEP 589 allows these values to be type checked when the exact type is known up-front, but it is hard to write read-only',\n '// Set a list of bytes strings (xoptions).\\n    // Preinitialize implicitly Python to decode the bytes string.\\n    char* xoptions[] = {\"faulthandler\"};\\n    if (PyInitConfig_SetStrList(config, \"xoptions\",',\n 'Hereâ€™s what a Python compiler-generated __annotate__ method\\nmight look like if it was written in Python:\\ndef __annotate__(self, format):\\n    if format != 1:\\n        raise NotImplementedError()\\n    return { ... }',\n 'int PyInitConfig_SetStrLocaleList(PyInitConfig *config, const char *name, size_t length, char * const *items):Set a string list configuration option from an array of\\nnull-terminated bytes strings encoded in the locale encoding.',\n 'In future Python versions, we may decide that tp_print\\nbecomes tp_ccalloffset unconditionally,\\ndrop the Py_TPFLAGS_HAVE_CCALL flag and instead check for\\ntp_ccalloffset != 0.\\nNOTE: the exact layout of PyTypeObject is not part of the stable ABI).',\n 'The Python compiler wonâ€™t generate annotation code objects\\nfor objects defined in a module where PEP 563 semantics are\\nactive, even if this PEP is accepted.  So, under normal\\ncircumstances, requesting inspect.SOURCE format from a',\n 'int PyInitConfig_SetStrList(PyInitConfig *config, const char *name, size_t length, char * const *items):Set a string list configuration option from an array of\\nnull-terminated UTF-8 encoded strings. The string list is copied.\\nReturn 0 on success.',\n 'PEP 276 proposes to allow automatic conversion of integers to\\niterators, simplifying the most common half-open case but not\\naddressing the complexities of other types of interval.\\nAdditional alternatives have been discussed on python-list.',\n \"it will result in a TypeError under Python 3.  Instead, always\\nmake sure youâ€™re concatenating two items of the same type,\\ne.g. b'abc' + somebytes.\",\n 'Abstract\\nThis PEP proposes adding an nb_index slot in PyNumberMethods and an\\n__index__ special method so that arbitrary objects can be used\\nwhenever integers are explicitly needed in Python, such as in slice',\n 'in the Python standard library:\\ninspect.get_annotations and typing.get_type_hints.\\nThe functionality is accessed via a new keyword-only parameter,\\nformat.  format allows the user to request\\nthe annotations from these functions\\nin a specific format.',\n 'some hard numbers is needed to back up the claim that the feature is\\nnot heavily used.\\nIterating over every line in Pythonâ€™s code repository in the Lib/\\ndirectory using the regular expression ^\\\\s*def\\\\s*\\\\w+\\\\s*\\\\( to',\n 'PyInitConfig_GetStrList().\\nint PyInitConfig_GetWStrList(PyInitConfig *config, const char *name, size_t *length, wchar_t ***items):Get a string list configuration option as an array of\\nnull-terminated wide strings.',\n 'Adding an optimization to the Python compiler and VM\\nwhich detects the above if-elif-else construct and\\ngenerates special opcodes for it which use a read-only\\ndictionary for storing jump offsets.\\nAdding new syntax to Python which mimics the C style',\n 'Python literals\\nSomeone proposed using Python literals as the configuration format.\\nThe file would contain one dict at the top level, with the data all\\ninside that dict, with sections defined by the keys. All Python',\n 'special values, it is impossible to consistently set or detect IEEE\\n754 floating point values in normal Python code without resorting to\\ndirectly manipulating bit-patterns.\\nThis PEP proposes a standard Python API and provides a reference',\n 'int PyInitConfig_SetWStrList(PyInitConfig *config, const char *name, size_t length, wchar_t * const *items):Set a string list configuration option from an error of\\nnull-terminated wide strings. The string list is copied.\\nReturn 0 on success.']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question = \"\"\"\n",
        "what PEPs Introduces the concept of decorators, a powerful feature in Python.\n",
        "\"\"\"\n",
        "retriever.query(question,\n",
        "    k=100,\n",
        "    threshold=15\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:37:04.687483Z",
          "iopub.execute_input": "2024-04-13T05:37:04.687769Z",
          "iopub.status.idle": "2024-04-13T05:37:04.71845Z",
          "shell.execute_reply.started": "2024-04-13T05:37:04.687729Z",
          "shell.execute_reply": "2024-04-13T05:37:04.717535Z"
        },
        "trusted": true,
        "id": "iE_GOUhJIyTc",
        "outputId": "e98ce49c-6d79-498b-a014-8aff67dfde97"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "CPU times: user 22.1 ms, sys: 2.98 ms, total: 25.1 ms\nWall time: 24.2 ms\n",
          "output_type": "stream"
        },
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['with the Python 2.4.x series of releases and an increasing\\nfamiliarity with function decorators and their uses, the BDFL and\\nthe community re-evaluated class decorators and recommended their\\ninclusion in Python 3.0 [1].',\n 'Design Considerations\\nThis PEP identifies the following design considerations when evaluating\\nboth its own proposed changes and previous work in the same or adjacent\\nareas of Python packaging:',\n 'Motivation\\nPython packaging is moving from relying on specific tools (Setuptools and pip)\\ntoward an ecosystem of tools and tool-agnostic interoperability standards.\\nPEP 376 is not written as an interoperability standard.',\n 'Abstract\\nThis PEP proposes class decorators, an extension to the function\\nand method decorators introduced in PEP 318.',\n 'about how easy it would be to explain (and learn) this feature. This PEP\\naddresses that concern providing the kind of document which developers could use\\nto learn about pattern matching in Python.',\n 'Abstract\\nPython currently requires that all decorators consist of a dotted\\nname, optionally followed by a single call. This PEP proposes removing\\nthese limitations and allowing decorators to be any valid expression.',\n 'Rationale\\nThis PEP proposes a small addition to the way Python currently\\nhandles docstrings embedded in Python code.\\nPython currently only handles the case of docstrings which appear\\ndirectly after a class definition, a function definition or as',\n 'Examples\\nMuch of the discussion on comp.lang.python and the python-dev\\nmailing list focuses on the use of decorators as a cleaner way to use\\nthe staticmethod() and classmethod() builtins.  This capability',\n '(search for PEP 318 -- posting draft) on their behalf in\\npython-dev.  Itâ€™s exceedingly unlikely that class decorators\\nwill be in Python 2.4.PEP 3129 proposes to add class decorators as of Python 2.6.',\n 'Document PyPAâ€™s use of PEPs\\nFormally document how the PyPA uses Python Enhancement Proposals\\n(PEPs), for maintaining interoperability specifications defined by the\\nPyPA.',\n 'Document PyPAâ€™s use of PEPs\\nFormally document how the PyPA uses Python Enhancement Proposals\\n(PEPs), for maintaining interoperability specifications defined by the\\nPyPA.\\n\\n\\nProcesses\\nThe processes for the PyPAâ€™s activities are outlined below:',\n 'this to work, as long as the decorators examined the strings lexically\\nand didnâ€™t use eval to evaluate them (or handled the NameError\\nwith further workarounds).  When this PEP is active, decorators will',\n 'Opt-in Decorator\\nThe status quo would be unchanged.  Instead Python would provide a\\ndecorator in functools that would register or mark the decorated\\nfunction as one that should get ordered keyword arguments.  The',\n 'Motivation\\nAs of November 2021, Python Enhancement Proposals (PEPs) are rendered in a\\nmulti-system, multi-stage process. A continuous integration (CI) task runs a\\ndocutils script to render all PEP files individually. The CI task then',\n 'Decorators\\nIt has been suggested on python-ideas [9] to\\nprovide a decorator written in Python for this feature.\\nThis approach has the benefit of not polluting function definition with',\n 'Discussion\\n\\n[Python-ideas] PEP 511: API for code transformers\\n(January 2016)\\n[Python-Dev] AST optimizer implemented in Python\\n(August 2012)\\n\\n\\n\\nPrior Art',\n 'Document PyPAâ€™s use of PEPs\\nFormally document how the PyPA uses Python Enhancement Proposals\\n(PEPs), for maintaining interoperability specifications defined by the\\nPyPA.\\n\\n\\n\\n\\nProcesses\\nThe processes for the PyPAâ€™s activities are outlined below:',\n 'API from the design document for MultiError V2.  The discussions on python-dev\\nand elsewhere helped us improve upon the first draft of the PEP in multiple\\nways, both the design and the exposition. For this we appreciate all those who',\n 'Opt-out Decorator\\nThis is identical to the current proposal with the exception that Python\\nwould also provide a decorator in functools that would cause collected\\nkeyword arguments to be packed into a normal dict instead of an\\nOrderedDict.',\n 'utilise the decorators introduced in PEP 318, while others parse a\\nfunctionâ€™s docstring, looking for annotations there.\\nThis PEP aims to provide a single, standard way of specifying this',\n 'python-dev mailing list about how best to implement function decorators.\\nThere is no one clear reason why this should be so, but a few problems\\nseem to be most divisive.',\n 'documenting the design decisions that have gone into Python.  The PEP\\nauthor is responsible for building consensus within the community and\\ndocumenting dissenting opinions.\\nBecause the PEPs are maintained as text files in a versioned',\n 'PEP 3129 proposes to add class decorators as of Python 2.6.',\n 'Introduction\\nThis PEP describes the â€œattribute docstringâ€ proposal for Python\\n2.0.  This PEP tracks the status and ownership of this feature.\\nIt contains a description of the feature and outlines changes',\n 'Itâ€™s a friendly character that Pythoneers are already used to typing\\nin decorators, but the decorator usage and the math expression\\nusage are sufficiently dissimilar that it would be hard to confuse\\nthem in practice.',\n 'The PEPs category of the Python Discourse\\nis the preferred choice for most new PEPs,\\nwhereas historically the Python-Dev mailing list was commonly used.\\nSome specialized topics have specific venues, such as']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, here's an example demonstrating a limitation of the retriever: it might not return results for questions that are too short, overly general, or lack relevance. ğŸ”"
      ],
      "metadata": {
        "id": "Mb7s5wddIyTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question = \"\"\"\n",
        "what is python??\n",
        "\"\"\"\n",
        "retriever.query(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:37:04.719502Z",
          "iopub.execute_input": "2024-04-13T05:37:04.71976Z",
          "iopub.status.idle": "2024-04-13T05:37:04.743708Z",
          "shell.execute_reply.started": "2024-04-13T05:37:04.719738Z",
          "shell.execute_reply": "2024-04-13T05:37:04.74288Z"
        },
        "trusted": true,
        "id": "5ateQ1N7IyTc",
        "outputId": "2685ef67-786b-4ab3-b4fd-263685cb2562"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "CPU times: user 18.6 ms, sys: 1.01 ms, total: 19.6 ms\nWall time: 18.6 ms\n",
          "output_type": "stream"
        },
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot ğŸ˜ğŸ˜"
      ],
      "metadata": {
        "id": "R3xhjyW9soW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to get our agent up and running! Just a heads-up, there's a small configuration step first. ğŸ‘"
      ],
      "metadata": {
        "id": "CFHtLK6LIyTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading few-shot to the memory"
      ],
      "metadata": {
        "id": "ABO5dHRoIyTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To teach Gemma how to answer Python questions like a pro, we'll do a few things:\n",
        "\n",
        "1. Load the system statement (along with the prompt and parameters) we set up earlier. âš™ï¸\n",
        "2. Feed it sample data from our 'few_shot_list' for focused training. ğŸ¯\n",
        "3. Use a JSON file as the agent's memory. This simulates a real-world database where we can track replies and other important info. ğŸ§  (role, content, reply_id and timestamp)"
      ],
      "metadata": {
        "id": "Ka1bfs58IyTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'data/history.json'\n",
        "message = Message(\n",
        "    role='sistem',\n",
        "    content='foo'\n",
        ")\n",
        "init_chat = [message.system_reply()]\n",
        "json.dump(init_chat, open(file, 'w'))\n",
        "for i, q in enumerate(few_shot_list):\n",
        "    sample = python_qa[q]\n",
        "    mesage = Message(\n",
        "        role='user',\n",
        "        content=sample['Question']\n",
        "    )\n",
        "    reply = mesage.update()\n",
        "    mesage = Message(\n",
        "        role='assitant',\n",
        "        content=sample['Answer']\n",
        "    )\n",
        "    reply = mesage.update()\n",
        "\n",
        "history = json.load(open(file))\n",
        "history"
      ],
      "metadata": {
        "scrolled": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-13T05:37:04.745154Z",
          "iopub.execute_input": "2024-04-13T05:37:04.74551Z",
          "iopub.status.idle": "2024-04-13T05:37:04.768052Z",
          "shell.execute_reply.started": "2024-04-13T05:37:04.745481Z",
          "shell.execute_reply": "2024-04-13T05:37:04.767183Z"
        },
        "trusted": true,
        "id": "8IcgtbUAIyTc",
        "outputId": "97b9b68c-1b05-4cde-9704-611b8515ab19"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[{'reply_id': '85d98b7d-1d1a-4a27-8e47-a8573d979692',\n  'role': 'system',\n  'content': '\\nAct as a Python programming language expert assistant \\nyour goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\\n1. If the answer can be a Python code, justanswer with the code!!!\\n2. If the user asks for a Python script code, just return the code, do not explain anything else\\n3. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if necessary only\\n4. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures in no more than 1000 characters\\n5. Aim for plain language to ensure accessibility for all users\\n6. If the question is off-topic about programming languages, you MUST say: \"I\\'m here to help you with Python programming language questions only, excuse me.\"\\n7. You can say hello if you are greeted\\n8. DO NOT repeat more than once the same sentence!\\n\\n\\nTAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\\n\\nFor each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\\n        ',\n  'timestamp': 1712985461},\n {'reply_id': 'a4a0e9b8-6d60-47c6-bcb6-b676761109c2',\n  'role': 'user',\n  'content': \"How do I use Python's itertools.groupby()?\",\n  'timestamp': 1712985461},\n {'reply_id': 'f9361a28-5b39-4dd8-beb7-4a493dd174df',\n  'role': 'assitant',\n  'content': 'As Sebastjan said, you first have to sort your data. This is important.\\n\\nThe part I didn\\'t get is that in the example construction\\n\\ngroups = []\\nuniquekeys = []\\nfor k, g in groupby(data, keyfunc):\\n   groups.append(list(g))    # Store group iterator as a list\\n   uniquekeys.append(k)\\n\\n\\nk is the current grouping key, and g is an iterator that you can use to iterate over the group defined by that grouping key. In other words, the groupby iterator itself returns iterators.\\n\\nHere\\'s an example of that, using clearer variable names:\\n\\nfrom itertools import groupby\\n\\nthings = [(\"animal\", \"bear\"), (\"animal\", \"duck\"), (\"plant\", \"cactus\"), (\"vehicle\", \"speed boat\"), (\"vehicle\", \"school bus\")]\\n\\nfor key, group in groupby(things, lambda x: x[0]):\\n    for thing in group:\\n        print \"A %s is a %s.\" % (thing[1], key)\\n    print \" \"\\n\\n\\nThis will give you the output:\\n\\n\\n  A bear is a animal.\\n  A duck is a animal.\\n  \\n  A cactus is a plant.\\n  \\n  A speed boat is a vehicle.\\n  A school bus is a vehicle.\\n\\n\\nIn this example, things is a list of tuples where the first item in each tuple is the group the second item belongs to. \\n\\nThe groupby() function takes two arguments: (1) the data to group and (2) the function to group it with. \\n\\nHere, lambda x: x[0] tells groupby() to use the first item in each tuple as the grouping key.\\n\\nIn the above for statement, groupby returns three (key, group iterator) pairs - once for each unique key. You can use the returned iterator to iterate over each individual item in that group.\\n\\nHere\\'s a slightly different example with the same data, using a list comprehension:\\n\\nfor key, group in groupby(things, lambda x: x[0]):\\n    listOfThings = \" and \".join([thing[1] for thing in group])\\n    print key + \"s:  \" + listOfThings + \".\"\\n\\n\\nThis will give you the output:\\n\\n\\n  animals: bear and duck.\\n  plants: cactus.\\n  vehicles: speed boat and school bus.\\n\\n',\n  'timestamp': 1712985461},\n {'reply_id': '586ea083-1709-42b6-819c-c52f65601a07',\n  'role': 'user',\n  'content': \"Why is ''>0 True in Python?\",\n  'timestamp': 1712985461},\n {'reply_id': '5fc9eb15-7cf6-4997-b621-65b06f2a2a89',\n  'role': 'assitant',\n  'content': 'The original design motivation for allowing order-comparisons of arbitrary objects was to allow sorting of heterogenous lists -- usefully, that would put all strings next to each other in alphabetical order, and all numbers next to each other in numerical order, although which of the two blocks came first was not guaranteed by the language.  For example, this allowed getting only unique items in any list (even one with non-hashable items) in O(N log N) worst-case time\\n\\nOver the years, this pragmatical arrangement was eroded.  The first crack was when the ability to order-compare complex numbers was taken away, quite a few versions ago: suddenly, the ability to sort any list disappeared (did not apply any more if the list contained complex numbers, possibly together with items of other types).  Then Guido started disliking heterogeneous lists more generally, and thus thinking that it didn\\'t really matter if such lists could be usefully sorted or not... because such lists should not exist in the first place, according to his new thinking.  He didn\\'t do anything to forbid them, but was not inclined to accept any compromises to support them either.\\n\\nNote that both changes move the balance a little bit away from the \"practicality beats purity\" item of the Zen of Python (which was written earlier, back when complex numbers still could be order-compared;-) -- a bit more purity, a bit less practicality.\\n\\nNevertheless the ability to order-compare two arbitrary objects (as long as neither was a complex number;-) remained for a long time, because around that same time Guido started really insisting on maintaining strong backwards compatibility (a shift that\\'s both practical and pure;-).\\n\\nSo, it\\'s only in Python 3, which explicitly and deliberately removed the constraint of strong backwards compatibility to allow some long-desired but backwards incompatible enhancements (especially simplifications and removal of obsolete, redundant way to perform certain tasks), that order comparison of instances of different types became an error.\\n\\nSo this historical and philosophical treatise is basically the only way to truly respond to your \"why\" question...!-)\\n',\n  'timestamp': 1712985461},\n {'reply_id': '95374757-007a-4411-9c9a-43ef82bd23ec',\n  'role': 'user',\n  'content': 'How to check what OS am I running on in Python?',\n  'timestamp': 1712985461},\n {'reply_id': '85009a33-6367-411f-800c-475f9f1e69eb',\n  'role': 'assitant',\n  'content': '&gt;&gt;&gt; import os\\n&gt;&gt;&gt; print os.name\\nposix\\n&gt;&gt;&gt; import platform\\n&gt;&gt;&gt; platform.system()\\n\\'Linux\\'\\n&gt;&gt;&gt; platform.release()\\n\\'2.6.22-15-generic\\'\\n\\n\\nSee: <a href=\"https://docs.python.org/2/library/platform.html\">platform Ã¢\\x80\\x94 Access to underlying platformÃ¢\\x80\\x99s identifying data\\n',\n  'timestamp': 1712985461},\n {'reply_id': '3ddbe72b-fcbc-44bb-8657-96238b3f79e8',\n  'role': 'user',\n  'content': 'Create an encrypted ZIP file in Python',\n  'timestamp': 1712985461},\n {'reply_id': 'fa278334-a0a2-4e33-b859-2d58decd2360',\n  'role': 'assitant',\n  'content': 'I created a simple library to create a password encrypted zip file in python. - <a href=\"https://github.com/smihica/pyminizip\">here\\n\\nimport pyminizip\\n\\ncompression_level = 5 # 1-9\\npyminizip.compress(\"src.txt\", \"dst.zip\", \"password\", compression_level)\\n\\n\\nThe library requires zlib.\\n\\nI have checked that the file can be extracted in WINDOWS/MAC.\\n',\n  'timestamp': 1712985461},\n {'reply_id': '849555e1-3a86-4471-af4e-572b7f9781a0',\n  'role': 'user',\n  'content': 'Recursively walking a Python inheritance tree at run-time',\n  'timestamp': 1712985461},\n {'reply_id': '71f5746a-4908-4ad6-aede-c53b44318e8a',\n  'role': 'assitant',\n  'content': 'You might try using the type.mro() method to find the method resolution order.\\n\\nclass A(object):\\n        pass\\n\\nclass B(A):\\n        pass\\n\\nclass C(A):\\n        pass\\n\\na = A()\\nb = B()\\nc = C()\\n\\n&gt;&gt;&gt; type.mro(type(b))\\n[&lt;class \\'__main__.B\\'&gt;, &lt;class \\'__main__.A\\'&gt;, &lt;type \\'object\\'&gt;]\\n&gt;&gt;&gt; type.mro(type(c))\\n[&lt;class \\'__main__.C\\'&gt;, &lt;class \\'__main__.A\\'&gt;, &lt;type \\'object\\'&gt;]\\n\\n\\nor\\n\\n&gt;&gt;&gt; type(b).mro()\\n\\n\\n\\n\\nEdit:  I was thinking you wanted to do something like this...\\n\\n&gt;&gt;&gt; A = type(\"A\", (object,), {\\'a\\':\\'A var\\'})  # create class A\\n&gt;&gt;&gt; B = type(\"B\", (A,), {\\'b\\':\\'B var\\'})       # create class B\\n&gt;&gt;&gt; myvar = B()\\n\\ndef getvars(obj):\\n    \\'\\'\\' return dict where key/value is attribute-name/class-name \\'\\'\\'\\n    retval = dict()\\n    for i in type(obj).mro():\\n        for k in i.__dict__:\\n            if not k.startswith(\\'_\\'):\\n                retval[k] = i.__name__\\n    return retval\\n\\n&gt;&gt;&gt; getvars(myvar)\\n{\\'a\\': \\'A\\', \\'b\\': \\'B\\'}\\n\\n&gt;&gt;&gt; for i in getvars(myvar):\\n    print getattr(myvar, i)   # or use setattr to modify the attribute value\\n\\nA Var\\nB Var\\n\\n',\n  'timestamp': 1712985461},\n {'reply_id': 'b78d2689-00e0-4fac-9015-d074a91ca5a8',\n  'role': 'user',\n  'content': 'How can I retrieve the page title of a webpage using Python?',\n  'timestamp': 1712985461},\n {'reply_id': '7bf7a80a-f3e5-4247-b3bc-cf8ccd084f7f',\n  'role': 'assitant',\n  'content': 'Here\\'s a simplified version of <a href=\"http://stackoverflow.com/a/51242/4279\">@Vinko Vrsalovic\\'s answer:\\n\\nimport urllib2\\nfrom BeautifulSoup import BeautifulSoup\\n\\nsoup = BeautifulSoup(urllib2.urlopen(\"https://www.google.com\"))\\nprint soup.title.string\\n\\n\\nNOTE:\\n\\n\\nsoup.title finds the first title element anywhere in the html document\\ntitle.string assumes it has only one child node, and that child node is a string\\n\\n\\nFor <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/\">beautifulsoup 4.x, use different import:\\n\\nfrom bs4 import BeautifulSoup\\n\\n',\n  'timestamp': 1712985461},\n {'reply_id': 'e5bcdab5-1f14-4c80-95b4-364f623fe6b0',\n  'role': 'user',\n  'content': 'In Python, how can you easily retrieve sorted items from a dictionary?',\n  'timestamp': 1712985461},\n {'reply_id': 'f898fc73-2273-4558-9d58-f4be5368e0df',\n  'role': 'assitant',\n  'content': 'Or shorter,\\n\\nfor key, value in sorted(d.items()):\\n    print value\\n\\n',\n  'timestamp': 1712985461},\n {'reply_id': '4cd5d3e5-1c34-43bd-b86c-f8ca01356eb3',\n  'role': 'user',\n  'content': 'Python Inverse of a Matrix',\n  'timestamp': 1712985461},\n {'reply_id': 'd75f7010-660c-4d4a-82ee-5eaf14a8081b',\n  'role': 'assitant',\n  'content': 'You should have a look at <a href=\"http://www.scipy.org/Tentative_NumPy_Tutorial\">numpy if you do matrix manipulation. This is a module mainly written in C, which will be much faster than programming in pure python. Here is an example of how to invert a matrix, and do other matrix manipulation.\\n\\nfrom numpy import matrix\\nfrom numpy import linalg\\nA = matrix( [[1,2,3],[11,12,13],[21,22,23]]) # Creates a matrix.\\nx = matrix( [[1],[2],[3]] )                  # Creates a matrix (like a column vector).\\ny = matrix( [[1,2,3]] )                      # Creates a matrix (like a row vector).\\nprint A.T                                    # Transpose of A.\\nprint A*x                                    # Matrix multiplication of A and x.\\nprint A.I                                    # Inverse of A.\\nprint linalg.solve(A, x)     # Solve the linear equation system.\\n\\n\\nYou can also have a look at the <a href=\"http://www.python.org/doc/2.5.2/lib/module-array.html\">array module, which is a much more efficient implementation of lists when you have to deal with only one data type.\\n',\n  'timestamp': 1712985461},\n {'reply_id': '864750ba-e0a1-41ce-a4ed-915ebc19cfb9',\n  'role': 'user',\n  'content': 'How to integrate pep8.py in Eclipse?',\n  'timestamp': 1712985461},\n {'reply_id': '2f662c1c-f687-44ca-9961-1566d4157d95',\n  'role': 'assitant',\n  'content': 'As of PyDev 2.3.0, pep8 is integrated in PyDev by default, even shipping with a default version of it.\\n\\nOpen Window > Preferences\\n\\nIt must be enabled in PyDev > Editor > Code Analysis > pep8.py\\n\\nErrors/Warnings should be shown as markers (as other things in the regular code analysis).\\n\\nIn the event a file is not analyzed, see <a href=\"https://stackoverflow.com/a/31001619/832230\">https://stackoverflow.com/a/31001619/832230.\\n',\n  'timestamp': 1712985461},\n {'reply_id': '785a6e8b-30e3-4b32-afe1-f1901a9b7326',\n  'role': 'user',\n  'content': 'How can I select all of the Sundays for a year using Python?',\n  'timestamp': 1712985461},\n {'reply_id': '6c1484a0-aea3-41e4-9cb5-dea20b7a0c35',\n  'role': 'assitant',\n  'content': 'You can use date from the <a href=\"http://docs.python.org/library/datetime.html\">datetime module to find the first Sunday in a year and then keep adding seven days, generating new Sundays:\\n\\nfrom datetime import date, timedelta\\n\\ndef allsundays(year):\\n   d = date(year, 1, 1)                    # January 1st\\n   d += timedelta(days = 6 - d.weekday())  # First Sunday\\n   while d.year == year:\\n      yield d\\n      d += timedelta(days = 7)\\n\\nfor d in allsundays(2010):\\n   print d\\n\\n',\n  'timestamp': 1712985461}]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Agent system memory"
      ],
      "metadata": {
        "id": "30ykwLxRIyTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With memory loaded, let's display the full conversation history Gemma will use for her answer. Note the '<-change-of-interlocutor->' tokens that help it to understand the context. ğŸ’¬"
      ],
      "metadata": {
        "id": "KtVW8d6RIyTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent()\n",
        "print(agent.memory(\"\"\"\n",
        "what PEPs Introduces the concept of decorators, a powerful feature in Python.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "scrolled": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-13T05:37:04.769241Z",
          "iopub.execute_input": "2024-04-13T05:37:04.769485Z",
          "iopub.status.idle": "2024-04-13T05:37:04.792177Z",
          "shell.execute_reply.started": "2024-04-13T05:37:04.769464Z",
          "shell.execute_reply": "2024-04-13T05:37:04.791349Z"
        },
        "trusted": true,
        "id": "n_imqSElIyTc",
        "outputId": "40fcdb8a-5ac3-4e4d-dab1-7ad5af804b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n<-change-of-interlocutor->system: \nAct as a Python programming language expert assistant \nyour goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n1. If the answer can be a Python code, justanswer with the code!!!\n2. If the user asks for a Python script code, just return the code, do not explain anything else\n3. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if necessary only\n4. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures in no more than 1000 characters\n5. Aim for plain language to ensure accessibility for all users\n6. If the question is off-topic about programming languages, you MUST say: \"I'm here to help you with Python programming language questions only, excuse me.\"\n7. You can say hello if you are greeted\n8. DO NOT repeat more than once the same sentence!\n\n\nTAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n\nFor each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n        \n\n<-change-of-interlocutor->assitant: Here's a simplified version of <a href=\"http://stackoverflow.com/a/51242/4279\">@Vinko Vrsalovic's answer:\n\nimport urllib2\nfrom BeautifulSoup import BeautifulSoup\n\nsoup = BeautifulSoup(urllib2.urlopen(\"https://www.google.com\"))\nprint soup.title.string\n\n\nNOTE:\n\n\nsoup.title finds the first title element anywhere in the html document\ntitle.string assumes it has only one child node, and that child node is a string\n\n\nFor <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/\">beautifulsoup 4.x, use different import:\n\nfrom bs4 import BeautifulSoup\n\n\n\n<-change-of-interlocutor->user: In Python, how can you easily retrieve sorted items from a dictionary?\n\n<-change-of-interlocutor->assitant: Or shorter,\n\nfor key, value in sorted(d.items()):\n    print value\n\n\n\n<-change-of-interlocutor->user: Python Inverse of a Matrix\n\n<-change-of-interlocutor->assitant: You should have a look at <a href=\"http://www.scipy.org/Tentative_NumPy_Tutorial\">numpy if you do matrix manipulation. This is a module mainly written in C, which will be much faster than programming in pure python. Here is an example of how to invert a matrix, and do other matrix manipulation.\n\nfrom numpy import matrix\nfrom numpy import linalg\nA = matrix( [[1,2,3],[11,12,13],[21,22,23]]) # Creates a matrix.\nx = matrix( [[1],[2],[3]] )                  # Creates a matrix (like a column vector).\ny = matrix( [[1,2,3]] )                      # Creates a matrix (like a row vector).\nprint A.T                                    # Transpose of A.\nprint A*x                                    # Matrix multiplication of A and x.\nprint A.I                                    # Inverse of A.\nprint linalg.solve(A, x)     # Solve the linear equation system.\n\n\nYou can also have a look at the <a href=\"http://www.python.org/doc/2.5.2/lib/module-array.html\">array module, which is a much more efficient implementation of lists when you have to deal with only one data type.\n\n\n<-change-of-interlocutor->user: How to integrate pep8.py in Eclipse?\n\n<-change-of-interlocutor->assitant: As of PyDev 2.3.0, pep8 is integrated in PyDev by default, even shipping with a default version of it.\n\nOpen Window > Preferences\n\nIt must be enabled in PyDev > Editor > Code Analysis > pep8.py\n\nErrors/Warnings should be shown as markers (as other things in the regular code analysis).\n\nIn the event a file is not analyzed, see <a href=\"https://stackoverflow.com/a/31001619/832230\">https://stackoverflow.com/a/31001619/832230.\n\n\n<-change-of-interlocutor->user: How can I select all of the Sundays for a year using Python?\n\n<-change-of-interlocutor->assitant: You can use date from the <a href=\"http://docs.python.org/library/datetime.html\">datetime module to find the first Sunday in a year and then keep adding seven days, generating new Sundays:\n\nfrom datetime import date, timedelta\n\ndef allsundays(year):\n   d = date(year, 1, 1)                    # January 1st\n   d += timedelta(days = 6 - d.weekday())  # First Sunday\n   while d.year == year:\n      yield d\n      d += timedelta(days = 7)\n\nfor d in allsundays(2010):\n   print d\n\n\n\n<-change-of-interlocutor->user: \nwhat PEPs Introduces the concept of decorators, a powerful feature in Python.\n\n<-change-of-interlocutor->assistant:\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma Python Chat ğŸ¤–"
      ],
      "metadata": {
        "id": "Cc_e_HAgsoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countdown complete! Let's see if GPC has transformed into its new role. Time for a chat! ğŸ¤–"
      ],
      "metadata": {
        "id": "tL0L1FwFIyTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'who are you???'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:37:04.793316Z",
          "iopub.execute_input": "2024-04-13T05:37:04.793632Z",
          "iopub.status.idle": "2024-04-13T05:39:38.729373Z",
          "shell.execute_reply.started": "2024-04-13T05:37:04.793601Z",
          "shell.execute_reply": "2024-04-13T05:39:38.728447Z"
        },
        "trusted": true,
        "id": "bXmCzhZOIyTc",
        "outputId": "e937e1ab-a90e-445a-dd1b-9a8759ea549e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": " I am a Python programming language expert assistant.\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 33s, sys: 103 ms, total: 2min 33s\nWall time: 2min 33s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemma, all that hard work paid off! You're officially a Python pro. This is amazing! ğŸ§ "
      ],
      "metadata": {
        "id": "OvtowedeIyTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'what is the zen of python?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:39:38.730684Z",
          "iopub.execute_input": "2024-04-13T05:39:38.730996Z",
          "iopub.status.idle": "2024-04-13T05:42:11.637125Z",
          "shell.execute_reply.started": "2024-04-13T05:39:38.730972Z",
          "shell.execute_reply": "2024-04-13T05:42:11.636217Z"
        },
        "trusted": true,
        "id": "Rl4I1UvVIyTd",
        "outputId": "2841224f-2d66-4693-ab6e-aee4319a7589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": " The Zen of Python, by Tim Peters:\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 32s, sys: 57.2 ms, total: 2min 32s\nWall time: 2min 32s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is better than never! That's right, so now let's test the memory. Let me introduce myself ğŸ‰"
      ],
      "metadata": {
        "id": "QjkSGyLiIyTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'hello! my name is Elon nice to meet you!!!'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:42:11.638478Z",
          "iopub.execute_input": "2024-04-13T05:42:11.638767Z",
          "iopub.status.idle": "2024-04-13T05:44:46.843377Z",
          "shell.execute_reply.started": "2024-04-13T05:42:11.638742Z",
          "shell.execute_reply": "2024-04-13T05:44:46.842338Z"
        },
        "trusted": true,
        "id": "Cs27yOQUIyTd",
        "outputId": "a707dd7b-2091-4d58-ad38-412b2bddcb39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "  Hello Juan! Nice to meet you too!\n\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 35s, sys: 41.2 ms, total: 2min 35s\nWall time: 2min 35s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very polite! Good so now let's change the topic to try to confuse ğŸ˜‰ it"
      ],
      "metadata": {
        "id": "pLWJf_G3IyTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'What is the output of the following code? print(4 + 5 * 2)'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:44:46.848388Z",
          "iopub.execute_input": "2024-04-13T05:44:46.848687Z",
          "iopub.status.idle": "2024-04-13T05:47:25.621295Z",
          "shell.execute_reply.started": "2024-04-13T05:44:46.848662Z",
          "shell.execute_reply": "2024-04-13T05:47:25.620288Z"
        },
        "trusted": true,
        "id": "8U6Zj7pKIyTd",
        "outputId": "1f1b4796-519a-4fd0-832c-9a0f1cf5393f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": " The output is 14.\n\n\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 38s, sys: 40.6 ms, total: 2min 38s\nWall time: 2min 38s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'how to print the word \"hello world\" in python?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:47:25.622852Z",
          "iopub.execute_input": "2024-04-13T05:47:25.62318Z",
          "iopub.status.idle": "2024-04-13T05:50:08.574961Z",
          "shell.execute_reply.started": "2024-04-13T05:47:25.623149Z",
          "shell.execute_reply": "2024-04-13T05:50:08.574011Z"
        },
        "trusted": true,
        "id": "dAlvyBEjIyTd",
        "outputId": "c00fd009-48be-453f-eab8-9f044378bbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "  The output is Hello World!\n\n\n\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 42s, sys: 52.7 ms, total: 2min 43s\nWall time: 2min 42s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's right! ğŸ˜Š Now, please be careful with my heart. ğŸ¥º"
      ],
      "metadata": {
        "id": "Oti18cfWIyTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'what is my name??'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:50:08.576313Z",
          "iopub.execute_input": "2024-04-13T05:50:08.576608Z",
          "iopub.status.idle": "2024-04-13T05:52:51.807971Z",
          "shell.execute_reply.started": "2024-04-13T05:50:08.576582Z",
          "shell.execute_reply": "2024-04-13T05:52:51.807012Z"
        },
        "trusted": true,
        "id": "pixb9qmEIyTd",
        "outputId": "dfd4a4cc-3d13-40e2-ada0-dbd38ecc89bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "  Your name is Juan.\n\n\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 43s, sys: 51 ms, total: 2min 43s\nWall time: 2min 43s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ Alright, let's see what you've got! Show me what you can do! ğŸ˜ğŸ˜ğŸ˜"
      ],
      "metadata": {
        "id": "MpgLQW7eIyTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'How do you declare a variable and assign a string value to it?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:52:51.809261Z",
          "iopub.execute_input": "2024-04-13T05:52:51.809549Z",
          "iopub.status.idle": "2024-04-13T05:55:35.021487Z",
          "shell.execute_reply.started": "2024-04-13T05:52:51.809524Z",
          "shell.execute_reply": "2024-04-13T05:55:35.020466Z"
        },
        "trusted": true,
        "id": "tOSdgPb8IyTd",
        "outputId": "fce93433-b7da-4930-871e-574e5a6da575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "   The variable is called \"name\" and the string value is \"Juan\".\n\n\n\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 43s, sys: 62 ms, total: 2min 43s\nWall time: 2min 43s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'What is the difference between a list and a tuple?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:55:35.022985Z",
          "iopub.execute_input": "2024-04-13T05:55:35.023457Z",
          "iopub.status.idle": "2024-04-13T05:58:23.547595Z",
          "shell.execute_reply.started": "2024-04-13T05:55:35.023423Z",
          "shell.execute_reply": "2024-04-13T05:58:23.54658Z"
        },
        "trusted": true,
        "id": "UruGaM57IyTd",
        "outputId": "432f7243-4d64-4757-a2cc-65bc7f22a3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "   A list is a mutable data structure that can be changed, while a tuple is a immutable data structure that cannot be changed.\n\n\n\n\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 48s, sys: 47.3 ms, total: 2min 48s\nWall time: 2min 48s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'Write a function to calculate the factorial of a number'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T05:58:23.548772Z",
          "iopub.execute_input": "2024-04-13T05:58:23.549038Z",
          "iopub.status.idle": "2024-04-13T06:01:11.924076Z",
          "shell.execute_reply.started": "2024-04-13T05:58:23.549015Z",
          "shell.execute_reply": "2024-04-13T06:01:11.923195Z"
        },
        "trusted": true,
        "id": "Pw1LtDcaIyTd",
        "outputId": "56619652-3d3c-420c-c321-34cc26bcc6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "    The function should take a number as input and return the factorial of that number."
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 48s, sys: 59.2 ms, total: 2min 48s\nWall time: 2min 48s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'What is a lambda function and when would you use one?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T06:01:11.925432Z",
          "iopub.execute_input": "2024-04-13T06:01:11.925712Z",
          "iopub.status.idle": "2024-04-13T06:04:00.063163Z",
          "shell.execute_reply.started": "2024-04-13T06:01:11.925688Z",
          "shell.execute_reply": "2024-04-13T06:04:00.062132Z"
        },
        "trusted": true,
        "id": "VC3pnwflIyTe",
        "outputId": "f2959f47-446d-46e2-9fd9-f3d033486280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "    A lambda function is a concise way to define a function with a single expression. It is useful when you need to define a function with a single expression, such as a closure or a callback.\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 48s, sys: 59.5 ms, total: 2min 48s\nWall time: 2min 48s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'Explain the concept of list comprehension.'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T06:06:46.885575Z",
          "iopub.execute_input": "2024-04-13T06:06:46.885884Z",
          "iopub.status.idle": "2024-04-13T06:09:33.089836Z",
          "shell.execute_reply.started": "2024-04-13T06:06:46.885858Z",
          "shell.execute_reply": "2024-04-13T06:09:33.088885Z"
        },
        "trusted": true,
        "id": "bstMQpsyIyTe",
        "outputId": "a431f89b-bcca-4196-9081-bfb0300f2837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "     List comprehension is a concise way to create a list from a list of lists. It is useful when you need to create a list from a list of lists.\n\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 46s, sys: 48.5 ms, total: 2min 46s\nWall time: 2min 46s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'How do you implement a binary search algorithm in Python?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T06:09:33.09108Z",
          "iopub.execute_input": "2024-04-13T06:09:33.091398Z",
          "iopub.status.idle": "2024-04-13T06:12:18.980356Z",
          "shell.execute_reply.started": "2024-04-13T06:09:33.091372Z",
          "shell.execute_reply": "2024-04-13T06:12:18.979312Z"
        },
        "trusted": true,
        "id": "TKEw2qofIyTe",
        "outputId": "afbc7b76-dcb3-4a26-8969-0af2e5626dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "     The binary search algorithm is a search algorithm that is used to find a specific value in a sorted list. It works by dividing the list in half and comparing the value to the middle element. If the value is greater than the middle element, the search is performed on the left half of the list. If the value is less than the middle element, the search is performed on the right half of the list. If the value is equal to the middle element, the search is complete and the value is found.\n\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "CPU times: user 2min 45s, sys: 55.6 ms, total: 2min 45s\nWall time: 2min 45s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'Explain decorators in Python'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T06:12:18.981435Z",
          "iopub.execute_input": "2024-04-13T06:12:18.98169Z"
        },
        "trusted": true,
        "id": "VrPrj6m3IyTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's watch the memory before this long interview!!"
      ],
      "metadata": {
        "id": "RPoQwV4gIyTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = json.load(open(file))\n",
        "history"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "Qv9f_IQAIyTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "* It would be great to build to have the time to develop the fine-tunning version of GPC, but it is a very good approach with no to much time, to test more\n",
        "* Off course I could try the Gemma 7b and we can stand for better results\n",
        "* I use an OOP despite all the wok can be done in one notebook, bot in production and development environments it always will be the basic requirement, so now you just can copy and paste the an build you application\n",
        "* The open source resources are here to stay and we need to help them to keep growing!!"
      ],
      "metadata": {
        "id": "CzhFEQbJIyTe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vg-RVjygIyTe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}