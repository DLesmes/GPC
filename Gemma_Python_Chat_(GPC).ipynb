{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 726715,
          "sourceType": "datasetVersion",
          "datasetId": 262
        },
        {
          "sourceId": 11384,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 6216
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "name": "Gemma Python Chat (GPC)",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/Gemma_Python_Chat_(GPC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240402%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240402T165349Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da73e111bc4f4749f4a8520c56b9085abd9a3952517e4118ce55b2f987a2e24a89bd6198c4d649c840a75d21925bd792dc4c9b38b41dc02f784d27c16cf908b21d2229ff6001c413de4aafeb8491cd9542693ec62062654a9f53768ff1917a2832dd490cf4bc7525862a6c632495375f8a34724eea64eb5357d51e3aefe5776d08245c92669fc431e929f7f1a0f6bdf03e00d0181729d6427844703bec84eb56acd9f8ec282373e124de67bdf0c523dc52a8e384d5d896c4184aceed98fcb0afa368a4684e5e183a39fa95cab705732a5aa1c63f87a9e588741db8d51ecb432ad5ac730ea5d89444d717dc8d9fddc305a41b37a7ab0cc25c2535d4213d4eede62,pythonquestions:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F262%2F726715%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240402%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240402T165349Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D68363abefc107bf8c87775ca60ef7e940051b4a1cbe630394dfa1dc780990bcba3ec2db6a1bcbe19d2da6672b1a3491948aa5dfa98e9d2d9a7ca16b4a07167a6093b2f6c3f0e65b370cc457ed57ee2696291eda5adcb1f16ee953d9b8e6de0aca4c5b5505f894d3565893b7482a0a5cae4ef5bb9c4f632885836c86acf2c98b7252ee75f608593aac7dcd556ac37692b1780b787eb00c12b55d32f8b9892d5d47a39202094f686ea53d95191936656aabe2d67b0f21bb892f614ed49a2354ca78464dad33d376defb0ba10c050bdeb4bafe587e095ff007c02d9a98ede1e1eb16a63fea69a87d282ba59ea1d5a8c6909018a76bdad46d9e9c155acce769908a3,gemma/transformers/2b/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F6216%2F11384%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240402%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240402T165350Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5a10d36cbe8b87108d11b740a973c803d5862035f5ea66ddd532caa70e2b6dc925be820f91d9169e9902b47124bc2ebf6414245501e27202b0d33dc5ed98c66925d57d54b63dab6c034fa76344f8d66957f06486dabcab0fa1ad8560c5fdbc163714385f2dddfbcd5f2637533ddab48b80cc83aebfeeaaf3f068c8ac75aff6187412faa2c267f5d472a50acc5bbb17de32b32934e085e4d8d0ec43e6003ba437ae75b8a57a7a6cd08bc5b6407e6ac66a7d0b1dbade9f2571e8d1135ff6e07c083e969d7250dd1f91691bf87bb6228a5e15f3766f73f2f253f06320034f628167a84558da0b86642ca1e68baee27f8f2bde8f906c263b3f85fc02c44ab37035ca'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Yt4cehLRu0Iz"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/GPC_(Gemma_Python_Chat).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP ü§ñ Gemma Python Chatbot"
      ],
      "metadata": {
        "id": "Qx8hbihKsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/8xZNc32/Gemma.png)"
      ],
      "metadata": {
        "id": "B0RdGIyrsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gemma Python Chatbot üöÄüöÄ help you to answer common questions about the üêç Python programming language, powered by [Gemma 2B IT](https://blog.google/technology/developers/gemma-open-models/) updated with the Python Enhancement Proposal ([PEPs](https://peps.python.org/)) documentation using a Retrival-Augmented Generation ([RAG](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)) sponsored by a Chroma vectorial database using the open source embeddings [hkunlp/instructor-large](https://huggingface.co/hkunlp/instructor-large) of 768 entries, orchested by [langchaing](https://python.langchain.com/docs/use_cases/chatbots/) that is a framework that let you use differen models y les code changes"
      ],
      "metadata": {
        "id": "5C9kSlAVsoW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "5GhJQPvLsoW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.209\n",
        "!pip install chromadb==0.3.26\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install InstructorEmbedding==1.0.1"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-02T04:26:18.336254Z",
          "iopub.execute_input": "2024-04-02T04:26:18.33709Z",
          "iopub.status.idle": "2024-04-02T04:27:57.369757Z",
          "shell.execute_reply.started": "2024-04-02T04:26:18.337054Z",
          "shell.execute_reply": "2024-04-02T04:27:57.36858Z"
        },
        "trusted": true,
        "id": "tnDUC7Zmu0I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "# variables\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "# model\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "# data\n",
        "from datasets import load_dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# embeddings\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "# vector database\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "Vq5ZFhX7soW4",
        "execution": {
          "iopub.status.busy": "2024-04-02T04:29:50.774285Z",
          "iopub.execute_input": "2024-04-02T04:29:50.775135Z",
          "iopub.status.idle": "2024-04-02T04:30:12.477292Z",
          "shell.execute_reply.started": "2024-04-02T04:29:50.775102Z",
          "shell.execute_reply": "2024-04-02T04:30:12.47651Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"/kaggle/input/gemma/transformers/2b/2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, torch_dtype=torch.float16).cuda()"
      ],
      "metadata": {
        "id": "Q1QizKYCsoW4",
        "execution": {
          "iopub.status.busy": "2024-04-02T04:31:53.734608Z",
          "iopub.execute_input": "2024-04-02T04:31:53.736175Z",
          "iopub.status.idle": "2024-04-02T04:32:27.954373Z",
          "shell.execute_reply.started": "2024-04-02T04:31:53.736137Z",
          "shell.execute_reply": "2024-04-02T04:32:27.953545Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "Y8VRrN8fu0I4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions and answers data"
      ],
      "metadata": {
        "id": "YIpcJynpu0I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pythonQAData:\n",
        "    \"\"\"\n",
        "    Processes data from Questions and Answers CSV files to provide a structured Q&A format.\n",
        "\n",
        "    Attributes:\n",
        "        questions_path (str): Path to the Questions CSV file\n",
        "        answers_path (str): Path to the Answers CSV file\n",
        "        tags_path (str): Path to the tags CSV file\n",
        "\n",
        "    Methods:\n",
        "        load_data(): Loads the CSV data into DataFrames.\n",
        "        merge(): Cleans, merges, and formats the question and answer data.\n",
        "        get_formatted_qa(): Returns a list of formatted question-answer strings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.questions_path = '../input/pythonquestions/Questions.csv'\n",
        "        self.answers_path = '../input/pythonquestions/Answers.csv'\n",
        "        self.tags_path = '../input/pythonquestions/Tags.csv'\n",
        "        self.regex = r\"<\\/?[\\w\\s]*>\"\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Loads Questions and Answers data from CSV files.\"\"\"\n",
        "        df_questions = pd.read_csv(\n",
        "            self.questions_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Score',\n",
        "                'Title'\n",
        "            ]\n",
        "        )\n",
        "        df_answers = pd.read_csv(\n",
        "            self.answers_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'ParentId',\n",
        "                'Score',\n",
        "                'Body'\n",
        "            ]\n",
        "        )\n",
        "        df_tags = pd.read_csv(\n",
        "            self.tags_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Tag'\n",
        "            ]\n",
        "        )\n",
        "        return df_questions, df_answers, df_tags\n",
        "\n",
        "\n",
        "    def qa_data(self):\n",
        "        \"\"\"Cleans, merges, and formats the question and answer data.\"\"\"\n",
        "        df_questions, df_answers, df_tags = self.load_data()\n",
        "        # Rename\n",
        "        df_questions.rename(\n",
        "            columns={\n",
        "                'Title': 'Question',\n",
        "                'Score': 'question_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        df_answers.rename(\n",
        "            columns={\n",
        "                'Body': 'Answer',\n",
        "                'ParentId':'Id',\n",
        "                'Score': 'answer_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        # Filter by score\n",
        "        df_questions = df_questions[df_questions['question_score'] > 5].copy()\n",
        "        # Sort and deduplicate answers\n",
        "        df_answers = df_answers.sort_values(\n",
        "            'answer_score',\n",
        "            ascending=False\n",
        "        ).drop_duplicates(subset=['Id'])\n",
        "        # Merge\n",
        "        qa_data = df_questions.merge(\n",
        "            df_answers,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        ).merge(\n",
        "            df_tags,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        )\n",
        "        # filter for python  questions\n",
        "        qa_data = qa_data[qa_data['answer_score'] > 5]\n",
        "        df_qa_data = qa_data[qa_data['Tag']=='python']\n",
        "        return df_qa_data\n",
        "\n",
        "    def get_formatted_qa(self):\n",
        "        \"\"\"Returns a list of formatted question-answer strings.\"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        df_qa_data['Answer'] = df_qa_data['Answer'].apply(\n",
        "            lambda x: re.sub(\n",
        "                self.regex,\n",
        "                \"\",\n",
        "                x\n",
        "            )\n",
        "        )\n",
        "        data = [\n",
        "            f\"Question:\\n{row['Question']}\\n\\nAnswer:\\n{row['Answer']}\"\n",
        "            for index, row\n",
        "            in df_qa_data.iterrows()\n",
        "        ]\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "bSU-ohmssoW5",
        "execution": {
          "iopub.status.busy": "2024-04-02T04:34:14.367423Z",
          "iopub.execute_input": "2024-04-02T04:34:14.368076Z",
          "iopub.status.idle": "2024-04-02T04:34:14.381811Z",
          "shell.execute_reply.started": "2024-04-02T04:34:14.368043Z",
          "shell.execute_reply": "2024-04-02T04:34:14.38082Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Enhancement Proposals data"
      ],
      "metadata": {
        "id": "xGPONsPpu0I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Peps:\n",
        "    \"\"\"\n",
        "    Scrapes Python Enhancement Proposals (PEPs) from https://peps.python.org/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initiates the scraping process.\"\"\"\n",
        "        self.base_url = 'https://peps.python.org/'\n",
        "\n",
        "    def scraper(self, url):\n",
        "        \"\"\"\n",
        "        Fetches the HTML content of a given URL.\n",
        "\n",
        "        Args:\n",
        "            url (str): The URL to fetch.\n",
        "\n",
        "        Returns:\n",
        "            BeautifulSoup: A BeautifulSoup object representing the parsed HTML.\n",
        "        \"\"\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    def _fetch_pep_links(self):\n",
        "        \"\"\"Fetches links to individual PEP pages (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of PEP URLs.\n",
        "        \"\"\"\n",
        "        soup = self.scraper(self.base_url)\n",
        "        return list(\n",
        "            set(\n",
        "                [\n",
        "                    self.base_url + ref['href']\n",
        "                    for ref in soup.find_all('a', class_='pep reference internal')\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _download_peps(self):\n",
        "        \"\"\"Downloads the content of individual PEPs (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of BeautifulSoup objects representing individual PEPs.\n",
        "        \"\"\"\n",
        "        pep_links = self._fetch_pep_links()\n",
        "        return [self.scraper(pep_link) for pep_link in pep_links]\n",
        "\n",
        "    def scrape(self):\n",
        "        \"\"\"Extracts the relevant text content from each PEP.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of text strings, each representing the content of a PEP.\n",
        "        \"\"\"\n",
        "        pep_soups = self._download_peps()\n",
        "        return [\n",
        "            '\\n'.join([word.text for word in soup.find_all('section')])\n",
        "            for soup in pep_soups\n",
        "        ]\n",
        "\n",
        "    def jsonl_format(self):\n",
        "        \"\"\"Format the spcraped data to a jsonl format is it alist of dictionaries\n",
        "        Returns:\n",
        "            list: A list of dictionaries with the following:\n",
        "                page_content: The content of each section scraped\n",
        "                source: The linke where it was scraped\n",
        "        \"\"\"\n",
        "        source_links = self._fetch_pep_links()\n",
        "        pep_content = self.scrape()\n",
        "        pep_data = dict(zip(source_links,pep_content))\n",
        "        return [\n",
        "            {\n",
        "                'text': value,\n",
        "                'source': key\n",
        "            }\n",
        "            for key, value in pep_data.items()\n",
        "        ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:34:22.152627Z",
          "iopub.execute_input": "2024-04-02T04:34:22.152979Z",
          "iopub.status.idle": "2024-04-02T04:34:22.163947Z",
          "shell.execute_reply.started": "2024-04-02T04:34:22.152953Z",
          "shell.execute_reply": "2024-04-02T04:34:22.16275Z"
        },
        "trusted": true,
        "id": "hQdTeLFKu0I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "x80UkLjgu0I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeder:\n",
        "    \"\"\"\n",
        "    Creates embeddings (numerical representations) of text documents for various tasks like semantic search and\n",
        "    similarity comparison. Provides flexibility to choose between a large model for more comprehensive\n",
        "    embeddings or a smaller, faster model.\n",
        "\n",
        "    Attributes:\n",
        "\n",
        "    large (bool): Flag indicating whether to use the large embedding model (default: True).\n",
        "    model (str): Name of the embedding model to use.\n",
        "    device (str): Device to use for computations (default: \"cuda\").\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            large: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        init(self, large: bool = True) -> None\n",
        "            Initializes the class with the specified model size preference.\n",
        "        \"\"\"\n",
        "        self.large = large\n",
        "        if self.large:\n",
        "            self.model = \"hkunlp/instructor-large\"\n",
        "            self.device: str = \"cuda\"\n",
        "        else:\n",
        "            self.model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "    def instructor(self):\n",
        "        if self.large:\n",
        "            embed = HuggingFaceInstructEmbeddings(\n",
        "                model_name=self.model,\n",
        "                model_kwargs={\"device\": self.device}\n",
        "            )\n",
        "        else:\n",
        "            embed = SentenceTransformerEmbeddings(\n",
        "                model_name=self.model\n",
        "            )\n",
        "        return embed\n",
        "\n",
        "    def run(\n",
        "            self,\n",
        "            docs_list: list,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        run(self, docs_list: list) -> List[List[float]]:\n",
        "            Generates embeddings for the provided list of documents.\n",
        "            Returns a list of lists, where each inner list represents the embedding vector for a document.\n",
        "        \"\"\"\n",
        "        if docs_list is None:\n",
        "            docs_list = ['']\n",
        "        embed = self.instructor()\n",
        "\n",
        "        return embed.embed_documents(docs_list)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:34:26.303781Z",
          "iopub.execute_input": "2024-04-02T04:34:26.304644Z",
          "iopub.status.idle": "2024-04-02T04:34:26.312921Z",
          "shell.execute_reply.started": "2024-04-02T04:34:26.304609Z",
          "shell.execute_reply": "2024-04-02T04:34:26.311962Z"
        },
        "trusted": true,
        "id": "-W_AjLr3u0I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Settings:\n",
        "    user_secrets = UserSecretsClient()\n",
        "    CHUNK_SIZE = user_secrets.get_secret(\"CHUNK_SIZE\")\n",
        "    CHUNK_OVERLAP = user_secrets.get_secret(\"CHUNK_OVERLAP\")\n",
        "    CHROMA_NAME_INDEX = user_secrets.get_secret(\"CHROMA_NAME_INDEX\")\n",
        "    K = user_secrets.get_secret(\"K\")\n",
        "    NN_THRESHOLD = user_secrets.get_secret(\"NN_THRESHOLD\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:34:29.913111Z",
          "iopub.execute_input": "2024-04-02T04:34:29.913824Z",
          "iopub.status.idle": "2024-04-02T04:34:30.472223Z",
          "shell.execute_reply.started": "2024-04-02T04:34:29.913793Z",
          "shell.execute_reply": "2024-04-02T04:34:30.47146Z"
        },
        "trusted": true,
        "id": "JinthbSyu0I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeder = Embeder()\n",
        "settings = Settings()\n",
        "class Retriever:\n",
        "    \"\"\" retriever\n",
        "    Retrieves the embeddings from a vectorial database\n",
        "    \"\"\"\n",
        "    def __init__(self, data: list[dict]):\n",
        "        \"\"\" initialize the retriever\n",
        "        \"\"\"\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=int(settings.CHUNK_SIZE),\n",
        "            length_function=len,\n",
        "            chunk_overlap=int(settings.CHUNK_OVERLAP)\n",
        "        )\n",
        "        self.index_name = settings.CHROMA_NAME_INDEX\n",
        "        self.data = data\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"\n",
        "        Loads documents from the initialized data and returns them as a list of Document objects.\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "        for obj in self.data:\n",
        "            page_content = obj.get(\"text\", \"\")\n",
        "            metadata = {\n",
        "                \"source\": obj.get(\"source\", \"local\")\n",
        "            }\n",
        "            documents.append(Document(page_content=page_content, metadata=metadata))\n",
        "        return documents\n",
        "\n",
        "    def set(self):\n",
        "        \"\"\"\n",
        "         update the vectorial database\n",
        "        \"\"\"\n",
        "        data = self.load()\n",
        "        splitter = self.text_splitter\n",
        "        documents = splitter.split_documents(data)\n",
        "        instructor = embeder.instructor()\n",
        "        vector_db = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=instructor,\n",
        "            persist_directory=self.index_name\n",
        "        )\n",
        "        vector_db.persist()\n",
        "\n",
        "    def query(\n",
        "            self,\n",
        "            message: str,\n",
        "            k: int = int(settings.K),\n",
        "            threshold: float = float(settings.NN_THRESHOLD)\n",
        "    ):\n",
        "        \"\"\" retrieve the 2 more similar text chunks based on the message given\n",
        "        \"\"\"\n",
        "        vectorial_db = Chroma(\n",
        "            embedding_function=embeder.instructor(),\n",
        "            persist_directory=self.index_name\n",
        "        )\n",
        "        res = vectorial_db.similarity_search_with_score(message, k=k)\n",
        "        return [vector[0].page_content for vector in res if vector[1] < threshold]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:52:19.644213Z",
          "iopub.execute_input": "2024-04-02T04:52:19.645139Z",
          "iopub.status.idle": "2024-04-02T04:52:19.656097Z",
          "shell.execute_reply.started": "2024-04-02T04:52:19.645107Z",
          "shell.execute_reply": "2024-04-02T04:52:19.655142Z"
        },
        "trusted": true,
        "id": "-pjlUbfru0I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "r8OX7Xe-soW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Data"
      ],
      "metadata": {
        "id": "MpJZ2yLMu0I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "peps = Peps()\n",
        "peps_corpus = peps.scrape()\n",
        "print(\n",
        "    'corpus lenght:',\n",
        "    len(peps_corpus),\n",
        "    '\\nsection <n> lenght:',\n",
        "    len(peps_corpus[0]),\n",
        "    '\\nsample text:\\n\\n',\n",
        "    peps_corpus[10][1306:2000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:38:26.852098Z",
          "iopub.execute_input": "2024-04-02T04:38:26.853036Z",
          "iopub.status.idle": "2024-04-02T04:39:56.683426Z",
          "shell.execute_reply.started": "2024-04-02T04:38:26.853003Z",
          "shell.execute_reply": "2024-04-02T04:39:56.682496Z"
        },
        "trusted": true,
        "id": "LVQ0gDRJu0I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "peps = Peps()\n",
        "data = peps.jsonl_format()\n",
        "data[-1]['text'][:2000]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:40:48.003271Z",
          "iopub.execute_input": "2024-04-02T04:40:48.003662Z",
          "iopub.status.idle": "2024-04-02T04:42:08.115039Z",
          "shell.execute_reply.started": "2024-04-02T04:40:48.003634Z",
          "shell.execute_reply": "2024-04-02T04:42:08.113982Z"
        },
        "trusted": true,
        "id": "Fe251Ptgu0I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fine-tunning data"
      ],
      "metadata": {
        "id": "ypsZan7lu0I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_context = PythonQAData.get_formatted_qa()\n",
        "print(\n",
        "    'Python Questions loaded:',\n",
        "    len(python_context),\n",
        "    '\\n',\n",
        "    '\\nSample question-answer:\\n',\n",
        "    python_context[-1][1306:2000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T04:43:16.279448Z",
          "iopub.execute_input": "2024-04-02T04:43:16.280393Z",
          "iopub.status.idle": "2024-04-02T04:43:48.343612Z",
          "shell.execute_reply.started": "2024-04-02T04:43:16.280357Z",
          "shell.execute_reply": "2024-04-02T04:43:48.342642Z"
        },
        "trusted": true,
        "id": "fHubbBYPu0I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "DJvR58fcsoW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "embeder = Embeder()\n",
        "vec = embeder.run([\"hello I'm goku\"])[0]\n",
        "print(len(vec))\n",
        "vec[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T20:56:43.783893Z",
          "iopub.execute_input": "2024-04-01T20:56:43.784946Z",
          "iopub.status.idle": "2024-04-01T20:56:45.229391Z",
          "shell.execute_reply.started": "2024-04-01T20:56:43.784896Z",
          "shell.execute_reply": "2024-04-01T20:56:45.228368Z"
        },
        "trusted": true,
        "id": "4R0qN3tmu0I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "retriever = Retriever(data)\n",
        "retriever.set()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T05:13:53.98249Z",
          "iopub.execute_input": "2024-04-02T05:13:53.982901Z",
          "iopub.status.idle": "2024-04-02T05:45:29.235927Z",
          "shell.execute_reply.started": "2024-04-02T05:13:53.982871Z",
          "shell.execute_reply": "2024-04-02T05:45:29.23471Z"
        },
        "trusted": true,
        "id": "PQLWOGdKu0I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = Retriever(data)\n",
        "question = \"what is a List comprehensions and what is it's relation with map() and filter()?\"\n",
        "retriever.query(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T05:55:36.416206Z",
          "iopub.execute_input": "2024-04-02T05:55:36.416588Z",
          "iopub.status.idle": "2024-04-02T05:55:41.166766Z",
          "shell.execute_reply.started": "2024-04-02T05:55:36.416562Z",
          "shell.execute_reply": "2024-04-02T05:55:41.165803Z"
        },
        "trusted": true,
        "id": "TxSf0IF4u0I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "R3xhjyW9soW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Use the model\n",
        "input_text = \"What is the data cut-off date for the Gemma Google models training dataset?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = {k: v.to(\"cuda\") for k, v in input_ids.items()}  if torch.cuda.is_available() else {k: v.to(\"cpu\")}\n",
        "outputs = model.generate(**input_ids, max_length=200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T04:13:29.90822Z",
          "iopub.execute_input": "2024-03-21T04:13:29.90861Z",
          "iopub.status.idle": "2024-03-21T04:13:35.388771Z",
          "shell.execute_reply.started": "2024-03-21T04:13:29.908582Z",
          "shell.execute_reply": "2024-03-21T04:13:35.387839Z"
        },
        "id": "P5DbvMzHsoW6",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Use the model\n",
        "input_text = \"if 5 t-shirts get dry in 10 hour ho long spend 30 t-shirts to get dry?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = {k: v.to(\"cuda\") for k, v in input_ids.items()}  if torch.cuda.is_available() else {k: v.to(\"cpu\")}\n",
        "outputs = model.generate(**input_ids, max_length=500)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "egQtE4VtsoW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.0.1"
      ],
      "metadata": {
        "id": "Cc_e_HAgsoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tunning"
      ],
      "metadata": {
        "id": "5Jl33mfjsoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.1.0"
      ],
      "metadata": {
        "id": "xUdtzcyTsoW6"
      }
    }
  ]
}