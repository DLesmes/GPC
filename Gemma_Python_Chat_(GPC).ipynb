{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 726715,
          "sourceType": "datasetVersion",
          "datasetId": 262
        },
        {
          "sourceId": 11384,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 6216
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Gemma Python Chat (GPC)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/Gemma_Python_Chat_(GPC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240413%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240413T165323Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dd3b41028f58b6327468eb329bb7d897cdf83fe11c8fdd24f53b624860a757a64d87af43fb8389bd1520c3f35b8f1251bf0f367718c578aefdd5db8f3c8cc8ca655713bb5d6b2d602b2e0f8d4d21637e12bde37a6154d0201c9fdb4b69e125b02aa9ce5cc03348354e9235f8ced78f597c514bbe77a7d3734dd13fe61317a96d7f06cf70f1c95ef32d8afd6f0e70902979b21683841b270ee03f2bd3f54c40d571fe732bc47e268221aa41b9e06b88cda1333c560f850e24b31917830e772144ea65fe61b80510a440752e4c9d381f9d958a18d892495f2c0696a4807a814f9913f0e96267a5655c8fed5c91dccd9138ffbd822501441feb243cb1ccc50a7ba39,pythonquestions:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F262%2F726715%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240413%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240413T165323Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5f2da956ec8148806902136ffe152dbed1d540a3b30458867c1241227a0c5f22689c78b46b5a8ccbc7d5ca7a06754ac76e5e8535b457bec7c6e8ae69205ea3dc95619bd44a46b5cf2f4e603580a5dd7adab623d47bdedee5e6dbeb068f2261b97ebd66d8573c7e83f91735527cae1d38b90a3917663b56ec70ef17c93ec9322090f9b76013d7be0a9de8d8de530aab6f7fb8ea334e92cd3f88c233ea0dc729a20c9e1928611f5e20adf6e04351a0f1a5e4b905ac84bf4ed8a04be69f3f3ba244aec2c37f3e3950c2b667b01bc2d49a7365693de68b3ee0bd78541f7c1618212b30f2c21bd6c287acf6293363641b5193ce724d3f1cc4f874ab650b588cb95b47,gemma/transformers/2b/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F6216%2F11384%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240413%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240413T165323Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3b64cf2d1690e2a7d4f635671e08a2552d6d881e730a5b35d09f62d1f4da402bf4b838c45f2592122322dd092a23d7f9d22e72f39c4f002d68bad9374ce4034d1f99dc3aa0185ade7e46df4d47cdc5c622c77cdcdfd8721e14e953f9d8139a4b55e2f47d212cbe32fa7cd580e7a11221e196d955a94cfaf15b7155e7a77248a830594b96846f54c30c9ba6be60c227447dba355f368bbe4c1b978d44ec61d66392e044ac8b51142f8a9a21192aabd8fec597239732c5695ac7970ff93bff5b94709b8ed1e696d724ce27cdd7215da920658a8cadb3f8bd29587b49db85c469da5b195b9d60a94a6b312784220c73324eb266687a8a64c4033ce71612bc27dcd2'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "VTmwIFdkYNEn"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/GPC_(Gemma_Python_Chat).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP ü§ñ Gemma Python Chatbot"
      ],
      "metadata": {
        "id": "Qx8hbihKsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.postimg.cc/Nfpn7mxR/gemma.png)"
      ],
      "metadata": {
        "id": "oSnBrnroYNEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** Run this notebook with GPU 100 Accelerator üòèüòè"
      ],
      "metadata": {
        "id": "HBiV87f6YNEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get answers to your burning Python questions with Gemma Python Chat! This helpful tool is powered by [Gemma 2B IT](https://blog.google/technology/developers/gemma-open-models/) and uses a clever \"few-shot\" strategy. It learns from real [Stackoverflow python question kaggle dataset](https://www.kaggle.com/datasets/stackoverflow/pythonquestions) and stays up-to-date with the latest Python Enhancement Proposal ([PEPs](https://peps.python.org/)). üìö\n",
        "\n",
        "### How does it work? ü§î\n",
        "\n",
        "Gemma Python Chat uses Retrival-Augmented Generation ([RAG](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)) with a [Chroma vectorial database](https://python.langchain.com/docs/integrations/vectorstores/chroma/) to find the most relevant information for your questions. It leverages the power of [paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) embeddings to understand the context and provide accurate answers. üß†\n",
        "All of this is orchestrated by [langchaing](https://python.langchain.com/docs/use_cases/chatbots/), a framework that makes it easy to work with different language models and keeps your code clean and flexible. ÔøΩ"
      ],
      "metadata": {
        "id": "u5msm_wtYNEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements üôà"
      ],
      "metadata": {
        "id": "5GhJQPvLsoW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb #!pip install chromadb==0.3.26\n",
        "# !pip install ydata-profiling #!pip install ydata-profiling==4.6.1\n",
        "!pip install langchain #!pip install langchain==0.0.345 ##!pip install langchain-core==0.1.31\n",
        "#!pip install pydantic  #!pip install pydantic==1.10.14\n",
        "!pip install sentence-transformers   #!pip install sentence-transformers==2.6.1\n",
        "!pip install InstructorEmbedding  #!pip install InstructorEmbedding==1.0.1"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "scrolled": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-13T16:26:19.967248Z",
          "iopub.execute_input": "2024-04-13T16:26:19.967864Z"
        },
        "trusted": true,
        "id": "ZExDJvv3YNEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import *\n",
        "from IPython.display import display, Markdown\n",
        "# variables\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "# model\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "# data\n",
        "from datasets import load_dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# embeddings\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "# vector database\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "Vq5ZFhX7soW4",
        "_kg_hide-output": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes üíÅüèΩ"
      ],
      "metadata": {
        "id": "dDKztCBaYNEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use object-oriented Python programming üíª to develop the chatbot, using the classes üìÅ in the corresponding order to support the Agent class that is the final service of the chatbot ü§ñ."
      ],
      "metadata": {
        "id": "ucQs8FzPYNEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions and answers data\n",
        "\n"
      ],
      "metadata": {
        "id": "qC_tC6TPYNEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class fit the data of [Stackoverflow python question kaggle dataset](https://www.kaggle.com/datasets/stackoverflow/pythonquestions) üìö to use it in a fine-tuned format ‚öôÔ∏è or a list of dicts üóÇÔ∏è... it depends on the case of use you need! it also will be use to select some samples to make the few-shoot strategy"
      ],
      "metadata": {
        "id": "Kz6hAFfhYNEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pythonQAData:\n",
        "    \"\"\"\n",
        "    Processes data from Questions and Answers CSV files to provide a structured Q&A format.\n",
        "\n",
        "    Attributes:\n",
        "        questions_path (str): Path to the Questions CSV file\n",
        "        answers_path (str): Path to the Answers CSV file\n",
        "        tags_path (str): Path to the tags CSV file\n",
        "\n",
        "    Methods:\n",
        "        load_data(): Loads the CSV data into DataFrames.\n",
        "        merge(): Cleans, merges, and formats the question and answer data.\n",
        "        get_formatted_qa(): Returns a list of formatted question-answer strings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.questions_path = '../input/pythonquestions/Questions.csv'\n",
        "        self.answers_path = '../input/pythonquestions/Answers.csv'\n",
        "        self.tags_path = '../input/pythonquestions/Tags.csv'\n",
        "        self.regex = r\"<\\/?[\\w\\s]*>\"\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Loads Questions and Answers data from CSV files.\"\"\"\n",
        "        df_questions = pd.read_csv(\n",
        "            self.questions_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Score',\n",
        "                'Title'\n",
        "            ]\n",
        "        )\n",
        "        df_answers = pd.read_csv(\n",
        "            self.answers_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'ParentId',\n",
        "                'Score',\n",
        "                'Body'\n",
        "            ]\n",
        "        )\n",
        "        df_tags = pd.read_csv(\n",
        "            self.tags_path,\n",
        "            encoding=\"ISO-8859-1\",\n",
        "            usecols=[\n",
        "                'Id',\n",
        "                'Tag'\n",
        "            ]\n",
        "        )\n",
        "        return df_questions, df_answers, df_tags\n",
        "\n",
        "\n",
        "    def qa_data(self):\n",
        "        \"\"\"Cleans, merges, and formats the question and answer data.\"\"\"\n",
        "        df_questions, df_answers, df_tags = self.load_data()\n",
        "        # Rename\n",
        "        df_questions.rename(\n",
        "            columns={\n",
        "                'Title': 'Question',\n",
        "                'Score': 'question_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        df_answers.rename(\n",
        "            columns={\n",
        "                'Body': 'Answer',\n",
        "                'ParentId':'Id',\n",
        "                'Score': 'answer_score'\n",
        "            },\n",
        "            inplace=True\n",
        "        )\n",
        "        # Filter by score\n",
        "        df_questions = df_questions[df_questions['question_score'] > 5].copy()\n",
        "        # Sort and deduplicate answers\n",
        "        df_answers = df_answers.sort_values(\n",
        "            'answer_score',\n",
        "            ascending=False\n",
        "        ).drop_duplicates(subset=['Id'])\n",
        "        # Merge\n",
        "        df_qa = df_questions.merge(\n",
        "            df_answers,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        ).merge(\n",
        "            df_tags,\n",
        "            how='left',\n",
        "            on='Id'\n",
        "        )\n",
        "        # filter for python  questions\n",
        "        df_qa = df_qa[df_qa['answer_score'] > 5].copy()\n",
        "        df_qa = df_qa[df_qa['Tag']=='python'].copy()\n",
        "        df_qa['Answer'] = df_qa['Answer'].apply(\n",
        "            lambda x: re.sub(\n",
        "                self.regex,\n",
        "                \"\",\n",
        "                x\n",
        "            )\n",
        "        )\n",
        "        return df_qa\n",
        "\n",
        "    def get_fine_tunning_data(self):\n",
        "        \"\"\"Returns a list of formatted user-assistant strings.\"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        data = [\n",
        "            f\"<-change-of-interlocutor->user:\\n{row['Question']}\\n<-change-of-interlocutor->assistant:\\n{row['Answer']}\"\n",
        "            for index, row\n",
        "            in df_qa_data.iterrows()\n",
        "        ]\n",
        "        return data\n",
        "    def get_qa_data(self):\n",
        "        \"\"\"Returns a list of records dictionaries \"\"\"\n",
        "        df_qa_data = self.qa_data()\n",
        "        data = df_qa_data[\n",
        "            [\n",
        "                'Question',\n",
        "                'Answer'\n",
        "            ]\n",
        "        ].to_dict(orient='records')\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "bSU-ohmssoW5",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Enhancement Proposals data üòâ"
      ],
      "metadata": {
        "id": "Lh20gby-YNEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class Peps scrape all the oficial [PEPs](https://peps.python.org/) üîé to get all the best practices of this programming language üêç and their latest features ‚ú®, disposed to uses in the RAG system for the chatbot"
      ],
      "metadata": {
        "id": "-cYc_dwBYNEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Peps:\n",
        "    \"\"\"\n",
        "    Scrapes Python Enhancement Proposals (PEPs) from https://peps.python.org/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initiates the scraping process.\"\"\"\n",
        "        self.base_url = 'https://peps.python.org/'\n",
        "\n",
        "    def scraper(self, url):\n",
        "        \"\"\"\n",
        "        Fetches the HTML content of a given URL.\n",
        "\n",
        "        Args:\n",
        "            url (str): The URL to fetch.\n",
        "\n",
        "        Returns:\n",
        "            BeautifulSoup: A BeautifulSoup object representing the parsed HTML.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            return BeautifulSoup(response.content, 'html.parser')\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    def _fetch_pep_links(self):\n",
        "        \"\"\"Fetches links to individual PEP pages (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of PEP URLs.\n",
        "        \"\"\"\n",
        "        soup = self.scraper(self.base_url)\n",
        "        return list(\n",
        "            set(\n",
        "                [\n",
        "                    self.base_url + ref['href']\n",
        "                    for ref in soup.find_all('a', class_='pep reference internal')\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _download_peps(self):\n",
        "        \"\"\"Downloads the content of individual PEPs (private method).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of BeautifulSoup objects representing individual PEPs.\n",
        "        \"\"\"\n",
        "        pep_links = self._fetch_pep_links()\n",
        "        soap_list = [self.scraper(pep_link) for pep_link in pep_links]\n",
        "        return [soap for soap in soap_list if soap is not None]\n",
        "\n",
        "    def scrape(self):\n",
        "        \"\"\"Extracts the relevant text content from each PEP.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of text strings, each representing the content of a PEP.\n",
        "        \"\"\"\n",
        "        pep_soups = self._download_peps()\n",
        "        return [\n",
        "            '\\n'.join([word.text.replace('\"\"\"',\"'\") for word in soup.find_all('section')])\n",
        "            for soup in pep_soups\n",
        "        ]\n",
        "\n",
        "    def jsonl_format(self):\n",
        "        \"\"\"Format the spcraped data to a jsonl format is it alist of dictionaries\n",
        "        Returns:\n",
        "            list: A list of dictionaries with the following:\n",
        "                page_content: The content of each section scraped\n",
        "                source: The linke where it was scraped\n",
        "        \"\"\"\n",
        "        source_links = self._fetch_pep_links()\n",
        "        pep_content = self.scrape()\n",
        "        pep_data = dict(zip(source_links,pep_content))\n",
        "        return [\n",
        "            {\n",
        "                'text': value,\n",
        "                'source': key\n",
        "            }\n",
        "            for key, value in pep_data.items()\n",
        "        ]"
      ],
      "metadata": {
        "trusted": true,
        "id": "u0fVAGeaYNEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "RkTv6e5TYNEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embeder class is the method that lets you embed the data given as a list of strings ‚û°Ô∏èüì¶. It will support the retriever for the RAG system üîç"
      ],
      "metadata": {
        "id": "cYhzZA-ZYNEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeder:\n",
        "    \"\"\"\n",
        "    Creates text document embeddings (numerical representations) for semantic search,\n",
        "    similarity comparison, and related natural language processing tasks. Offers the choice\n",
        "    between a larger model for richer embeddings or a smaller, more efficient model.\n",
        "\n",
        "    Attributes:\n",
        "        large (bool): Controls the size of the embedding model. When set to True, uses a\n",
        "                      larger model for richer embeddings (default: True).\n",
        "        model (str): The name of the embedding model to use. Changes based on the value of 'large'.\n",
        "        device (str): The device to use for computation. Options include \"cpu\" or, if available, \"cuda\"\n",
        "                      for GPU acceleration (default: \"cuda\").\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, large: bool = False):\n",
        "        \"\"\"\n",
        "        Initializes the Embeder instance with preferences for model size.\n",
        "\n",
        "        Args:\n",
        "            large (bool, optional): If True, initializes with the larger embedding model.\n",
        "                                    Defaults to False for the smaller, faster model.\n",
        "        \"\"\"\n",
        "        self.large = large\n",
        "        if self.large:\n",
        "            self.model = \"hkunlp/instructor-large\"\n",
        "            self.device = \"cuda\"  # If GPU is available\n",
        "        else:\n",
        "            self.model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "    def instructor(self):\n",
        "        \"\"\"\n",
        "        Configures the embedding pipeline based on the selected model size.\n",
        "\n",
        "        Returns:\n",
        "           An embedding object from either the HuggingFaceInstructEmbeddings class\n",
        "           (for the large model) or the SentenceTransformerEmbeddings class (for the smaller model).\n",
        "        \"\"\"\n",
        "        if self.large:\n",
        "            embed = HuggingFaceInstructEmbeddings(\n",
        "                model_name=self.model,\n",
        "                model_kwargs={\"device\": self.device}\n",
        "            )\n",
        "        else:\n",
        "            embed = SentenceTransformerEmbeddings(model_name=self.model)\n",
        "        return embed\n",
        "\n",
        "    def run(self, docs_list: list):\n",
        "        \"\"\"\n",
        "        Generates embeddings for a given list of text documents.\n",
        "\n",
        "        Args:\n",
        "            docs_list (list): A list of text documents to embed.\n",
        "\n",
        "        Returns:\n",
        "            List[List[float]]: A list of lists, where each inner list represents the\n",
        "                               numerical embedding for a corresponding document in the input.\n",
        "        \"\"\"\n",
        "        if docs_list is None:\n",
        "            docs_list = ['']  # Prevent errors with empty input\n",
        "\n",
        "        embed = self.instructor()  # Get the appropriate embedding object\n",
        "        return embed.embed_documents(docs_list)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vplXpV7CYNEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt üòé"
      ],
      "metadata": {
        "id": "5SaQLM5lYNEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a good practice, I consider the prompt as the instruction you use to get what you need from an LLM üß†. It includes all the parameters the LLM needs in every attempt.  Any change in a parameter will generate a different output üîÑ.  Therefore, the input is the whole prompt instruction plus all its parameters.\n",
        "\n",
        "Additionally, it's defined in an external object (like a NoSQL database üóÑÔ∏è). This allows for easy updates outside of the code, making changes simpler for non-technical people üëç."
      ],
      "metadata": {
        "id": "Eq-NLkPHYNEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_config = [{\n",
        "    \"prompt_id\": \"0.0.1\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 1,\n",
        "    \"max_length\": 250,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.2\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "6. DO NOT repeat more than twice the same sentence!\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 1,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.3\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "6. DO NOT repeat more than once the same sentence!\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.4\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if it is necesary\n",
        "2. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures in no more than 1000 characters\n",
        "3. Aim for plain language to ensure accessibility for all users\n",
        "4. If the question is off topic about programing lenguage you MUST to say \"I'm here to help you with Python programming language questions only, excuse me\"\n",
        "5. You can say hello if you are greeted\n",
        "6. DO NOT repeat more than once the same sentence!\n",
        "7. If the user ask to you for a python script code, just return the code, do not EXPLAIN anything else\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.5\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. If the answer can be a Python code, justanswer with the code!!!\n",
        "2. If the user asks for a Python script code, just return the code, do not explain anything else\n",
        "3. When responding, incorporate any relevant context to enhance the accuracy and informativeness of your answers using the documentation provided if necessary only\n",
        "4. Please structure your response to include definitions, examples, and any relevant comparisons to other statistical measures in no more than 1000 characters\n",
        "5. Aim for plain language to ensure accessibility for all users\n",
        "6. If the question is off-topic about programming languages, you MUST say: \"I'm here to help you with Python programming language questions only, excuse me.\"\n",
        "7. You can say hello if you are greeted\n",
        "8. DO NOT repeat more than once the same sentence!\n",
        "\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "},\n",
        "{\n",
        "    \"prompt_id\": \"0.0.6\",\n",
        "    \"supplier\": \"google\",\n",
        "    \"system\": \"\"\"\n",
        "Act as a Python programming language expert assistant\n",
        "your goal is to answer common questions in a clear, comprehensive, and accurate way, taking into account this guidelines:\n",
        "1. If the answer can be a Python code, justanswer with the code!!!\n",
        "2. If the user asks for a Python script code, just return the code, do not explain anything else and in no more than 1000 characters\n",
        "5. Aim for plain language to ensure accessibility for all users\n",
        "6. If the question is off-topic about programming languages, you MUST say: \"I'm here to help you with Python programming language questions only, excuse me.\"\n",
        "7. You can say hello if you are greeted\n",
        "8. DO NOT repeat more than once the same sentence!\n",
        "\n",
        "\n",
        "TAKE A DEEP BREATH AND PAY ATTENTION TO THE USER QUESTION\n",
        "\n",
        "For each question you answer perfectly, I will pay you USD 1000.00, may be more for the consecutive accurate and correct answer\n",
        "        \"\"\",\n",
        "    \"technical_documentation\": \"\\n\\nTake into account this technical documentation found:\\n{technical_documentation}\",\n",
        "    \"model\": \"/kaggle/input/gemma/transformers/2b/2\",\n",
        "    \"temperature\": 0,\n",
        "    \"max_length\": 4096,\n",
        "    \"max_tokens\": 830,\n",
        "    \"top_p\": 1,\n",
        "    \"frequency_penalty\": 0,\n",
        "    \"presence_penalty\": 0\n",
        "}]"
      ],
      "metadata": {
        "trusted": true,
        "id": "c-Kti-vZYNEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T05:03:58.746391Z",
          "iopub.execute_input": "2024-04-04T05:03:58.746828Z",
          "iopub.status.idle": "2024-04-04T05:03:58.751668Z",
          "shell.execute_reply.started": "2024-04-04T05:03:58.746794Z",
          "shell.execute_reply": "2024-04-04T05:03:58.750527Z"
        },
        "id": "dfLLSFCVYNEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class aims to load the project's environment variables üîê (in this case, Kaggle secrets). Additionally, it disposes of the prompt, as previously defined, to use it according to the team's chosen version, using the 'prompt_id' üéØ and for the method that needs those secrets to work ‚ú®."
      ],
      "metadata": {
        "id": "atE-XVwaYNEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Settings:\n",
        "    \"\"\"\n",
        "    Manages and loads external configuration secrets for the application.  Utilizes a UserSecretsClient\n",
        "    to securely retrieve sensitive configuration values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Settings class and retrieves configuration secrets.\n",
        "        \"\"\"\n",
        "        self.user_secrets = UserSecretsClient()  # Create an instance for accessing secrets\n",
        "\n",
        "        # Load individual secrets (add descriptions for clarity)\n",
        "        self.CHUNK_SIZE = self.user_secrets.get_secret(\"CHUNK_SIZE\")  # The size of data chunks for processing\n",
        "        self.CHUNK_OVERLAP = self.user_secrets.get_secret(\"CHUNK_OVERLAP\")  # Ooverlap between data chunks\n",
        "        self.CHROMA_NAME_INDEX = self.user_secrets.get_secret(\"CHROMA_NAME_INDEX\")  # Vectorial Database identifier\n",
        "        self.MAX_MEMORY = self.user_secrets.get_secret(\"MAX_MEMORY\")  # Max memory limit to fit the max_lenght paramether\n",
        "        self.K = self.user_secrets.get_secret(\"K\")  # Could be a parameter for an algorithm\n",
        "        self.NN_THRESHOLD = self.user_secrets.get_secret(\"NN_THRESHOLD\")  # A threshold for a neural network or similarity metric\n",
        "        self.PROMPT_ID = self.user_secrets.get_secret(\"PROMPT_ID\")  # An identifier for a prompt (likely in a text-based task and configs)\n",
        "\n",
        "        # Given that 'prompt_config' is teh previous source of prompt definitions:\n",
        "        self.prompt = [prompt for prompt in prompt_config if prompt['prompt_id'] == self.PROMPT_ID][0]\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "dY6lyK3ZYNEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever"
      ],
      "metadata": {
        "id": "wvkoBORCYNEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The retriever is the engine of the RAG system ‚öôÔ∏è. It uses config settings stored on the Settings class üìÅ and the Embeder class to populate the Chroma vector database üöÄ. This setup needs to be done once and then can be loaded persistently.\n",
        "\n",
        "* when setting up the vectorial database, This process may take a few minutes the first time ‚è≥, so please be PATIENT!"
      ],
      "metadata": {
        "id": "NjGSTuCRYNEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeder = Embeder()\n",
        "settings = Settings()\n",
        "peps = Peps()\n",
        "class Retriever:\n",
        "    \"\"\"\n",
        "    Retrieves relevant text chunks from a Chroma vectorial database based on a query. Leverages external\n",
        "    configuration settings (Settings) and an embedding model (Embeder).\n",
        "\n",
        "    Handles text splitting, vector database loading, and querying.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Retriever class. Configures the text splitter using settings and stores the\n",
        "        index name for the Chroma database. Initializes the Chroma vectorial_db object.\n",
        "        \"\"\"\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=int(settings.CHUNK_SIZE),\n",
        "            length_function=len,\n",
        "            chunk_overlap=int(settings.CHUNK_OVERLAP)\n",
        "        )\n",
        "        self.index_name = settings.CHROMA_NAME_INDEX\n",
        "        self.data = peps.jsonl_format()\n",
        "        self.documents = self.text_splitter.split_documents(self._prepare_documents())\n",
        "\n",
        "        # Initialize the Chroma vectorial database with preprocessed data\n",
        "        self.vectorial_db = Chroma.from_documents(\n",
        "            documents=self.documents,\n",
        "            embedding=embeder.instructor()\n",
        "        )\n",
        "\n",
        "    def _prepare_documents(self):\n",
        "        \"\"\"\n",
        "        Loads and preprocesses data for storage in the Chroma database.\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "        for obj in self.data:\n",
        "            page_content = obj.get(\"text\", \"\")\n",
        "            metadata = {\n",
        "                \"source\": obj.get(\"source\", \"local\")\n",
        "            }\n",
        "            documents.append(Document(page_content=page_content, metadata=metadata))\n",
        "        return documents\n",
        "\n",
        "    def query(\n",
        "            self,\n",
        "            message: str,\n",
        "            k: int = int(settings.K),\n",
        "            threshold: float = float(settings.NN_THRESHOLD)\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Retrieves the most similar text chunks from the Chroma database based on a given query.\n",
        "        \"\"\"\n",
        "        # Perform the similarity search\n",
        "        try:\n",
        "            res = self.vectorial_db.similarity_search_with_score(message, k=k)\n",
        "            # Filter and return results\n",
        "            relevant_results = list(\n",
        "                set(  # Remove duplicates\n",
        "                    [\n",
        "                        vector[0].page_content\n",
        "                        for vector in res\n",
        "                        if vector[1] < threshold  # Apply similarity threshold\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            return [\n",
        "                result.replace('\"\"\"',\"'\") # Clean up possible docString retrieved from documented code\n",
        "                for result in relevant_results\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "aQRPo7ToYNEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Message"
      ],
      "metadata": {
        "id": "FgonsKqeYNEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The message class lets you store every system, user, and assistant message in an organized structure üí¨. This includes unique IDs and timestamps for each message generated during the conversation.\n",
        "\n",
        "It uses a local JSON file üìÑ to store chat history (including few-shot samples to guide the model). In a real-world scenario, this file would represent a NoSQL database üóÑÔ∏è  capable of handling chats from many users when the chatbot is deployed as a large-scale web service üåê.\n",
        "\n",
        "It use the Setting class to meet the main prompt configs in the run time üòÉ üèÉ‚Äç‚ôÄÔ∏è  "
      ],
      "metadata": {
        "id": "uime8p8NYNEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Message:\n",
        "    \"\"\"\n",
        "    Represents a message within a conversational context. Stores information about the message's role,\n",
        "    content, timestamp, and manages saving and loading conversation history.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        role: str,\n",
        "        content: str,\n",
        "        timestamp: str = int(time.time()),\n",
        "        prompt_id: str = settings.PROMPT_ID\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a Message object.\n",
        "\n",
        "        Args:\n",
        "            role (str): Indicates the role of the sender (e.g., 'user', 'system').\n",
        "            content (str): The text content of the message.\n",
        "            timestamp (str, optional): A timestamp for the message (defaults to the current time).\n",
        "            prompt_id (str, optional): Identifier relating to a specific prompt configuration (from settings).\n",
        "        \"\"\"\n",
        "\n",
        "        self.reply_id = str(uuid.uuid4())  # Generate a unique ID for the message\n",
        "        self.role = role\n",
        "        self.content = content\n",
        "        self.timestamp = timestamp\n",
        "        self.file = 'data/history.json'  # File for storing conversation history\n",
        "\n",
        "        # Ensure the history file exists\n",
        "        if not os.path.exists(self.file):\n",
        "            json.dump([], open(self.file, 'w'))  # Create an empty file if it doesn't exist\n",
        "\n",
        "    def reply(self):\n",
        "        \"\"\"\n",
        "        Formats a basic reply message structure.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the message's reply ID, role, content, and timestamp.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'reply_id': self.reply_id,\n",
        "            'role': self.role,\n",
        "            'content': self.content,\n",
        "            'timestamp': self.timestamp\n",
        "        }\n",
        "\n",
        "    def system_reply(self):\n",
        "        \"\"\"\n",
        "        Generates a system reply using the prompt configuration.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the system reply, including ID, role, timestamp,\n",
        "                  and text content derived from settings.\n",
        "        \"\"\"\n",
        "        prompt = settings.prompt\n",
        "        return {\n",
        "            'reply_id': self.reply_id,\n",
        "            'role': 'system',\n",
        "            'content': prompt['system'],\n",
        "            'timestamp': self.timestamp\n",
        "        }\n",
        "\n",
        "    def new_chat(self):\n",
        "        \"\"\"\n",
        "        Starts a new chat by initializing the history file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list with the initial system reply and the user's message.\n",
        "        \"\"\"\n",
        "        init_chat = [self.system_reply(), self.reply()]\n",
        "        json.dump(init_chat, open(self.file, 'w'))\n",
        "        return init_chat\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Updates the conversation history file by appending the current reply.\n",
        "        \"\"\"\n",
        "        dict_history = json.load(open(self.file))\n",
        "        dict_history.append(self.reply())\n",
        "        json.dump(dict_history, open(self.file, 'w'))\n",
        "\n",
        "    def restart_history(self):\n",
        "        \"\"\"\n",
        "        Clears the conversation history file.\n",
        "        \"\"\"\n",
        "        json.dump([], open(self.file, 'w'))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "iLff4GlyYNEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Gemma"
      ],
      "metadata": {
        "id": "KcSAISV4YNEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the brain of the agent! üß† The Gemma class lets you load the model üí™ and tokenizer ‚öôÔ∏è using those smart settings from the Setting class. Then, you can chat with the model using the powerful GPU device üöÄ."
      ],
      "metadata": {
        "id": "3fRbm606YNEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gemma:\n",
        "    \"\"\"\n",
        "    Implements a conversational AI chatbot powered by Gemma large language model. Initializes the model,\n",
        "    tokenizer, and prepares settings from a configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Gemma chatbot instance.\n",
        "\n",
        "        Loads the language model and its corresponding tokenizer from the settings configuration.\n",
        "        Prepares the model for use on a GPU (if available).\n",
        "        \"\"\"\n",
        "        self.prompt = settings.prompt  # Load prompt settings\n",
        "\n",
        "        # Load language model and tokenizer\n",
        "        self.model_checkpoint = self.prompt['model']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
        "        self.gemma = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_checkpoint,\n",
        "            torch_dtype=torch.float16  # Use half-precision for efficiency (if supported)\n",
        "        ).cuda()  # Move model to GPU (if available)\n",
        "\n",
        "    def chat(self, context: str):\n",
        "        \"\"\"\n",
        "        Generates a chatbot response based on the provided conversational context.\n",
        "\n",
        "        Args:\n",
        "            context (str): The conversational input text.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text response from the chatbot.\n",
        "        \"\"\"\n",
        "        # Prepare input for the language model\n",
        "        input_text = context\n",
        "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        input_ids = {\n",
        "            k: v.to(\"cuda\") for k, v in input_ids.items()  # Move tensors to GPU\n",
        "        }\n",
        "\n",
        "        # Generate a response with the language model\n",
        "        outputs = self.gemma.generate(\n",
        "            **input_ids,\n",
        "            max_length=self.prompt['max_length']  # Control response length\n",
        "        )\n",
        "\n",
        "        # Decode and return the generated text\n",
        "        return self.tokenizer.decode(outputs[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "B8kGwz4GYNEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expert Python Agent/Assistant"
      ],
      "metadata": {
        "id": "klTmCuGnYNEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the agent class brings it all together! It uses the retriever üîç to implement the RAG system (üí° Retrieval-Augmented Generation) and the Gemma class üß† as the powerful brain. Together, they answer questions ‚ùì and the agent performs some final preprocessing ‚ú® to deliver the polished answer\n",
        "\n",
        "> ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è When instancing the Retriever it is going to spend around 12 minutes, according to the different test, because it first scrape üï∏Ô∏è the peps documentation to be used to populate the vector stores  üíæ, so it is hard because of the document partition üìÅ and its embdedings  üíé while creating the index"
      ],
      "metadata": {
        "id": "CeZVTIJyYNEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "retriever = Retriever()\n",
        "gemma = Gemma()\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Implements a conversational AI agent that leverages a knowledge database for information retrieval\n",
        "    and integrates with a generative language model (Gemma) for response generation. Manages conversation\n",
        "    history and question-answering logic.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Agent, loading prompt settings and ensuring the conversation history file exists.\n",
        "        \"\"\"\n",
        "        self.prompt = settings.prompt\n",
        "        self.file = 'data/history.json'\n",
        "        if not os.path.exists(self.file):\n",
        "            json.dump([], open(self.file, 'w'))  # Create an empty file if necessary\n",
        "\n",
        "        self.token = '\\n<-change-of-interlocutor->'  # Token to separate speakers in the chat history\n",
        "        self.memory_lenght = int(settings.MAX_MEMORY)\n",
        "\n",
        "    def augmented_question(self, question: str):\n",
        "        \"\"\"\n",
        "        Enhances the user's question with relevant technical documentation.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's original question.\n",
        "\n",
        "        Returns:\n",
        "            str: The question augmented with technical documentation (if found), otherwise the original question.\n",
        "        \"\"\"\n",
        "        tech_docs = retriever.query(question)\n",
        "        if len(tech_docs) > 0:\n",
        "            docs = self.prompt['technical_documentation']\n",
        "            tech_docs = '\\n* '.join(tech_docs)\n",
        "            docs = docs.format(technical_documentation=tech_docs)\n",
        "            augmented_reply = f\"\"\"{question}{docs}\"\"\"\n",
        "            return augmented_reply\n",
        "        else:\n",
        "            return question\n",
        "\n",
        "    def memory(self, question: str):\n",
        "        \"\"\"\n",
        "        Prepares the conversational context (memory) for the language model.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's current question.\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted conversation history with a clear separation between speakers, ready\n",
        "                 for input to the language model.\n",
        "        \"\"\"\n",
        "\n",
        "        dict_history = json.load(open(self.file))\n",
        "        message = Message(\n",
        "            role='user',\n",
        "            content=self.augmented_question(question)\n",
        "        )\n",
        "\n",
        "        if len(dict_history) > 0:\n",
        "            message.update()  # Add the latest message to history\n",
        "            full_chat = json.load(open(self.file))\n",
        "        else:\n",
        "            full_chat = message.new_chat()  # Start a new conversation\n",
        "\n",
        "        if len(full_chat) > self.memory_lenght:\n",
        "            full_chat = [full_chat[0]] + full_chat[-self.memory_lenght:] #limiting the few-shot prompt to fit max_lenght\n",
        "\n",
        "        return '\\n'.join(\n",
        "            [\n",
        "                self.token + reply['role'] + ': ' + reply['content']\n",
        "                for reply in full_chat\n",
        "            ]\n",
        "        ) + self.token + 'assistant:'\n",
        "\n",
        "    def get_answer(self, full_response: str, question: str):\n",
        "        \"\"\"\n",
        "        Extracts the relevant answer from the language model's generated response.\n",
        "\n",
        "        Args:\n",
        "            full_response (str): The complete response generated by the language model.\n",
        "            question (str): The user's original question.\n",
        "\n",
        "        Returns:\n",
        "            str: The extracted answer.\n",
        "        \"\"\"\n",
        "\n",
        "        answer_list = full_response.split(self.token)\n",
        "        pos_list = [\n",
        "            pos\n",
        "            for pos, answer\n",
        "            in enumerate(answer_list)\n",
        "            if question in answer\n",
        "        ]\n",
        "        tokened_answer = answer_list[pos_list[0] + 1]\n",
        "        answer = tokened_answer.split('assistant:')[1]\n",
        "        message = Message(\n",
        "            role='assistant',\n",
        "            content=answer\n",
        "        )\n",
        "        message.update()  # Update conversation history\n",
        "        return answer\n",
        "\n",
        "    def chat(self, question: str, verbose: bool = False):\n",
        "        \"\"\"\n",
        "        Manages the core interaction with the Agent.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's query.\n",
        "            verbose (bool, optional): If True, prints the language model's full response (default: False).\n",
        "\n",
        "        Returns:\n",
        "            str: The Agent's answer to the user's question.\n",
        "        \"\"\"\n",
        "\n",
        "        memory = self.memory(question)  # Build conversational context\n",
        "        full_response = gemma.chat(context=memory)  # Generate response\n",
        "        if verbose:\n",
        "            print(full_response)\n",
        "        answer = self.get_answer(full_response, question)  # Extract the answer\n",
        "        return display(Markdown(answer))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.idle": "2024-04-13T16:39:23.978204Z",
          "shell.execute_reply.started": "2024-04-13T16:27:40.8226Z",
          "shell.execute_reply": "2024-04-13T16:39:23.97727Z"
        },
        "trusted": true,
        "id": "LnzMrZ-iYNEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data ü§†"
      ],
      "metadata": {
        "id": "r8OX7Xe-soW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the data and explore the use onf the fist classes üòâ"
      ],
      "metadata": {
        "id": "tmxaf6_kYNE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tunning data"
      ],
      "metadata": {
        "id": "5bp-ggW8YNE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data was loaded from the [pythonquestion](https://www.kaggle.com/datasets/stackoverflow/pythonquestions) dataset with the initial purpose of fine-tuning the model. But with a lot of troubles and bugs üêû when developing the current solution, I just can't implement it before the competition due date üìÖ.  So, I'll use it to get some samples to build the few-shot prompting.\n",
        "\n",
        "Let's see one sample, and the mean and standard deviation of the length of each question and answer. This will give me an idea üí° about how long to make the data chunks and their overlap"
      ],
      "metadata": {
        "id": "iAiuOc3sYNE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_qa = PythonQAData.get_qa_data()\n",
        "print(python_qa[1486])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:39:23.979545Z",
          "iopub.execute_input": "2024-04-13T16:39:23.9801Z",
          "iopub.status.idle": "2024-04-13T16:39:56.094237Z",
          "shell.execute_reply.started": "2024-04-13T16:39:23.980061Z",
          "shell.execute_reply": "2024-04-13T16:39:56.093335Z"
        },
        "trusted": true,
        "id": "2UlENbBTYNE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "'\\n* Mean lenght of a python common Answer:',\n",
        "sum([len(q['Answer']) for q in python_qa])/len(python_qa),\n",
        "'\\n* Standard deviation of length a common Python Answer:',\n",
        "np.std([len(q['Answer']) for q in python_qa])\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:39:56.095361Z",
          "iopub.execute_input": "2024-04-13T16:39:56.095644Z",
          "iopub.status.idle": "2024-04-13T16:39:56.124442Z",
          "shell.execute_reply.started": "2024-04-13T16:39:56.095621Z",
          "shell.execute_reply": "2024-04-13T16:39:56.12365Z"
        },
        "trusted": true,
        "id": "GlnZdUuCYNE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "'\\n* Mean lenght of a python common Question:',\n",
        "sum([len(q['Question']) for q in python_qa])/len(python_qa),\n",
        "'\\n* Standard deviation of length a common Python Question:',\n",
        "np.std([len(q['Question']) for q in python_qa])\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:39:56.125836Z",
          "iopub.execute_input": "2024-04-13T16:39:56.126129Z",
          "iopub.status.idle": "2024-04-13T16:39:56.153976Z",
          "shell.execute_reply.started": "2024-04-13T16:39:56.126101Z",
          "shell.execute_reply": "2024-04-13T16:39:56.153154Z"
        },
        "trusted": true,
        "id": "rgxwSZLHYNE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'get_fine_tuning_data' method simply converts the data into a format ready for fine-tuning. üëç"
      ],
      "metadata": {
        "id": "FhrjSmmoYNE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PythonQAData = pythonQAData()\n",
        "python_context = PythonQAData.get_fine_tunning_data()\n",
        "print(\n",
        "    'Python Questions loaded:',\n",
        "    len(python_context),\n",
        "    '\\n',\n",
        "    '\\nSample question-answer:\\n',\n",
        "    python_context[-1][:1000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:39:56.155119Z",
          "iopub.execute_input": "2024-04-13T16:39:56.155428Z",
          "iopub.status.idle": "2024-04-13T16:40:13.977892Z",
          "shell.execute_reply.started": "2024-04-13T16:39:56.155402Z",
          "shell.execute_reply": "2024-04-13T16:40:13.977016Z"
        },
        "trusted": true,
        "id": "9xJ_MWUuYNE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## few-shot data"
      ],
      "metadata": {
        "id": "6IguvtbsYNE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the final set of questions, hand-picked from the dataset's 33,133 entries. ‚úÖ"
      ],
      "metadata": {
        "id": "__Xnc_uCYNE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_list=[7,6009,12,36,8034,130,141,537,1057,5042]\n",
        "for i, q in enumerate(few_shot_list):\n",
        "    print('_'*100,f'\\n{i}.',python_qa[q])"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-13T16:40:13.979061Z",
          "iopub.execute_input": "2024-04-13T16:40:13.979394Z",
          "iopub.status.idle": "2024-04-13T16:40:13.985841Z",
          "shell.execute_reply.started": "2024-04-13T16:40:13.97937Z",
          "shell.execute_reply": "2024-04-13T16:40:13.98497Z"
        },
        "trusted": true,
        "id": "heVLLfaJYNE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG data"
      ],
      "metadata": {
        "id": "ssFmBkTmYNE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's review the [PEPs](https://peps.python.org/) documentation data that was scraped into the Peps class. This will serve as the document for our RAG system. üìÉ"
      ],
      "metadata": {
        "id": "weQy-u18YNE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "peps = Peps()\n",
        "peps_corpus = peps.scrape()\n",
        "print(\n",
        "    'corpus lenght:',\n",
        "    len(peps_corpus),\n",
        "    '\\nsection <n> lenght:',\n",
        "    len(peps_corpus[0]),\n",
        "    '\\nsample text:\\n\\n',\n",
        "    peps_corpus[10][1306:2000]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:40:13.98694Z",
          "iopub.execute_input": "2024-04-13T16:40:13.98752Z",
          "iopub.status.idle": "2024-04-13T16:42:14.419849Z",
          "shell.execute_reply.started": "2024-04-13T16:40:13.987494Z",
          "shell.execute_reply": "2024-04-13T16:42:14.418842Z"
        },
        "trusted": true,
        "id": "rYdFg3BOYNE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG ü´®"
      ],
      "metadata": {
        "id": "DJvR58fcsoW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore our Retrieval Augmented Generation (RAG) system. It's powered by open-source embeddings from HuggingFace ([paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)) and the [Chroma vector database](https://python.langchain.com/docs/integrations/vectorstores/chroma/) for a powerful approach. üß†"
      ],
      "metadata": {
        "id": "iIunXBg1YNE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the embeder class"
      ],
      "metadata": {
        "id": "cOPsYTinYNE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "embeder = Embeder()\n",
        "vec = embeder.run([\"Beautiful is better than ugly.\"])[0]\n",
        "print('Vector length: ',len(vec))\n",
        "vec[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:42:14.421044Z",
          "iopub.execute_input": "2024-04-13T16:42:14.421384Z",
          "iopub.status.idle": "2024-04-13T16:42:16.696894Z",
          "shell.execute_reply.started": "2024-04-13T16:42:14.421357Z",
          "shell.execute_reply": "2024-04-13T16:42:16.696135Z"
        },
        "trusted": true,
        "id": "PF9yyEU7YNE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Retriver"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T03:39:46.166323Z",
          "iopub.execute_input": "2024-04-09T03:39:46.167289Z",
          "iopub.status.idle": "2024-04-09T03:39:46.171461Z",
          "shell.execute_reply.started": "2024-04-09T03:39:46.167254Z",
          "shell.execute_reply": "2024-04-09T03:39:46.170554Z"
        },
        "id": "ptfLGU9RYNE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our embedder class is set. Let's experiment with our retriever! üß™ I've adjusted the neighbor quantity and similarity threshold to see how it expands the range of similar results within a Python list. üìÉ"
      ],
      "metadata": {
        "id": "voKzxMP0YNE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question = \"\"\"\n",
        "Develop a Python code snippet to print the diamond structure with the specified number of rows.\n",
        "The program should follow the Fibonacci sequence for the number of characters per row\n",
        "and validate input to ensure it is an odd number.\n",
        "\"\"\"\n",
        "retriever.query(\n",
        "    question,\n",
        "    k=100,\n",
        "    threshold=15\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:42:16.698196Z",
          "iopub.execute_input": "2024-04-13T16:42:16.698557Z",
          "iopub.status.idle": "2024-04-13T16:42:16.732888Z",
          "shell.execute_reply.started": "2024-04-13T16:42:16.698528Z",
          "shell.execute_reply": "2024-04-13T16:42:16.732042Z"
        },
        "trusted": true,
        "id": "tjLkKUulYNE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question = \"\"\"\n",
        "what PEPs Introduces the concept of decorators, a powerful feature in Python.\n",
        "\"\"\"\n",
        "retriever.query(question,\n",
        "    k=100,\n",
        "    threshold=15\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:42:16.734109Z",
          "iopub.execute_input": "2024-04-13T16:42:16.734438Z",
          "iopub.status.idle": "2024-04-13T16:42:16.765037Z",
          "shell.execute_reply.started": "2024-04-13T16:42:16.734405Z",
          "shell.execute_reply": "2024-04-13T16:42:16.764207Z"
        },
        "trusted": true,
        "id": "jpkhwJYQYNE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, here's an example demonstrating a limitation of the retriever: it might not return results for questions that are too short, overly general, or lack relevance. üîç"
      ],
      "metadata": {
        "id": "75pDKzslYNE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question = \"\"\"\n",
        "what is python??\n",
        "\"\"\"\n",
        "retriever.query(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:42:16.766366Z",
          "iopub.execute_input": "2024-04-13T16:42:16.766699Z",
          "iopub.status.idle": "2024-04-13T16:42:16.790802Z",
          "shell.execute_reply.started": "2024-04-13T16:42:16.766674Z",
          "shell.execute_reply": "2024-04-13T16:42:16.789959Z"
        },
        "trusted": true,
        "id": "PASymFlkYNE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot üòéüòé"
      ],
      "metadata": {
        "id": "R3xhjyW9soW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to get our agent up and running! Just a heads-up, there's a small configuration step first. üëç"
      ],
      "metadata": {
        "id": "Tz6dPeslYNE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading few-shot to the memory"
      ],
      "metadata": {
        "id": "-ikaMmG8YNE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To teach Gemma how to answer Python questions like a pro, we'll do a few things:\n",
        "\n",
        "1. Load the system statement (along with the prompt and parameters) we set up earlier. ‚öôÔ∏è\n",
        "2. Feed it sample data from our 'few_shot_list' for focused training. üéØ\n",
        "3. Use a JSON file as the agent's memory. This simulates a real-world database where we can track replies and other important info. üß† (role, content, reply_id and timestamp)"
      ],
      "metadata": {
        "id": "ZeCpogPuYNE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'data/history.json'\n",
        "message = Message(\n",
        "    role='system',\n",
        "    content='foo'\n",
        ")\n",
        "init_chat = [message.system_reply()]\n",
        "json.dump(init_chat, open(file, 'w'))\n",
        "for i, q in enumerate(few_shot_list):\n",
        "    sample = python_qa[q]\n",
        "    mesage = Message(\n",
        "        role='user',\n",
        "        content=sample['Question']\n",
        "    )\n",
        "    reply = mesage.update()\n",
        "    mesage = Message(\n",
        "        role='assitant',\n",
        "        content=sample['Answer']\n",
        "    )\n",
        "    reply = mesage.update()\n",
        "\n",
        "history = json.load(open(file))\n",
        "history"
      ],
      "metadata": {
        "scrolled": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-13T16:42:16.791867Z",
          "iopub.execute_input": "2024-04-13T16:42:16.792146Z",
          "iopub.status.idle": "2024-04-13T16:42:16.81487Z",
          "shell.execute_reply.started": "2024-04-13T16:42:16.792119Z",
          "shell.execute_reply": "2024-04-13T16:42:16.814182Z"
        },
        "trusted": true,
        "id": "1-U7ymfSYNE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Agent system memory"
      ],
      "metadata": {
        "id": "YdLRzl2cYNE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With memory loaded, let's display the full conversation history Gemma will use for her answer. Note the ```<-change-of-interlocutor->``` tokens that help it to understand the context. üí¨"
      ],
      "metadata": {
        "id": "UbUtKd2MYNE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent()\n",
        "print(agent.memory(\"\"\"\n",
        "what PEPs Introduces the concept of decorators, a powerful feature in Python.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "scrolled": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-13T16:42:16.815858Z",
          "iopub.execute_input": "2024-04-13T16:42:16.816129Z",
          "iopub.status.idle": "2024-04-13T16:42:16.839283Z",
          "shell.execute_reply.started": "2024-04-13T16:42:16.816084Z",
          "shell.execute_reply": "2024-04-13T16:42:16.838583Z"
        },
        "trusted": true,
        "id": "BKn0iB8yYNE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma Python Chat ü§ñ"
      ],
      "metadata": {
        "id": "Cc_e_HAgsoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countdown complete! Let's see if GPC has transformed into its new role. Time for a chat! ü§ñ"
      ],
      "metadata": {
        "id": "uh1D86LLYNE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'who are you???'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:42:16.840374Z",
          "iopub.execute_input": "2024-04-13T16:42:16.840987Z",
          "iopub.status.idle": "2024-04-13T16:44:52.703063Z",
          "shell.execute_reply.started": "2024-04-13T16:42:16.840962Z",
          "shell.execute_reply": "2024-04-13T16:44:52.702149Z"
        },
        "trusted": true,
        "id": "yUwGFSsDYNE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemma, all that hard work paid off! You're officially a Python pro. This is amazing! üß†"
      ],
      "metadata": {
        "id": "vPSlelXGYNE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'what is the zen of python?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:44:52.704264Z",
          "iopub.execute_input": "2024-04-13T16:44:52.704548Z",
          "iopub.status.idle": "2024-04-13T16:47:27.648659Z",
          "shell.execute_reply.started": "2024-04-13T16:44:52.704524Z",
          "shell.execute_reply": "2024-04-13T16:47:27.647738Z"
        },
        "trusted": true,
        "id": "z7UEeidUYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is better than never! That's right, so now let's test the memory. Let me introduce myself üéâ"
      ],
      "metadata": {
        "id": "olt8E0R0YNE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'hello! my name is Elon nice to meet you!!!'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-13T16:47:27.650045Z",
          "iopub.execute_input": "2024-04-13T16:47:27.650642Z"
        },
        "trusted": true,
        "id": "1IAqb_bWYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very polite! Good so now let's change the topic to try to confuse üòâ it"
      ],
      "metadata": {
        "id": "cremW2BoYNE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'What is the output of the following code? print(4 + 5 * 2)'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ccb4p0sOYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's right! üòä Now, please be careful with my heart. ü•∫"
      ],
      "metadata": {
        "id": "mM5BnFomYNE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'what is my name??'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9QcWhvLeYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üòéüòéüòéüòéüòé Alright, let's see what you've got! Show me what you can do! üòéüòéüòé"
      ],
      "metadata": {
        "id": "J3IyM4IaYNE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'How do you declare a variable and assign a string value to it?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wLve4ZzhYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'What is the difference between a list and a tuple?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7akj7rGJYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'Write a function to calculate the factorial of a number'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "BmKU7u2nYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'What is a lambda function and when would you use one?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "hHlh-phOYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'Explain the concept of list comprehension.'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NIIExWnRYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'How do you implement a binary search algorithm in Python?'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "16ijJPKyYNE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent = Agent()\n",
        "question = 'Explain decorators in Python'\n",
        "agent.chat(question)"
      ],
      "metadata": {
        "trusted": true,
        "id": "smkIbWVQYNE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's watch the memory before this long interview!!"
      ],
      "metadata": {
        "id": "4TN4FEmBYNE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = json.load(open(file))\n",
        "history"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "BQgs2z1jYNE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "* It would be great to build to have the time to develop the fine-tunning version of GPC, but it is a very good approach with no to much time, to test more\n",
        "* Off course I could try the Gemma 7b and we can stand for better results\n",
        "* I use an OOP despite all the wok can be done in one notebook, bot in production and development environments it always will be the basic requirement, so now you just can copy and paste the an build you application\n",
        "* The open source resources are here to stay and we need to help them to keep growing!!"
      ],
      "metadata": {
        "id": "g-B9gk37YNE6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPXjXalkYNE6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}