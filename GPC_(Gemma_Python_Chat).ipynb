{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 11384,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 6216
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "GPC (Gemma Python Chat)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/GPC_(Gemma_Python_Chat).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240321%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240321T051744Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dabeca6c2a2189a2c51a47762d071c394984da64a3445c44e6451448e74aaf04f235724096086981be98a40e7653346d3ac7c8879e78d6eb1e1d771e8eaeae0b783f1f7cc41ed678b38ba63556a8c6e6f4efc775198cd8cadafdf4efa026d949e594a56488d5cd461a4d711dcd231cfe368b01741c7caf653359ee4c8d9b53f0f656f29f77743004dfc4eebfdd55f1f6ad73192dbe011a36ea3aeca44abffff3424a07176f25dbccf5f351887515a749ecd6d062edfada44a2f4024b19a9f28f306795bb53b44952734594a99190bedbe03874efd7d8dafa79e805dc56c8c8b0838c97038b22b94005b60b3811aa61b9e414dabd0beb243a1f181d4ca763946bb,gemma/transformers/2b/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F6216%2F11384%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240321%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240321T051744Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D19c85a6b97d0dfd4c0b3505ea6e3e17323d9871b56cf2cc3be3afd90eb38c0fcf700a373b427c303a18f3e78d34fd2b26a54ca27c7b1c14f840952c7e3ad6126ca786f5785907ebffffb949cd45b8fde100eb7b606eb5f0b7fef16149627b5e77fd84c30b4b0843df260f7dc6ecdf6714255f0e362d80b3cff76dcd91c5ff374d3cc13f9909d70ed730b017df93bdd0efed69651884e503413ed00dd88314559807a300dbcc80f947eee2c931869d16a19a0ccc53527bd7c195b905ad5b3e769c1ebc06a114f7c64a34e942bda325843338acd96dac7a853382dc808d0001aa4b2188847a29d7a9395abe32256c6377535c4adb0edc3c589ecfc8ef9289a5b41'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "bHMJR0FhcagF"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP ü§ñ Gemma Python Chatbot"
      ],
      "metadata": {
        "id": "fOBLblyucagI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/8xZNc32/Gemma.png)"
      ],
      "metadata": {
        "id": "4xWVoshKcagJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gemma Python Chatbot üöÄüöÄ help you to answer common questions about the üêç Python programming language, powereg by [Gemma 7B IT](https://blog.google/technology/developers/gemma-open-models/) updated with the latest"
      ],
      "metadata": {
        "id": "LGcmuw4ucagJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "axqvPRrUcagJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#base\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "from typing import *\n",
        "# model\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "# data\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T04:02:13.523874Z",
          "iopub.execute_input": "2024-03-21T04:02:13.524264Z",
          "iopub.status.idle": "2024-03-21T04:02:25.800884Z",
          "shell.execute_reply.started": "2024-03-21T04:02:13.524233Z",
          "shell.execute_reply": "2024-03-21T04:02:25.800086Z"
        },
        "trusted": true,
        "id": "ieJlzFYlcagJ",
        "outputId": "a1660dbe-b69a-47de-83d9-e9fb5d3c7ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-21 04:02:17.421086: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-21 04:02:17.421186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-21 04:02:17.550034: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"/kaggle/input/gemma/transformers/2b/2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, torch_dtype=torch.float16).cuda()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T04:02:44.316355Z",
          "iopub.execute_input": "2024-03-21T04:02:44.317033Z",
          "iopub.status.idle": "2024-03-21T04:03:21.584459Z",
          "shell.execute_reply.started": "2024-03-21T04:02:44.317002Z",
          "shell.execute_reply": "2024-03-21T04:03:21.58344Z"
        },
        "trusted": true,
        "id": "aaCSLxmLcagK",
        "outputId": "3870c053-adde-4b17-af4a-2968eb2824f2",
        "colab": {
          "referenced_widgets": [
            "9961b3f5c1e4471e9e27d9d46c9fda1f"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9961b3f5c1e4471e9e27d9d46c9fda1f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "cq-9VWXNcagK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"KonradSzafer/stackoverflow_python_preprocessed\")\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T05:14:48.772839Z",
          "iopub.execute_input": "2024-03-21T05:14:48.773573Z",
          "iopub.status.idle": "2024-03-21T05:14:48.959289Z",
          "shell.execute_reply.started": "2024-03-21T05:14:48.773541Z",
          "shell.execute_reply": "2024-03-21T05:14:48.958203Z"
        },
        "trusted": true,
        "id": "rF3mW0jTcagL",
        "outputId": "6ed80e1b-cd88-4dd7-eea2-d95ef0eef12d"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKonradSzafer/stackoverflow_python_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m dataset\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'load_dataset' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "WN622_9ycagL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "HBZOH_mfcagL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Use the model\n",
        "input_text = \"What is the data cut-off date for the Gemma Google models training dataset?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = {k: v.to(\"cuda\") for k, v in input_ids.items()}  if torch.cuda.is_available() else {k: v.to(\"cpu\")}\n",
        "outputs = model.generate(**input_ids, max_length=200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T04:13:29.90822Z",
          "iopub.execute_input": "2024-03-21T04:13:29.90861Z",
          "iopub.status.idle": "2024-03-21T04:13:35.388771Z",
          "shell.execute_reply.started": "2024-03-21T04:13:29.908582Z",
          "shell.execute_reply": "2024-03-21T04:13:35.387839Z"
        },
        "trusted": true,
        "id": "_szoFieacagL",
        "outputId": "489ad733-40cd-45bf-eda4-118b585d8c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "<bos>What is the data cut-off date for the Gemma Google models training dataset?\n\nThe data cut-off date for the Gemma Google models training dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models validation dataset?\n\nThe data cut-off date for the Gemma Google models validation dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models test dataset?\n\nThe data cut-off date for the Gemma Google models test dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models development dataset?\n\nThe data cut-off date for the Gemma Google models development dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models training dataset\nCPU times: user 5.48 s, sys: 0 ns, total: 5.48 s\nWall time: 5.47 s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Use the model\n",
        "input_text = \"if 5 t-shirts get dry in 10 hour ho long spend 30 t-shirts to get dry?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = {k: v.to(\"cuda\") for k, v in input_ids.items()}  if torch.cuda.is_available() else {k: v.to(\"cpu\")}\n",
        "outputs = model.generate(**input_ids, max_length=500)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "zWfAcB0WcagL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.0.1"
      ],
      "metadata": {
        "id": "mS4DjK_vcagL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tunning"
      ],
      "metadata": {
        "id": "ccGH8BTJcagL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.1.0"
      ],
      "metadata": {
        "id": "n9RsdUkpcagL"
      }
    }
  ]
}