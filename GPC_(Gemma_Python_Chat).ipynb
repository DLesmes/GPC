{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "GPC (Gemma Python Chat)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/GPC_(Gemma_Python_Chat).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240316%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240316T171316Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D81eebbb2e32417d8f45a4b8914f3ad164f40d03356accd8043e10a67ed1d01f922837642cacbf2affa981547ebfcc1955f168fd4ea790f065d707dbd464678d854769c0c5074b6b6d01a96733f7cb1d63b9627d99c24b99397df620d551c7ba5dd643b95bd5ac70bf64b7fde645c8f8bc09b6ae86505a422b760ac535b6066f4fc9deb254a90da18e3173892b5fff7a25ee56876d2921dfbb7fa044e9b2a831484fd19f0ad4697e3cbcb0840d164c12584b452d0ea007532529959a517e1a169623ecdfc5e55b39c420df3f3e5e6337d360eaf1fed82288dd5196346b242f6caaf0549a90233c7274ee2feaddd191329c2f8140713ef845ca90797484fba285c'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_S1-F7-aQPo8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-16T16:36:32.478399Z",
          "iopub.execute_input": "2024-03-16T16:36:32.478739Z",
          "iopub.status.idle": "2024-03-16T16:36:32.487467Z",
          "shell.execute_reply.started": "2024-03-16T16:36:32.478695Z",
          "shell.execute_reply": "2024-03-16T16:36:32.486551Z"
        },
        "trusted": true,
        "id": "1vJ9C2XoQPo_",
        "outputId": "4fa4871b-dd3d-48d3-a776-d2bcf4d0c1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/data-assistants-with-gemma/submission_categories.txt\n/kaggle/input/data-assistants-with-gemma/submission_instructions.txt\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP 🤖 Gemma Python Chatbot"
      ],
      "metadata": {
        "id": "Y5shAih4QPo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/8xZNc32/Gemma.png)"
      ],
      "metadata": {
        "id": "H62rHo2FQPpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gemma Python Chatbot 🚀🚀 help you to answer common questions about the 🐍 Python programming language, powereg by [Gemma 7B IT](https://blog.google/technology/developers/gemma-open-models/) updated with the latest"
      ],
      "metadata": {
        "id": "eNx3FnIIQPpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain-google-vertexai"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T16:36:32.489067Z",
          "iopub.execute_input": "2024-03-16T16:36:32.489417Z",
          "iopub.status.idle": "2024-03-16T16:37:18.071535Z",
          "shell.execute_reply.started": "2024-03-16T16:36:32.489389Z",
          "shell.execute_reply": "2024-03-16T16:37:18.070498Z"
        },
        "trusted": true,
        "id": "SBm3nDVIQPpB",
        "outputId": "458fe4bb-29a4-41ed-af7b-82ff381a7460"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting langchain\n  Downloading langchain-0.1.12-py3-none-any.whl.metadata (13 kB)\nCollecting langchain-google-vertexai\n  Downloading langchain_google_vertexai-0.1.1-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.28 (from langchain)\n  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\nCollecting langchain-core<0.2.0,>=0.1.31 (from langchain)\n  Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.27-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nCollecting google-cloud-aiplatform<2.0.0,>=1.44.0 (from langchain-google-vertexai)\n  Downloading google_cloud_aiplatform-1.44.0-py2.py3-none-any.whl.metadata (27 kB)\nCollecting google-cloud-storage<3.0.0,>=2.14.0 (from langchain-google-vertexai)\n  Downloading google_cloud_storage-2.15.0-py2.py3-none-any.whl.metadata (6.1 kB)\nCollecting types-protobuf<5.0.0.0,>=4.24.0.4 (from langchain-google-vertexai)\n  Downloading types_protobuf-4.24.0.20240311-py3-none-any.whl.metadata (1.9 kB)\nCollecting types-requests<3.0.0,>=2.31.0 (from langchain-google-vertexai)\n  Downloading types_requests-2.31.0.20240311-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (2.11.1)\nRequirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (2.26.1)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (1.23.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (3.20.3)\nRequirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (21.3)\nRequirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (2.34.4)\nRequirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (1.11.0)\nRequirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (1.8.5.post1)\nCollecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai)\n  Downloading google_api_core-2.17.1-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (2.4.1)\nRequirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (2.7.0)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (1.5.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (4.2.0)\nCollecting packaging>=14.3 (from google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nCollecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (1.62.0)\nINFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (1.51.1)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (1.48.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (4.9)\nINFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\nCollecting google-cloud-bigquery<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai)\n  Downloading google_cloud_bigquery-3.19.0-py2.py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (2.8.2)\nRequirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (0.12.7)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (0.5.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.44.0->langchain-google-vertexai) (1.16.0)\nDownloading langchain-0.1.12-py3-none-any.whl (809 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_google_vertexai-0.1.1-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_cloud_aiplatform-1.44.0-py2.py3-none-any.whl (4.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading google_cloud_storage-2.15.0-py2.py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.27-py3-none-any.whl (68 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading types_protobuf-4.24.0.20240311-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading types_requests-2.31.0.20240311-py3-none-any.whl (14 kB)\nDownloading google_api_core-2.17.1-py3-none-any.whl (137 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_cloud_bigquery-3.19.0-py2.py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: urllib3, types-protobuf, packaging, orjson, types-requests, langsmith, google-api-core, langchain-core, langchain-text-splitters, langchain-community, google-cloud-storage, google-cloud-bigquery, langchain, google-cloud-aiplatform, langchain-google-vertexai\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: google-api-core\n    Found existing installation: google-api-core 2.11.1\n    Uninstalling google-api-core-2.11.1:\n      Successfully uninstalled google-api-core-2.11.1\n  Attempting uninstall: google-cloud-storage\n    Found existing installation: google-cloud-storage 1.44.0\n    Uninstalling google-cloud-storage-1.44.0:\n      Successfully uninstalled google-cloud-storage-1.44.0\n  Attempting uninstall: google-cloud-bigquery\n    Found existing installation: google-cloud-bigquery 2.34.4\n    Uninstalling google-cloud-bigquery-2.34.4:\n      Successfully uninstalled google-cloud-bigquery-2.34.4\n  Attempting uninstall: google-cloud-aiplatform\n    Found existing installation: google-cloud-aiplatform 0.6.0a1\n    Uninstalling google-cloud-aiplatform-0.6.0a1:\n      Successfully uninstalled google-cloud-aiplatform-0.6.0a1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\nbotocore 1.34.34 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2024.2.0 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.17.1 which is incompatible.\ngoogle-cloud-pubsub 2.19.0 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed google-api-core-2.17.1 google-cloud-aiplatform-1.44.0 google-cloud-bigquery-3.19.0 google-cloud-storage-2.15.0 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-google-vertexai-0.1.1 langchain-text-splitters-0.0.1 langsmith-0.1.27 orjson-3.9.15 packaging-23.2 types-protobuf-4.24.0.20240311 types-requests-2.31.0.20240311 urllib3-2.1.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import GemmaChatLocalHF, GemmaLocalHF\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "access_token_read = UserSecretsClient().get_secret(\"HUGGINGFACE_TOKEN\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T16:37:18.072929Z",
          "iopub.execute_input": "2024-03-16T16:37:18.073205Z",
          "iopub.status.idle": "2024-03-16T16:37:30.536753Z",
          "shell.execute_reply.started": "2024-03-16T16:37:18.073179Z",
          "shell.execute_reply": "2024-03-16T16:37:30.53575Z"
        },
        "trusted": true,
        "id": "oLWHyAZ7QPpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_access_token = access_token_read\n",
        "model_name: str = \"google/gemma-2b\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T16:37:55.565367Z",
          "iopub.execute_input": "2024-03-16T16:37:55.565733Z",
          "iopub.status.idle": "2024-03-16T16:37:55.570256Z",
          "shell.execute_reply.started": "2024-03-16T16:37:55.565703Z",
          "shell.execute_reply": "2024-03-16T16:37:55.569292Z"
        },
        "trusted": true,
        "id": "7OTrx-MuQPpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GemmaLocalHF(model_name=\"google/gemma-2b\", hf_access_token=hf_access_token)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T16:38:02.605037Z",
          "iopub.execute_input": "2024-03-16T16:38:02.605684Z",
          "iopub.status.idle": "2024-03-16T16:38:27.001399Z",
          "shell.execute_reply.started": "2024-03-16T16:38:02.60565Z",
          "shell.execute_reply": "2024-03-16T16:38:27.00029Z"
        },
        "trusted": true,
        "id": "QR-udkOJQPpB",
        "outputId": "e18d837a-5d78-4d1a-c9ce-ebedcea6ad71",
        "colab": {
          "referenced_widgets": [
            "5abb2a1adf0940ba8f0afcb852bf0e24",
            "de5e7990b03843ba9a841053b00e5a90",
            "5c951f5dc4164bc2902c39ddd48de433",
            "5b201048b3c145c992d9c98c9bdde6bf",
            "f0e619dec70c45d3ac4b52f411751fb6",
            "41fac3595015484a9b170da9adf71e01",
            "8f2a846478aa45158aa341cdedac7501",
            "4430555ecae64994ae5bf26e0b2861a1",
            "0803ba2aa91b42cab205412d94a4922c",
            "f75d2efc1cd44986b101714e1f7e0ec7",
            "4fff60d83e1c44a09fe2504cae531e73"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5abb2a1adf0940ba8f0afcb852bf0e24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de5e7990b03843ba9a841053b00e5a90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c951f5dc4164bc2902c39ddd48de433"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b201048b3c145c992d9c98c9bdde6bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0e619dec70c45d3ac4b52f411751fb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41fac3595015484a9b170da9adf71e01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f2a846478aa45158aa341cdedac7501"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4430555ecae64994ae5bf26e0b2861a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0803ba2aa91b42cab205412d94a4922c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f75d2efc1cd44986b101714e1f7e0ec7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fff60d83e1c44a09fe2504cae531e73"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke(\"what is the time cut of training dataset of Gemma google models?\", max_tokens=200)\n",
        "print(output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T16:38:42.411253Z",
          "iopub.execute_input": "2024-03-16T16:38:42.411581Z",
          "iopub.status.idle": "2024-03-16T16:41:32.871709Z",
          "shell.execute_reply.started": "2024-03-16T16:38:42.411555Z",
          "shell.execute_reply": "2024-03-16T16:41:32.87069Z"
        },
        "trusted": true,
        "id": "gqKj1Zl7QPpC",
        "outputId": "3c6445f5-4c9a-4e0b-8545-fbe3d76377f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "what is the time cut of training dataset of Gemma google models?\n\nHi @sagar.sahu,\n\nThe training time of the models is dependent on the model and the dataset.\n\nFor example, the training time for the <code>bert-base-uncased</code> model is 10 minutes.\n\nThe training time for the <code>bert-base-cased</code> model is 10 minutes.\n\nThe training time for the <code>bert-base-multilingual-uncased</code> model is 10 minutes.\n\nThe training time for the <code>bert-base-multilingual-cased</code> model is 10 minutes.\n\nThe training time for the <code>bert-large-uncased</code> model is 10 minutes.\n\nThe training time for the <code>bert-large-cased</code> model is 10 minutes.\n\nThe training time for the <code>bert-large-multilingual-uncased</code> model is\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke(\"What is the data cut-off date for the Gemma Google models training dataset?\", max_tokens=200)\n",
        "print(output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T16:44:36.995379Z",
          "iopub.execute_input": "2024-03-16T16:44:36.996093Z",
          "iopub.status.idle": "2024-03-16T16:47:25.137815Z",
          "shell.execute_reply.started": "2024-03-16T16:44:36.996058Z",
          "shell.execute_reply": "2024-03-16T16:47:25.136891Z"
        },
        "trusted": true,
        "id": "xTqL2MaHQPpC",
        "outputId": "a08ef7e0-ba40-419f-d58a-31f21c2914dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "What is the data cut-off date for the Gemma Google models training dataset?\n\nThe data cut-off date for the Gemma Google models training dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models validation dataset?\n\nThe data cut-off date for the Gemma Google models validation dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models test dataset?\n\nThe data cut-off date for the Gemma Google models test dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models development dataset?\n\nThe data cut-off date for the Gemma Google models development dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models training dataset\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "llm.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T17:04:36.616391Z",
          "iopub.execute_input": "2024-03-16T17:04:36.616784Z",
          "iopub.status.idle": "2024-03-16T17:04:37.054643Z",
          "shell.execute_reply.started": "2024-03-16T17:04:36.61675Z",
          "shell.execute_reply": "2024-03-16T17:04:37.053123Z"
        },
        "trusted": true,
        "id": "95c7bLstQPpC",
        "outputId": "2915910c-3114-4319-b77e-ef07086e9ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GemmaLocalHF' object has no attribute 'to'"
          ],
          "ename": "AttributeError",
          "evalue": "'GemmaLocalHF' object has no attribute 'to'",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "6_bXbLiEQPpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "xutERp44QPpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "rlxWul0kQPpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "MG0U8zvPQPpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.0.1"
      ],
      "metadata": {
        "id": "NEep9fHCQPpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tunning"
      ],
      "metadata": {
        "id": "ezicsRq_QPpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.1.0"
      ],
      "metadata": {
        "id": "wO8xN68EQPpD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvlAzToyQPpD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}