{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 726715,
          "sourceType": "datasetVersion",
          "datasetId": 262
        },
        {
          "sourceId": 11384,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 6216
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "GPC (Gemma Python Chat)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/GPC/blob/main/GPC_(Gemma_Python_Chat).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240322%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240322T054640Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5bd6a675fb9b15764ea12c62b83e4658889c025a1c7084704ea3782d1a427eb2156d1b3787f9bb9e9535dc1c46fa838a0825274721cef9f6a4f9feafa5fd902322508c907b9d6167395bb268d97455056e15cf069c731756c8e07948821585c55b7dc28e133e2cc7c6914c5beae7e734a2564cfc8be3b70a3bf1de3e8f2970215a5418d669d001bf4ce3f82919ff9450d17dac4211d2295a7c1eae8c164038cbfea51c4a87094642ec424ecbb9180c18d910b130cfb042ccd36173365e0356ec9ec6b138c710d7c1011f1245b72357520f0f396e85dc60084151334a45ea909e1aa7dd83f1f0b4ad7c7c839bfe45d8e4be3c77face6956676e47dcd225bd9102,pythonquestions:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F262%2F726715%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240322%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240322T054641Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7f774320aff8ecbf8f867a88f4d2edf032b5cb153ea972c0692dc605e1de5c6c3d0e384746b449bb7a30b8b64d5c8a44c708b5ad51048c18a7927994ad28febbd8f7bb55e489b9cc149a8c2504a01b46bff94a6bd91d28dd50a45c81fccde462f61482d63c0ca00e2475a223725a2f239a30cf35ef7a1159aa7260f9cde5d04a837c0466517f1558c5bee8ccd8525cb253e96eaeec26cf67a896607abf486f6315f73bd38da94eac7770b9b64e84269ea1c53640a334e8422ccaf5e699e3747e3baf71213b1595e2facad4e0c9cf8c21d14c663a22a0dd9fe1d3c4e5e595a8b9fb7387176dea61c8ed5eca000fe8dc40822b70b9024c301ca8607c3e022072b8,gemma/transformers/2b/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F6216%2F11384%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240322%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240322T054641Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D74552fbacb0ffa1a43ebb9a21d9afea18f81c6e99a74b95a5b128deb9c8eff747db0a0293e125afa38a765e93469872519ae876029689adb918add244502d1711689d633d6a4dfa9b1c25040a307e9b7cd7ffae2bb6e5e2b0fae903051d21616a11ec08e8fecbe68e920f9b9aa141aaad752995e242cad931cd5b61cba6db8bf80b3d8f963e2973ec6904064e8f5b3cebb33743d3cb7db61d48f95d1f761ac611c99222a9e800f952712c6321292f87973ed8629ae22974d0e1e4ff63627fa705c26bbee05e7893135699760acf26cbed7e49917d7c2ac6425bb657e088db5a16eafd74e76e8ca9eb5b8695e9391072784d7895d66c4d2484844dbb936bdc59c'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "qo0jpnRKsoW0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCP ü§ñ Gemma Python Chatbot"
      ],
      "metadata": {
        "id": "Qx8hbihKsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/8xZNc32/Gemma.png)"
      ],
      "metadata": {
        "id": "B0RdGIyrsoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gemma Python Chatbot üöÄüöÄ help you to answer common questions about the üêç Python programming language, powereg by [Gemma 7B IT](https://blog.google/technology/developers/gemma-open-models/) updated with the latest"
      ],
      "metadata": {
        "id": "5C9kSlAVsoW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "5GhJQPvLsoW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#base\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "# model\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "# data\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-22T05:23:53.116891Z",
          "iopub.execute_input": "2024-03-22T05:23:53.117261Z",
          "iopub.status.idle": "2024-03-22T05:23:53.123695Z",
          "shell.execute_reply.started": "2024-03-22T05:23:53.11723Z",
          "shell.execute_reply": "2024-03-22T05:23:53.122691Z"
        },
        "trusted": true,
        "id": "Vq5ZFhX7soW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"/kaggle/input/gemma/transformers/2b/2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, torch_dtype=torch.float16).cuda()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-22T04:26:50.864179Z",
          "iopub.execute_input": "2024-03-22T04:26:50.86444Z",
          "iopub.status.idle": "2024-03-22T04:27:30.333713Z",
          "shell.execute_reply.started": "2024-03-22T04:26:50.864418Z",
          "shell.execute_reply": "2024-03-22T04:27:30.332922Z"
        },
        "trusted": true,
        "id": "Q1QizKYCsoW4",
        "outputId": "9b505082-176e-4ec8-c951-63e124418a80",
        "colab": {
          "referenced_widgets": [
            "d8422f31fb25405998a357c26c61525d"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8422f31fb25405998a357c26c61525d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "r8OX7Xe-soW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pythonQAData:\n",
        "    \"\"\"\n",
        "    Processes data from Questions and Answers CSV files to provide a structured Q&A format.\n",
        "\n",
        "    Attributes:\n",
        "        questions_path (str): Path to the Questions CSV file.\n",
        "        answers_path (str): Path to the Answers CSV file.\n",
        "\n",
        "    Methods:\n",
        "        load_data(): Loads the CSV data into DataFrames.\n",
        "        merge(): Cleans, merges, and formats the question and answer data.\n",
        "        get_formatted_qa(): Returns a list of formatted question-answer strings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, questions_path, answers_path):\n",
        "        self.questions_path = '../input/pythonquestions/Questions.csv'\n",
        "        self.answers_path = '../input/pythonquestions/Answers.csv'\n",
        "        self.answers_path = '../input/pythonquestions/Tags.csv'\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Loads Questions and Answers data from CSV files.\"\"\"\n",
        "        self.df_questions = pd.read_csv(self.questions_path, encoding=\"ISO-8859-1\", usecols=['Id', 'Score', 'Title'])\n",
        "        self.df_answers = pd.read_csv(self.answers_path, encoding=\"ISO-8859-1\", usecols=['ParentId', 'Score', 'Body'])\n",
        "\n",
        "\n",
        "    def merge(self):\n",
        "        \"\"\"Cleans, merges, and formats the question and answer data.\"\"\"\n",
        "        # Filter by score\n",
        "        self.df_questions = self.df_questions[self.df_questions['Score'] > 0]\n",
        "        self.df_answers = self.df_answers[self.df_answers['Score'] > 0]\n",
        "\n",
        "        # Sort and deduplicate answers\n",
        "        self.df_answers = self.df_answers.sort_values('Score', ascending=False).drop_duplicates(subset=['ParentId'])\n",
        "\n",
        "        # Merge and rename\n",
        "        self.qa_data = self.df_questions.merge(self.df_answers, left_on='Id', right_on='ParentId')\\\n",
        "                                        .rename(columns={'Title': 'Question', 'Body': 'Answer'})[['Question', 'Answer', 'Score_x']]\n",
        "\n",
        "    def get_formatted_qa(self):\n",
        "        \"\"\"Returns a list of formatted question-answer strings.\"\"\"\n",
        "        data = []\n",
        "        for index, row in self.qa_data.iterrows():\n",
        "            data.append(f\"Question:\\n{row['Question']}\\n\\nAnswer:\\n{row['Answer']}\")\n",
        "        return data\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-22T05:42:33.068396Z",
          "iopub.execute_input": "2024-03-22T05:42:33.069099Z",
          "iopub.status.idle": "2024-03-22T05:42:33.079539Z",
          "shell.execute_reply.started": "2024-03-22T05:42:33.069066Z",
          "shell.execute_reply": "2024-03-22T05:42:33.078529Z"
        },
        "trusted": true,
        "id": "bSU-ohmssoW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "DJvR58fcsoW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "R3xhjyW9soW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Use the model\n",
        "input_text = \"What is the data cut-off date for the Gemma Google models training dataset?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = {k: v.to(\"cuda\") for k, v in input_ids.items()}  if torch.cuda.is_available() else {k: v.to(\"cpu\")}\n",
        "outputs = model.generate(**input_ids, max_length=200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T04:13:29.90822Z",
          "iopub.execute_input": "2024-03-21T04:13:29.90861Z",
          "iopub.status.idle": "2024-03-21T04:13:35.388771Z",
          "shell.execute_reply.started": "2024-03-21T04:13:29.908582Z",
          "shell.execute_reply": "2024-03-21T04:13:35.387839Z"
        },
        "trusted": true,
        "id": "P5DbvMzHsoW6",
        "outputId": "25947db0-dae7-47a1-8faa-cf3274824dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "<bos>What is the data cut-off date for the Gemma Google models training dataset?\n\nThe data cut-off date for the Gemma Google models training dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models validation dataset?\n\nThe data cut-off date for the Gemma Google models validation dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models test dataset?\n\nThe data cut-off date for the Gemma Google models test dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models development dataset?\n\nThe data cut-off date for the Gemma Google models development dataset is <strong>2020-01-01</strong>.\n\nWhat is the data cut-off date for the Gemma Google models training dataset\nCPU times: user 5.48 s, sys: 0 ns, total: 5.48 s\nWall time: 5.47 s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Use the model\n",
        "input_text = \"if 5 t-shirts get dry in 10 hour ho long spend 30 t-shirts to get dry?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = {k: v.to(\"cuda\") for k, v in input_ids.items()}  if torch.cuda.is_available() else {k: v.to(\"cpu\")}\n",
        "outputs = model.generate(**input_ids, max_length=500)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "egQtE4VtsoW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.0.1"
      ],
      "metadata": {
        "id": "Cc_e_HAgsoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tunning"
      ],
      "metadata": {
        "id": "5Jl33mfjsoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP - 0.1.0"
      ],
      "metadata": {
        "id": "xUdtzcyTsoW6"
      }
    }
  ]
}